{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x:'%.6f'%x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('F:/DATA/CIC-MalMem-2022/archive/Ransomware_dataset_no_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38757, 54)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>pslist.nproc</th>\n",
       "      <th>pslist.nppid</th>\n",
       "      <th>pslist.avg_threads</th>\n",
       "      <th>pslist.avg_handlers</th>\n",
       "      <th>dlllist.ndlls</th>\n",
       "      <th>dlllist.avg_dlls_per_proc</th>\n",
       "      <th>handles.nhandles</th>\n",
       "      <th>handles.avg_handles_per_proc</th>\n",
       "      <th>handles.nfile</th>\n",
       "      <th>handles.nevent</th>\n",
       "      <th>handles.ndesktop</th>\n",
       "      <th>handles.nkey</th>\n",
       "      <th>handles.nthread</th>\n",
       "      <th>handles.ndirectory</th>\n",
       "      <th>handles.nsemaphore</th>\n",
       "      <th>handles.ntimer</th>\n",
       "      <th>handles.nsection</th>\n",
       "      <th>handles.nmutant</th>\n",
       "      <th>ldrmodules.not_in_load</th>\n",
       "      <th>ldrmodules.not_in_init</th>\n",
       "      <th>ldrmodules.not_in_mem</th>\n",
       "      <th>ldrmodules.not_in_load_avg</th>\n",
       "      <th>ldrmodules.not_in_init_avg</th>\n",
       "      <th>ldrmodules.not_in_mem_avg</th>\n",
       "      <th>malfind.ninjections</th>\n",
       "      <th>malfind.commitCharge</th>\n",
       "      <th>malfind.protection</th>\n",
       "      <th>malfind.uniqueInjections</th>\n",
       "      <th>psxview.not_in_pslist</th>\n",
       "      <th>psxview.not_in_eprocess_pool</th>\n",
       "      <th>psxview.not_in_ethread_pool</th>\n",
       "      <th>psxview.not_in_pspcid_list</th>\n",
       "      <th>psxview.not_in_csrss_handles</th>\n",
       "      <th>psxview.not_in_session</th>\n",
       "      <th>psxview.not_in_deskthrd</th>\n",
       "      <th>psxview.not_in_pslist_false_avg</th>\n",
       "      <th>psxview.not_in_eprocess_pool_false_avg</th>\n",
       "      <th>psxview.not_in_ethread_pool_false_avg</th>\n",
       "      <th>psxview.not_in_pspcid_list_false_avg</th>\n",
       "      <th>psxview.not_in_csrss_handles_false_avg</th>\n",
       "      <th>psxview.not_in_session_false_avg</th>\n",
       "      <th>psxview.not_in_deskthrd_false_avg</th>\n",
       "      <th>modules.nmodules</th>\n",
       "      <th>svcscan.nservices</th>\n",
       "      <th>svcscan.kernel_drivers</th>\n",
       "      <th>svcscan.fs_drivers</th>\n",
       "      <th>svcscan.process_services</th>\n",
       "      <th>svcscan.shared_process_services</th>\n",
       "      <th>svcscan.nactive</th>\n",
       "      <th>callbacks.ncallbacks</th>\n",
       "      <th>callbacks.nanonymous</th>\n",
       "      <th>callbacks.ngeneric</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "      <td>38757.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.735428</td>\n",
       "      <td>42.123745</td>\n",
       "      <td>14.152592</td>\n",
       "      <td>11.946173</td>\n",
       "      <td>266.553469</td>\n",
       "      <td>1950.600562</td>\n",
       "      <td>46.287024</td>\n",
       "      <td>11233.228630</td>\n",
       "      <td>268.879986</td>\n",
       "      <td>998.592048</td>\n",
       "      <td>3934.740873</td>\n",
       "      <td>45.633434</td>\n",
       "      <td>827.986918</td>\n",
       "      <td>1034.425343</td>\n",
       "      <td>104.044018</td>\n",
       "      <td>721.102898</td>\n",
       "      <td>136.311608</td>\n",
       "      <td>350.318523</td>\n",
       "      <td>344.511262</td>\n",
       "      <td>68.307970</td>\n",
       "      <td>107.765849</td>\n",
       "      <td>68.309260</td>\n",
       "      <td>0.034931</td>\n",
       "      <td>0.055679</td>\n",
       "      <td>0.034932</td>\n",
       "      <td>6.268571</td>\n",
       "      <td>513.033129</td>\n",
       "      <td>37.945197</td>\n",
       "      <td>1.600174</td>\n",
       "      <td>1.730604</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>2.140259</td>\n",
       "      <td>1.735222</td>\n",
       "      <td>6.142400</td>\n",
       "      <td>3.730500</td>\n",
       "      <td>8.115618</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.044913</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>0.137280</td>\n",
       "      <td>0.083938</td>\n",
       "      <td>0.182452</td>\n",
       "      <td>137.975798</td>\n",
       "      <td>392.747529</td>\n",
       "      <td>221.669556</td>\n",
       "      <td>25.995769</td>\n",
       "      <td>25.653792</td>\n",
       "      <td>117.425962</td>\n",
       "      <td>122.941972</td>\n",
       "      <td>87.212426</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>7.999845</td>\n",
       "      <td>0.245788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.476369</td>\n",
       "      <td>5.451762</td>\n",
       "      <td>2.443592</td>\n",
       "      <td>1.475778</td>\n",
       "      <td>131.327717</td>\n",
       "      <td>303.247063</td>\n",
       "      <td>5.308865</td>\n",
       "      <td>5637.709662</td>\n",
       "      <td>174.785054</td>\n",
       "      <td>4144.464483</td>\n",
       "      <td>737.633648</td>\n",
       "      <td>4.901205</td>\n",
       "      <td>138.680472</td>\n",
       "      <td>221.012896</td>\n",
       "      <td>9.103882</td>\n",
       "      <td>84.617664</td>\n",
       "      <td>13.132713</td>\n",
       "      <td>117.778979</td>\n",
       "      <td>68.645255</td>\n",
       "      <td>18.464215</td>\n",
       "      <td>21.292917</td>\n",
       "      <td>18.462835</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>0.010160</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>10.633361</td>\n",
       "      <td>3294.657706</td>\n",
       "      <td>63.808224</td>\n",
       "      <td>1.878680</td>\n",
       "      <td>3.010827</td>\n",
       "      <td>0.033675</td>\n",
       "      <td>4.550713</td>\n",
       "      <td>3.038826</td>\n",
       "      <td>4.550746</td>\n",
       "      <td>3.010844</td>\n",
       "      <td>4.670152</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.068238</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.062779</td>\n",
       "      <td>0.055751</td>\n",
       "      <td>0.062989</td>\n",
       "      <td>0.153678</td>\n",
       "      <td>4.434494</td>\n",
       "      <td>2.061265</td>\n",
       "      <td>0.174587</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>1.434006</td>\n",
       "      <td>2.553608</td>\n",
       "      <td>2.954405</td>\n",
       "      <td>0.027811</td>\n",
       "      <td>0.012441</td>\n",
       "      <td>0.430559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.994681</td>\n",
       "      <td>44.186170</td>\n",
       "      <td>896.000000</td>\n",
       "      <td>8.632979</td>\n",
       "      <td>4972.000000</td>\n",
       "      <td>142.078125</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>1568.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.724367</td>\n",
       "      <td>241.703229</td>\n",
       "      <td>1709.000000</td>\n",
       "      <td>42.349033</td>\n",
       "      <td>9170.000000</td>\n",
       "      <td>244.621228</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>3115.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>836.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>668.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.052065</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.048410</td>\n",
       "      <td>0.145998</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.522272</td>\n",
       "      <td>282.125700</td>\n",
       "      <td>2031.000000</td>\n",
       "      <td>48.886336</td>\n",
       "      <td>11909.000000</td>\n",
       "      <td>283.482479</td>\n",
       "      <td>1049.000000</td>\n",
       "      <td>4157.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>1129.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>741.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.054501</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.288867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.113720</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.160017</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>13.125000</td>\n",
       "      <td>295.541547</td>\n",
       "      <td>2140.000000</td>\n",
       "      <td>50.232546</td>\n",
       "      <td>12466.000000</td>\n",
       "      <td>296.711760</td>\n",
       "      <td>1111.000000</td>\n",
       "      <td>4436.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>764.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>0.057829</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.151268</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>24845.951220</td>\n",
       "      <td>3243.000000</td>\n",
       "      <td>53.170732</td>\n",
       "      <td>1047310.000000</td>\n",
       "      <td>33784.193550</td>\n",
       "      <td>807008.000000</td>\n",
       "      <td>7892.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>1525.000000</td>\n",
       "      <td>5637.000000</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>4268.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>174368.000000</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>22.600000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Category  pslist.nproc  pslist.nppid  pslist.avg_threads  \\\n",
       "count 38757.000000  38757.000000  38757.000000        38757.000000   \n",
       "mean      0.735428     42.123745     14.152592           11.946173   \n",
       "std       1.476369      5.451762      2.443592            1.475778   \n",
       "min       0.000000     22.000000      8.000000            1.994681   \n",
       "25%       0.000000     40.000000     12.000000           10.724367   \n",
       "50%       0.000000     41.000000     13.000000           12.522272   \n",
       "75%       0.000000     43.000000     16.000000           13.125000   \n",
       "max       5.000000    188.000000     27.000000           16.500000   \n",
       "\n",
       "       pslist.avg_handlers  dlllist.ndlls  dlllist.avg_dlls_per_proc  \\\n",
       "count         38757.000000   38757.000000               38757.000000   \n",
       "mean            266.553469    1950.600562                  46.287024   \n",
       "std             131.327717     303.247063                   5.308865   \n",
       "min              44.186170     896.000000                   8.632979   \n",
       "25%             241.703229    1709.000000                  42.349033   \n",
       "50%             282.125700    2031.000000                  48.886336   \n",
       "75%             295.541547    2140.000000                  50.232546   \n",
       "max           24845.951220    3243.000000                  53.170732   \n",
       "\n",
       "       handles.nhandles  handles.avg_handles_per_proc  handles.nfile  \\\n",
       "count      38757.000000                  38757.000000   38757.000000   \n",
       "mean       11233.228630                    268.879986     998.592048   \n",
       "std         5637.709662                    174.785054    4144.464483   \n",
       "min         4972.000000                    142.078125     358.000000   \n",
       "25%         9170.000000                    244.621228     808.000000   \n",
       "50%        11909.000000                    283.482479    1049.000000   \n",
       "75%        12466.000000                    296.711760    1111.000000   \n",
       "max      1047310.000000                  33784.193550  807008.000000   \n",
       "\n",
       "       handles.nevent  handles.ndesktop  handles.nkey  handles.nthread  \\\n",
       "count    38757.000000      38757.000000  38757.000000     38757.000000   \n",
       "mean      3934.740873         45.633434    827.986918      1034.425343   \n",
       "std        737.633648          4.901205    138.680472       221.012896   \n",
       "min       1568.000000         23.000000    327.000000       455.000000   \n",
       "25%       3115.000000         44.000000    748.000000       836.000000   \n",
       "50%       4157.000000         45.000000    815.000000      1129.000000   \n",
       "75%       4436.000000         47.000000    898.000000      1196.000000   \n",
       "max       7892.000000         96.000000   1525.000000      5637.000000   \n",
       "\n",
       "       handles.ndirectory  handles.nsemaphore  handles.ntimer  \\\n",
       "count        38757.000000        38757.000000    38757.000000   \n",
       "mean           104.044018          721.102898      136.311608   \n",
       "std              9.103882           84.617664       13.132713   \n",
       "min             60.000000          369.000000       84.000000   \n",
       "25%            101.000000          668.000000      129.000000   \n",
       "50%            103.000000          741.000000      140.000000   \n",
       "75%            107.000000          764.000000      144.000000   \n",
       "max            498.000000         4268.000000      382.000000   \n",
       "\n",
       "       handles.nsection  handles.nmutant  ldrmodules.not_in_load  \\\n",
       "count      38757.000000     38757.000000            38757.000000   \n",
       "mean         350.318523       344.511262               68.307970   \n",
       "std          117.778979        68.645255               18.464215   \n",
       "min           69.000000       162.000000                7.000000   \n",
       "25%          216.000000       289.000000               52.000000   \n",
       "50%          396.000000       350.000000               70.000000   \n",
       "75%          431.000000       392.000000               80.000000   \n",
       "max         1110.000000       583.000000              240.000000   \n",
       "\n",
       "       ldrmodules.not_in_init  ldrmodules.not_in_mem  \\\n",
       "count            38757.000000           38757.000000   \n",
       "mean               107.765849              68.309260   \n",
       "std                 21.292917              18.462835   \n",
       "min                 16.000000               7.000000   \n",
       "25%                 92.000000              52.000000   \n",
       "50%                109.000000              70.000000   \n",
       "75%                119.000000              80.000000   \n",
       "max                264.000000             240.000000   \n",
       "\n",
       "       ldrmodules.not_in_load_avg  ldrmodules.not_in_init_avg  \\\n",
       "count                38757.000000                38757.000000   \n",
       "mean                     0.034931                    0.055679   \n",
       "std                      0.008993                    0.010160   \n",
       "min                      0.017563                    0.040615   \n",
       "25%                      0.029963                    0.052065   \n",
       "50%                      0.034483                    0.054501   \n",
       "75%                      0.038296                    0.057829   \n",
       "max                      0.441441                    0.486486   \n",
       "\n",
       "       ldrmodules.not_in_mem_avg  malfind.ninjections  malfind.commitCharge  \\\n",
       "count               38757.000000         38757.000000          38757.000000   \n",
       "mean                    0.034932             6.268571            513.033129   \n",
       "std                     0.008997            10.633361           3294.657706   \n",
       "min                     0.017563             1.000000              1.000000   \n",
       "25%                     0.029963             3.000000              4.000000   \n",
       "50%                     0.034483             4.000000              5.000000   \n",
       "75%                     0.038296             5.000000              6.000000   \n",
       "max                     0.441441           125.000000         174368.000000   \n",
       "\n",
       "       malfind.protection  malfind.uniqueInjections  psxview.not_in_pslist  \\\n",
       "count        38757.000000              38757.000000           38757.000000   \n",
       "mean            37.945197                  1.600174               1.730604   \n",
       "std             63.808224                  1.878680               3.010827   \n",
       "min              6.000000                  1.000000               0.000000   \n",
       "25%             18.000000                  1.166667               0.000000   \n",
       "50%             24.000000                  1.288867               0.000000   \n",
       "75%             30.000000                  1.500000               2.000000   \n",
       "max            750.000000                 22.600000              43.000000   \n",
       "\n",
       "       psxview.not_in_eprocess_pool  psxview.not_in_ethread_pool  \\\n",
       "count                  38757.000000                 38757.000000   \n",
       "mean                       0.001135                     2.140259   \n",
       "std                        0.033675                     4.550713   \n",
       "min                        0.000000                     0.000000   \n",
       "25%                        0.000000                     0.000000   \n",
       "50%                        0.000000                     1.000000   \n",
       "75%                        0.000000                     3.000000   \n",
       "max                        1.000000                   152.000000   \n",
       "\n",
       "       psxview.not_in_pspcid_list  psxview.not_in_csrss_handles  \\\n",
       "count                38757.000000                  38757.000000   \n",
       "mean                     1.735222                      6.142400   \n",
       "std                      3.038826                      4.550746   \n",
       "min                      0.000000                      4.000000   \n",
       "25%                      0.000000                      4.000000   \n",
       "50%                      0.000000                      5.000000   \n",
       "75%                      2.000000                      7.000000   \n",
       "max                     43.000000                    156.000000   \n",
       "\n",
       "       psxview.not_in_session  psxview.not_in_deskthrd  \\\n",
       "count            38757.000000             38757.000000   \n",
       "mean                 3.730500                 8.115618   \n",
       "std                  3.010844                 4.670152   \n",
       "min                  2.000000                 4.000000   \n",
       "25%                  2.000000                 6.000000   \n",
       "50%                  2.000000                 6.000000   \n",
       "75%                  4.000000                 9.000000   \n",
       "max                 45.000000               158.000000   \n",
       "\n",
       "       psxview.not_in_pslist_false_avg  \\\n",
       "count                     38757.000000   \n",
       "mean                          0.037789   \n",
       "std                           0.058266   \n",
       "min                           0.000000   \n",
       "25%                           0.000000   \n",
       "50%                           0.008050   \n",
       "75%                           0.047619   \n",
       "max                           0.551282   \n",
       "\n",
       "       psxview.not_in_eprocess_pool_false_avg  \\\n",
       "count                            38757.000000   \n",
       "mean                                 0.000072   \n",
       "std                                  0.001106   \n",
       "min                                  0.000000   \n",
       "25%                                  0.000000   \n",
       "50%                                  0.000000   \n",
       "75%                                  0.000000   \n",
       "max                                  0.027778   \n",
       "\n",
       "       psxview.not_in_ethread_pool_false_avg  \\\n",
       "count                           38757.000000   \n",
       "mean                                0.044913   \n",
       "std                                 0.068238   \n",
       "min                                 0.000000   \n",
       "25%                                 0.000000   \n",
       "50%                                 0.022222   \n",
       "75%                                 0.063830   \n",
       "max                                 0.800000   \n",
       "\n",
       "       psxview.not_in_pspcid_list_false_avg  \\\n",
       "count                          38757.000000   \n",
       "mean                               0.037899   \n",
       "std                                0.059100   \n",
       "min                                0.000000   \n",
       "25%                                0.000000   \n",
       "50%                                0.008074   \n",
       "75%                                0.047619   \n",
       "max                                1.000000   \n",
       "\n",
       "       psxview.not_in_csrss_handles_false_avg  \\\n",
       "count                            38757.000000   \n",
       "mean                                 0.137280   \n",
       "std                                  0.062779   \n",
       "min                                  0.062500   \n",
       "25%                                  0.097561   \n",
       "50%                                  0.113720   \n",
       "75%                                  0.151268   \n",
       "max                                  0.821053   \n",
       "\n",
       "       psxview.not_in_session_false_avg  psxview.not_in_deskthrd_false_avg  \\\n",
       "count                      38757.000000                       38757.000000   \n",
       "mean                           0.083938                           0.182452   \n",
       "std                            0.055751                           0.062989   \n",
       "min                            0.012987                           0.093750   \n",
       "25%                            0.048410                           0.145998   \n",
       "50%                            0.057143                           0.160017   \n",
       "75%                            0.095238                           0.195122   \n",
       "max                            0.576923                           0.831579   \n",
       "\n",
       "       modules.nmodules  svcscan.nservices  svcscan.kernel_drivers  \\\n",
       "count      38757.000000       38757.000000            38757.000000   \n",
       "mean         137.975798         392.747529              221.669556   \n",
       "std            0.153678           4.434494                2.061265   \n",
       "min          137.000000          94.000000               55.000000   \n",
       "25%          138.000000         392.000000              221.000000   \n",
       "50%          138.000000         395.000000              222.000000   \n",
       "75%          138.000000         395.000000              222.000000   \n",
       "max          138.000000         395.000000              222.000000   \n",
       "\n",
       "       svcscan.fs_drivers  svcscan.process_services  \\\n",
       "count        38757.000000              38757.000000   \n",
       "mean            25.995769                 25.653792   \n",
       "std              0.174587                  1.546094   \n",
       "min              6.000000                  7.000000   \n",
       "25%             26.000000                 24.000000   \n",
       "50%             26.000000                 27.000000   \n",
       "75%             26.000000                 27.000000   \n",
       "max             26.000000                 27.000000   \n",
       "\n",
       "       svcscan.shared_process_services  svcscan.nactive  callbacks.ncallbacks  \\\n",
       "count                     38757.000000     38757.000000          38757.000000   \n",
       "mean                        117.425962       122.941972             87.212426   \n",
       "std                           1.434006         2.553608              2.954405   \n",
       "min                          26.000000        30.000000             50.000000   \n",
       "25%                         117.000000       122.000000             87.000000   \n",
       "50%                         118.000000       123.000000             88.000000   \n",
       "75%                         118.000000       124.000000             88.000000   \n",
       "max                         118.000000       129.000000             89.000000   \n",
       "\n",
       "       callbacks.nanonymous  callbacks.ngeneric        Class  \n",
       "count          38757.000000        38757.000000 38757.000000  \n",
       "mean               0.000774            7.999845     0.245788  \n",
       "std                0.027811            0.012441     0.430559  \n",
       "min                0.000000            7.000000     0.000000  \n",
       "25%                0.000000            8.000000     0.000000  \n",
       "50%                0.000000            8.000000     0.000000  \n",
       "75%                0.000000            8.000000     0.000000  \n",
       "max                1.000000            8.000000     1.000000  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in data.columns if col not in ['Category', 'Class']]\n",
    "X = data[feature_cols]\n",
    "y = data['Class']\n",
    "# X=X.values\n",
    "# Y=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7768173155455429, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9721756085320976, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1511721566462825, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.304136107636566, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3405969071013146, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6888881263450344, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7938595026933513, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8475811999113603, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0564134923218376, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1074353733055773, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1220747041286359, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4957001525663145, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.586765172800142, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0964209175629094, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3021831677598072, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3717604252904323, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4038722898664844, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3882458390150703, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3446198941422836, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.346382852542149, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3095389819987417, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1949588522040813, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3525545648687682, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.57752332613893, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.901114383683392, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.298752577327434, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.432831155532568, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.459253819491607, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.404963749543583, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.257757047040684, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.06156966190798, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.717971890107208, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.624329447523564, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.003157788588737, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.814278338948867, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.583338613908666, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.976783225136785, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.177220962665757, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.231693705075884, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.174263601429942, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.019494041333985, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.775671928890745, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.425382533783647, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.30448675029814, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 37.96696330132821, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44.003626515025104, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 49.45829035255329, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.375493345381926, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 58.806823398139215, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62.804049602061326, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.41529308313343, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 69.55484445011231, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 71.93944441928409, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 74.75872073157612, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 77.30975979453146, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 79.62731793354877, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 81.73845184312711, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 83.66728408946437, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 85.43411390657656, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 87.05609815259547, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88.54792531885258, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89.92231457365841, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.19037414772126, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 92.3618646001288, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 93.44540006055044, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 94.44860783264562, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 95.37825892975235, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.24037686103136, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 97.04033011463045, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 97.78212181262836, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.45111958564596, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.09338761090657, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.68917212269662, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.2378738769514, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.75380703719868, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.23614606037357, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.68032842440034, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.09086345831776, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.46953086714478, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.81984192104936, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.14334701132964, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.44130577687093, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.71476367018306, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.96461678646473, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.19164082977481, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.3965069032749, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.57979088291816, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.74197969773945, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.8834751124242, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.00459632174932, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.10558165509197, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.19535452981869, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.26160011853172, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.30266968645361, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.32074226322088, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.31687860676419, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.29154362819965, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.24482638488516, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.17655753528688, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.06522956864802, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.95359372598524, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.89066459442921, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.21804562351255, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107.45830348712417, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108.61745777301337, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109.70125159282097, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.71502084938787, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111.66368611390007, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.55177976454812, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.38347991722789, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.10347975522465, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.83804722679429, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.53932387647465, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.1976206001391, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.80547703009985, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.37185436711626, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.901934049453, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.39911828646555, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.86594835236524, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.30452098944086, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.71667719375043, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.10408851019218, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.46829783666901, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.81074025584678, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.1327551105394, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.43559426672155, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.72042862209628, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.98835380556574, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.24039526798435, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.47751307326392, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.70060632434804, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.91051731050001, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.10803532701843, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.29390030079422, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.46880609210162, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.63340359364048, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.78830359305925, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.93407946542024, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.07126960147707, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.20037971660929, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.32188498809293, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.43623200557425, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.54384061510754, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.783314117195232, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0012375293117657, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0774466502351743, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0975108925260884, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1720177971579915, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7945524665702237, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9186992475078455, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0135391783729801, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0478897955380262, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0550236294525348, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0573129068325215, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3212864509906694, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5685994353153774, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.181992074128118, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4457277513538713, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.552982533441309, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.599929756322865, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.7186809628747994, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.7340551320014868, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.64482618213691, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5330423136800277, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5738447734919987, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.670457432858598, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.937151521214389, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.302364363513675, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.358726360346395, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.377606800437434, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.378978666056526, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.344602443429153, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.254309687092444, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.141107405936793, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.923901053136433, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.988144770198545, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.556453285436476, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.617686525460357, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.421807553719816, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.013816072211284, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.421230964665654, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.687351222612563, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.842023271361256, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.88249138744871, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.79301591916129, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.075325824947356, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.571077772318404, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.41455986487429, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.65000741300571, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.339705610524874, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 61.5703055184666, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.61248052434476, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 68.11910915971077, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 71.25491383384491, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 74.19777908511736, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 76.86922950734237, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 79.29490397313853, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 81.50353805152898, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 83.51989933180911, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 85.36505194652784, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 87.05704254139317, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88.61146907837565, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 90.04191003640679, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.36025068472841, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 92.57693824973683, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 93.7011865712822, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 94.74114425035779, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 95.70403487786899, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.59627545243492, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 97.4235769671076, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.19103062235226, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.90318180208644, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.56409391943899, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.17740348989824, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.74636793773186, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.27390697600973, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.76253635927893, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.20141730277419, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.61390215760156, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.99794501030183, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.35364342944357, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.68215791869852, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.98442571828176, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.26137525213602, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.5138961979251, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.74281622212612, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.94888502919972, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.13276636182499, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.29503519223942, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.4361781285981, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.55659475911679, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.6565994212867, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.73642308547667, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.79621458203206, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.83604125194509, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.85588901589006, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.85566164566674, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.8351793866814, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.6307099737335, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.70729136846239, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.63339671366475, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.53649588686784, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.41531459556148, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.26943227929667, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.09841680516664, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.28967094328681, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.64498861044973, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107.91203714896525, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109.09705320966705, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.20578674872, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111.2435435065161, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.21522362754328, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.12535601353892, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.97812939777002, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.77742025180689, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.52681800150546, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.2296482763822, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.8889937484353, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.50771367194533, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.08846140937366, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.6337007847518, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.14572085258496, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.62664959642736, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.0784664379371, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.50301369620375, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.90200708956081, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.27704542704963, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.62961939327258, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.96111972615778, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.27284460052029, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.56600653490325, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.84173860523603, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.10110023315777, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.34508252103429, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.57461306838181, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.79056052063902, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.99373863295597, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.18491013657223, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.36479020755661, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.53404971879077, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.69331822966838, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.84318673760046, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.98421025100984, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.11691012425104, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.24177627357446, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7865075344481056, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9847684506646601, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.101719318647497, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.197605889855481, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3465244005213322, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6814569505211239, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7723086804146533, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9715270255758242, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.046545991801736, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0838372546919004, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1049342411047292, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1301150638015542, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.941710571935971, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3040421569061493, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4421057053110076, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5161913468764965, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5237198716520766, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.493894279209286, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4699320982865345, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4564050607714023, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4436004531049775, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.60773736432958, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.805194787963501, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.08428123667477, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.473766162708387, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.51288802107831, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.585562996871602, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.555073917082012, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.514667896004653, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.41418688893097, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.603247110208372, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.206156170723546, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.120379263717354, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.900936006498796, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.201624136306407, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.261163242628584, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.085080529065465, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.708113232409687, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.146520278942916, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.444503571399167, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.611972323475612, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.651292608859592, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.5217027287905, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.95216651217808, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.74250376699061, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.93160698617123, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.585394403168664, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 61.76934013951288, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.54093709899678, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 68.94947809151674, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 72.03726581360266, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 74.84081163483563, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 77.3917888073359, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 79.71777373183727, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 81.7519794472741, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 83.3444590493593, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 85.18801364913432, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 86.88557924973668, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88.44144188726668, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89.87067098828629, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.18669287888085, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 92.40086064557573, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 93.52282655996589, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 94.56091001007212, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 95.5223688351848, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.41359189121286, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 97.24023818566724, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.00734178459732, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.71939399395507, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.38041022696211, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.99398571482413, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.56334307101966, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.09137281806116, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.58308958820734, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.02951521200879, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.44940111408471, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.83963637519412, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.19993448536843, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.5327263799839, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.83022135677217, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.10139683792397, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.34999813317415, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.57608887670168, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.77996983845463, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.96204715305282, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.12275460122999, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.26250490310795, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.38166260687126, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.480529521362, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.55933727670217, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.61824303076318, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.657327529187, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.67659336859938, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.67596371082652, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.65528063717355, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.61430275129447, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.55270265715811, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.44497951011135, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.34890364388315, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.23120196766982, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.08881225033008, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.92118743569205, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.25033647332651, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.62517612927613, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107.9098976366788, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109.11083194336418, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.2338317846429, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111.28430667434378, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.2672592425235, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.18731879655995, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.04877209866112, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.85559061105339, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.61145558008481, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.31978065463007, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.98373269229475, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.6062507624924, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.19006378544553, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.73770658363537, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.2515347706452, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.73373843891869, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.18635472153301, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.61127943990648, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.01027772687233, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.38499387062095, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.73696031446569, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.06760597433077, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.3782638950937, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.67017831123482, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.94451105995911, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.20234758474328, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.4447024400411, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.67252429355045, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.88670065335745, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.08806212629337, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.27738639208239, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.45540187272455, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.622791091554, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.78019383170553, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.928209992717, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.0674022647761, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.19829861555094, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.32139458022579, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6832078229135732, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7276630884599626, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8698560934606689, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0431210130282693, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1989898071575453, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9930882290638579, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0401186680932142, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0755195767862347, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0944168522639472, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.083439097158191, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2607304888604745, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.531262120099939, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0402092619442556, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2469748308769226, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.34855835183032, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3600282388990195, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3306500246112876, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2833558070212234, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.36871566089809, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.125710717596547, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.974010547728426, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.54422104965613, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.709429301897387, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.436223068551726, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.769445375421412, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.75616698683018, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.444071405358756, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.83775026078041, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.96713373913599, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.873685699528778, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.59263211662244, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.16245158630585, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.61933907858503, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.97907641083185, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17.17803015272625, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.13956180687387, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.63962497877708, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.882848261156823, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.896321562958462, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.696838235905517, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.55707496955324, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.775765930686674, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.1175751919167, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44.9455553852836, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50.22000169517452, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.96459833343576, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 59.224973446454754, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.192429766994735, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.76351822955502, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 69.98314804702736, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 72.88900091809123, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 75.51620052554907, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 77.89676860234891, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 80.05912409263269, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 82.02811345703321, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 83.8253167936893, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 85.46944322068106, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 86.97672434502118, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88.36044783170786, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89.6313487274912, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 90.80011441643894, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.87707589019028, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 92.87095685508288, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 93.78928797312325, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 94.63864155019348, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 95.42479320250192, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.15284198438557, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.82730447863304, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 97.45219056814977, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.0310668376158, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.58583006609898, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.09752650947178, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.561699013041, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.98536775571903, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.35623396872847, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.69912052359246, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.01400135740863, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.30147969051866, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.5623174084959, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.79732311615129, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.007282464745, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.19291652523492, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.35485793016717, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.49363805438564, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.60968047003998, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.70329847090363, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.77469346154454, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.82395380853855, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.8510536195337, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.85585035192406, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.8025584633467, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.88668138033724, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.91132623629487, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.88011995332778, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107.79643045537289, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108.66338842205151, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109.48390706157294, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.26070000779785, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.99629750788331, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111.69306115535002, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.35319720941774, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.97876872814408, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.5717066206319, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.13381962966925, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.66680350530424, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.17224933045703, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.6516511482219, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.10641287788117, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.53785473228278, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.94721900479593, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.33567547295452, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.70432626551641, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.05421039756236, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.38630795599786, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.70154388908682, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.00079158097631, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.28487610587646, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.5545772669454, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.81063237627298, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.05373885630537, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.28455665898511, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.50371050580054, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.71179195115477, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.90936133869833, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.09694960207193, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.2750599450469, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.44416943430916, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.60473044015117, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.75717204480317, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.90190130173204, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.03930447324636, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.16974814155179, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.2935802749601, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.41113123368231, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.5227147105021, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.62862860386684, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.72915585320615, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.82456522840509, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.91511206398974, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.00103895344942, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.08257640270168, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4930227848167874, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7726810943705118, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.605744268814675, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8173472713251329, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4279732405777565, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7382280405745405, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.618101898907554, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0815814029699595, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.007925489914953, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.543249159787322, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.824377905297325, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.946411133512157, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.3489897282819925, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.6178630379909436, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.458596279118069, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.3388876532038125, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8909273132184694, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0061100543194357, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5157948591136687, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8917571811283835, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0338128846100716, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8283199096654528, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6783971861629254, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.190317592581366, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.3528748696844985, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.950768027447168, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.238414268593743, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.341782962831161, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.389781138917144, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.39167268830937, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.328308057320328, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.532870758547233, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.18036492230739, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.056213692612715, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.892111359555116, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.116608303402288, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.067081414226365, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.869745654827682, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.52761139755249, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.045086091892955, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.435555270306168, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.716817330804474, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.907107446894528, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.023512636881321, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.666948705574953, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.396979793630251, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.9353245184725, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17.966535942280046, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.996694023942396, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.027816419454382, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.371234989202236, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.91175451399147, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.351900286968828, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.7266890009206, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.04656198012698, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.310304300382455, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.148675775820337, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.27693296412903, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.434126642580225, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.569937260863924, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.65689299019044, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.68771658037673, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 36.664736084116875, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 37.593280210558625, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.47869270391777, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.31945280331225, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.117137282368724, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.890238469256175, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.60954068307425, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42.251362638090264, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42.87956634826346, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 43.48002885984802, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44.05131032446913, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44.591570446357146, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45.091335614263244, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45.56956103380853, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 46.029906455007, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 46.47398325036605, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 46.902587002883735, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.31621977527563, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.71531010172394, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 48.10029132210036, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 48.471617679084616, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 48.829756924508125, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 49.175177427025325, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 49.59402083089179, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50.031876053786334, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50.4426314066403, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50.82817022094003, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 51.19022798254252, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 51.53040444749822, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 51.85017494129536, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.150900858718636, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.43383936919325, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.70015234153505, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.94776957430866, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.152574745828844, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.35233635588724, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.55130952175999, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.74574825021743, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.93323706167623, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.11254175961166, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.283170810792356, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.44505864788118, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.59837067090136, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.74339155561612, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.8804638316534, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.00995476263434, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.13223856470471, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.24768672275465, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.35666252784941, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.45951787691517, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.556591354038915, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.64820706146098, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.734673964415094, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.81628561593874, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.89332015689311, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.96604053347785, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.03469488924559, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.09951706609043, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.160727215170354, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.21853242392033, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.2731274074891, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.324695194386024, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.37340780891707, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.41942693335709, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.4629045519063, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.5039835598656, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.54279834171513, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.57947532125848, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.614133464495104, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.65044806337278, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.68194236812364, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.71103422568146, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.73860136946502, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.763607351175146, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.787870115912504, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.81096478696395, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.83284568729102, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.85353859752804, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.873094025616986, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.89156961012039, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.747e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.623237882163124e-06\n",
      "                                        Feature Importance\n",
      "pslist.nproc                                     -0.000000\n",
      "pslist.nppid                                     -0.012861\n",
      "pslist.avg_threads                               -0.082743\n",
      "pslist.avg_handlers                              -0.000104\n",
      "dlllist.ndlls                                    -0.000351\n",
      "dlllist.avg_dlls_per_proc                        -0.024059\n",
      "handles.nhandles                                 -0.000000\n",
      "handles.avg_handles_per_proc                     -0.000130\n",
      "handles.nfile                                    -0.000000\n",
      "handles.nevent                                   -0.000100\n",
      "handles.ndesktop                                 -0.000000\n",
      "handles.nkey                                     -0.000105\n",
      "handles.nthread                                   0.000847\n",
      "handles.ndirectory                                0.008500\n",
      "handles.nsemaphore                                0.000578\n",
      "handles.ntimer                                   -0.009236\n",
      "handles.nsection                                 -0.001164\n",
      "handles.nmutant                                   0.000983\n",
      "ldrmodules.not_in_load                           -0.001157\n",
      "ldrmodules.not_in_init                           -0.000284\n",
      "ldrmodules.not_in_mem                            -0.000524\n",
      "ldrmodules.not_in_load_avg                        0.000000\n",
      "ldrmodules.not_in_init_avg                       -2.108956\n",
      "ldrmodules.not_in_mem_avg                         0.000000\n",
      "malfind.ninjections                               0.003726\n",
      "malfind.commitCharge                              0.000000\n",
      "malfind.protection                                0.000000\n",
      "malfind.uniqueInjections                         -0.014996\n",
      "psxview.not_in_pslist                             0.001696\n",
      "psxview.not_in_eprocess_pool                      0.018669\n",
      "psxview.not_in_ethread_pool                       0.000000\n",
      "psxview.not_in_pspcid_list                        0.000000\n",
      "psxview.not_in_csrss_handles                      0.000000\n",
      "psxview.not_in_session                            0.000000\n",
      "psxview.not_in_deskthrd                          -0.000000\n",
      "psxview.not_in_pslist_false_avg                   1.164804\n",
      "psxview.not_in_eprocess_pool_false_avg            0.000000\n",
      "psxview.not_in_ethread_pool_false_avg            -0.000000\n",
      "psxview.not_in_pspcid_list_false_avg              0.000000\n",
      "psxview.not_in_csrss_handles_false_avg           -1.930373\n",
      "psxview.not_in_session_false_avg                  0.000000\n",
      "psxview.not_in_deskthrd_false_avg                 0.000000\n",
      "modules.nmodules                                  0.431147\n",
      "svcscan.nservices                                 0.000000\n",
      "svcscan.kernel_drivers                            0.027690\n",
      "svcscan.fs_drivers                                0.352316\n",
      "svcscan.process_services                          0.029946\n",
      "svcscan.shared_process_services                  -0.119589\n",
      "svcscan.nactive                                  -0.014142\n",
      "callbacks.ncallbacks                             -0.000960\n",
      "callbacks.nanonymous                              0.118561\n",
      "callbacks.ngeneric                                0.321794\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 1)\n",
      "程序运行时间:12394.766092300415毫秒\n",
      "Index(['pslist.nppid', 'pslist.avg_threads', 'pslist.avg_handlers',\n",
      "       'dlllist.ndlls', 'dlllist.avg_dlls_per_proc',\n",
      "       'handles.avg_handles_per_proc', 'handles.nevent', 'handles.nkey',\n",
      "       'handles.nthread', 'handles.ndirectory', 'handles.nsemaphore',\n",
      "       'handles.ntimer', 'handles.nsection', 'handles.nmutant',\n",
      "       'ldrmodules.not_in_load', 'ldrmodules.not_in_init',\n",
      "       'ldrmodules.not_in_mem', 'ldrmodules.not_in_init_avg',\n",
      "       'malfind.ninjections', 'malfind.uniqueInjections',\n",
      "       'psxview.not_in_pslist', 'psxview.not_in_eprocess_pool',\n",
      "       'psxview.not_in_pslist_false_avg',\n",
      "       'psxview.not_in_csrss_handles_false_avg', 'modules.nmodules',\n",
      "       'svcscan.kernel_drivers', 'svcscan.fs_drivers',\n",
      "       'svcscan.process_services', 'svcscan.shared_process_services',\n",
      "       'svcscan.nactive', 'callbacks.ncallbacks', 'callbacks.nanonymous',\n",
      "       'callbacks.ngeneric', 'Category', 'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "T1 = time.time()\n",
    "\n",
    "df=data.copy()\n",
    "# 重置索引\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "##\n",
    "num_features  = data[feature_cols]\n",
    "num_feature_names = list(num_features.columns)\n",
    "num_features_df= pd.melt(df, value_vars=num_feature_names)\n",
    "lambdas = 10**np.linspace(-10,5,500)\n",
    "lasso_model = LassoCV(alphas=lambdas,random_state=100, normalize=True).fit(X,y)\n",
    "print(lasso_model.alpha_)\n",
    "\n",
    "# 索引和重要性做成dataframe形式\n",
    "FI_lasso = pd.DataFrame({\"Feature Importance\":lasso_model.coef_}, index=feature_cols)\n",
    "print(FI_lasso)\n",
    "\n",
    "## 由高到低进行排序\n",
    "FI_lasso.sort_values(\"Feature Importance\",ascending=False).round(4)\n",
    "\n",
    "# 获取重要程度大于0的系数指标\n",
    "FI_lasso[FI_lasso[\"Feature Importance\"] !=0 ].sort_values(\"Feature Importance\").plot(kind=\"barh\",color='firebrick',alpha=0.8)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "FI_index = FI_lasso[FI_lasso[\"Feature Importance\"] !=0 ].index\n",
    "FI_val = FI_lasso[FI_lasso[\"Feature Importance\"] !=0 ].values\n",
    "FI_lasso = pd.DataFrame(FI_val, columns = ['Feature Importance'], index = FI_index)\n",
    "print(FI_lasso.shape)\n",
    "T2 = time.time()\n",
    "print('程序运行时间:%s毫秒' % ((T2 - T1)*1000))\n",
    "\n",
    "choose_cols = FI_lasso.index.tolist()\n",
    "choose_cols.append('Category')\n",
    "choose_cols.append('Class')\n",
    "choose_data = df[choose_cols].copy()\n",
    "\n",
    "choose_data.to_csv(path_or_buf='data_clean.csv',index=None)\n",
    "print(choose_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.85230763228898, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72704189768976, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.73224091970576, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.07822294173383, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.8631173037341, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.85105742468798, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72572983540707, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.73097495791924, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.07688715225453, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.86276585037486, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.84971759751645, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72432375507256, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72961828104343, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.07545564491173, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.86238920404487, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.84828172556018, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72281692141485, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72816438988744, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.07392156218853, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.86198555845236, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.84674292182055, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72120211554795, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72660631874703, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0722775549154, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.8615529777528, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.84509380521195, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.71947160237804, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72493660499984, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.07051574853102, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.86108938724797, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.84332646588146, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.7176170927695, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72314724992992, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.06862770382457, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.86059256327908, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.84143242577875, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.71562970417858, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.72122968260518, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0666043774931, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.86006012283173, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.83940259858394, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.71349991760326, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.7191747185817, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.06443607865975, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85948951187969, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.83722724615154, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.711217532658, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.7169725150289, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.06211242247468, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85887799295793, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.83489593090658, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.70877161866247, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.71461252506603, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0596222808622, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85822263235124, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.8323974666529, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.70615046243245, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.71208344685525, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0569537290603, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85752028570262, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.82971986422368, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.7033415127211, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.70937316853298, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.05409398831645, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85676758299498, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.8268502731208, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.70033131898953, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.70646871200339, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.05102936562787, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85596091224839, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.82377492001604, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.69710546828964, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.70335617006083, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.04774518793775, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85509640214404, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.82047904363975, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.69364851681263, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.70002064041228, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.04422573183527, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85416990359481, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.8169468215116, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.6899439155937, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.6964461539639, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0404541493252, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85317696928296, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.8131612943263, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.68597393014548, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.6926155986872, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.03641238612653, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85211283268063, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.80910428535287, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.68171955913627, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.68851063852713, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.03208109698645, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.85097238473894, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.80475631102735, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.67716044112464, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.684111625847, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0274395527701, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.84975014943241, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.80009648678805, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.67227475754143, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.67939750740413, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.02246554166246, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.8484402571849, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.79510242820083, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.6670391314774, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.67434572408138, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.01713526359927, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.84703641666192, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.78975014006426, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.66142851279366, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.66893210402569, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0114232168071, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.84553188410611, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.78401390184922, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.65541606128022, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.66313074662062, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.00530207614426, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.84391943106522, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.77786614264696, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.64897301810524, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.65691389934463, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.9987425635851, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.84219195574939, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.7712773078914, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.64206856931497, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.65025182723204, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.99171330922016, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.84033990580608, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.7642157148811, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.6346696992895, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.64311266977714, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.98418070156114, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.83835498110044, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.555e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.75664740097002, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.6267410342399, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.63546228990387, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.97610872818206, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.83622763771218, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.555e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.74853595553787, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.61824467322475, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.62726411231557, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.96745880481777, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.83394764443251, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.555e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.73984234662123, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.60914001087048, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.61847895057787, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.9581895932897, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.83150403331544, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.555e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.73052472696148, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.59938354184752, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.60906481938642, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.9482568036359, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.82888504608353, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.555e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.72053823313092, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.58892865717175, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.59897645438576, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.93761298655943, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.82607807718536, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.555e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.70983476448842, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.57772542302163, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.58816620864513, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.9262073078953, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.82306991022223, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.555e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.69836274905808, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.56572034415973, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.5765828744465, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.91398533040794, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.81984677673458, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.68606689094142, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.55285611258458, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.56416982313664, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.90088866991618, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.81639092176505, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.67288789902477, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.53907133584676, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.55086844062953, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.8868548496511, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.81268679716223, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.65876219584914, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.52430024867746, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.53661525234776, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.87181692005103, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.80871666177832, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.64362160597045, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.5084724041565, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.52134226250357, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.85570316505044, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.80446141732587, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.554e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.62739301873222, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.49151234119736, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.50497663575976, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.8384362760431, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.79990218598284, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.553e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.6099980274789, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.473339231159, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.48744035310105, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.81993490561277, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.79501361705091, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.553e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.59135254286804, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.45386650027376, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.46864984635837, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.80011042865428, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.7897734703156, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.553e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.57136596794186, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.43300142267015, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.44851560865226, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.778868541768, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.7841568718018, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.553e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.54994234546533, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.41064468876573, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.42694177322602, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.7561081054472, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.77813873260052, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.552e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.52697744852293, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.38668994324783, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.4038256674899, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.7317214231104, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.77168215332198, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.552e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.50235999579692, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.36102461613882, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.37905733479445, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.70559146855874, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.76476446185094, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.552e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.475970725456, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.33352418019211, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.35251902355637, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.67759460555848, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.75735185775142, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.551e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.44768138999999, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.30405927307805, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.32408372161981, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.64759789044973, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.74940204558675, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.551e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.41735450718207, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.2724900572404, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.29361816673986, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.6154627678866, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.74088073140953, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.550e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.38484254576875, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.23866681211913, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.26097784311688, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.58102952713355, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.73174987664333, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.550e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.3499871783596, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.20242924961616, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.22600626962608, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.54413854934074, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.72195703089007, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.549e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.3126184728455, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.16360577859425, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.18853855658628, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.50461384029026, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.71146246972154, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.549e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.27255402374223, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.12201272331794, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.14839731950997, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.46227098818738, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.70020613937812, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.548e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.22979455102922, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.0774534706017, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.1053926834464, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.41690866432245, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.68814222518728, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.548e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.18405169319354, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.02971754322755, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.05932145731498, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.3683127086582, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.67520168210449, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.13504850224837, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.97857996045212, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 130.00996362339106, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.31625387996337, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.66132756026703, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.546e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.08255260394145, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.92380252424613, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.95709127445122, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.26048706048212, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.64646048883027, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.546e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.0263229936021, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.86512198795404, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.90045285364073, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.20074983929595, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.63051170618527, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.545e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.96608710468485, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.80226536896134, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.83978563801466, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.13675900606398, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.61341614907555, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.544e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.90156538813193, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.7349374123967, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.774794988569, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.06821916943684, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.59508396821009, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.543e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.83245528336226, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.66282276621638, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.70518423025254, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.99480694279858, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.57542368441867, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.542e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.7584332201592, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.58558406365054, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.63062520874945, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.9161789872921, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.55434316091049, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.541e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.67915481643723, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.50286487734493, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.5507690782086, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.83196446844657, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.53173530424296, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.540e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.59424667184288, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.4142713438598, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.46523915687513, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.74178830945176, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.5074791582779, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.539e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.50331569856343, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.31939531834905, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.37364428365237, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.64520792248979, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.48146573402941, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.537e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.40593896302323, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.21779637873294, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.2755550103733, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.54178473325385, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.4535757683055, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.536e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.3016650513033, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.1090096107551, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.17051724102498, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.4310341044418, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.42364050430277, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.534e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.19000784405534, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.99252152439726, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 129.05804250702803, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.31245563379503, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.39151929566097, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.533e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.07045924150344, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.8678081254393, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.93760674335306, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.1854970546938, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.35706595615746, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.531e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.94247402004203, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.73428508399678, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.80866563359373, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.04956913204971, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.32010630377434, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.80544917192051, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.59134626143512, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.67062305614579, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.90406340598767, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.28044996131507, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.527e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.65875974824542, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.43833377555347, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.5228376491733, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.74831756262972, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.23787527207917, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.525e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.50174836199378, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.27456327895166, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.364651852267, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.58160415291012, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.19219633095508, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.523e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.33369053027245, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.099276301133, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.19533767989083, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.40316816726641, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.14315498526716, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.520e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.15383616031804, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.91169141862908, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 128.01411660545224, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.21222336787893, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.09046787154517, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.518e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.96137208626931, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.71096669946691, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.82019339822041, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.00791973035045, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 66.0339448550453, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.515e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.75543684623001, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.49619983597948, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.61270368828418, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.78930946200376, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.97326793378939, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.512e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.53510216538909, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.26643944981738, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.39067058868407, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.55543654663715, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.90812242252395, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.509e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.2994102787156, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.02067012811023, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 127.15313494937365, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.3052501550488, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.8381240872839, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.505e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.04729548248058, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.75779148452227, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.89905638947405, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.0377028296704, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.76038451327027, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.77766312454213, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.47668071754907, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.62729430896914, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.75157342486506, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.68009513208548, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.498e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.48931497599344, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.17614004560127, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.33668777934453, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.44547320268039, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.59441898189156, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.494e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.18103732433556, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.85478559468851, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126.02593955137704, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.11626022384083, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.50172145378008, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.489e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.85146630147221, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.51130663608848, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.6937259660321, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.76547455817226, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.40172535889413, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.484e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.49923586167557, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.14362773794915, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125.33866355798034, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.39080519623272, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.29436658152186, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.479e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.12287066441525, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.75012004254793, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.95919883811936, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.99042438535943, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.17901537028057, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.474e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.72072871230236, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.32959529416644, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.5536583905951, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.56239306478085, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.05490954457956, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.468e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.29114565305494, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.8803543249752, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124.12050263626085, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.10517522753943, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.92155717628461, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.461e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.83235774942972, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.40041703035172, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.65777717240485, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.61677952251739, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.77796301503221, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.36505850514544, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.89182717732993, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123.16710390146028, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.0952027153757, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.62351775198985, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.447e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.81880185666759, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.33672626653458, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.6323651744973, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.53869434987303, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.45735656616444, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.25802802384314, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.75156328449765, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 122.06868280738053, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.9443302152763, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.27840529498874, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.431e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.66200794675873, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.12793238024645, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 121.46778312903842, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.31009136794498, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.08554573144443, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.422e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.0268736132387, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.4627598210653, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.82704572209772, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.63353021908131, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.877685872022084, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.412e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.3502106996373, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.75412953865165, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 120.14406319698983, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.9120069345644, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.6538476715328, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.402e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.62885771473134, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.99817877806794, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119.41594261430566, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.14278517428573, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.41229532538317, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.391e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.86064975609449, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.19262031634784, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 118.64002683299768, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.32272126829866, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.15214153597537, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.04282567004398, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.33431535779908, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.81405765698136, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.4492563101449, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62.87101490524101, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.367e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.17206971123917, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.4205607138548, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 117.11466828188091, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.70423217867585, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62.56755568817468, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.353e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.24605069264507, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 115.66782538568839, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116.03544481150044, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.492393037178, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62.23892723848509, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.338e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.2609620452333, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.505121967375, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 114.85292656499107, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.20794916850485, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 61.88362890298575, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.323e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.34375860375243, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.08508630387519, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.54022082385262, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.86645317471982, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 61.49857852397052, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.306e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.1315073161974, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111.64141255761263, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 112.20883977065648, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109.46588257194418, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 61.0816528390708, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111.47097470757703, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.16283486802428, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.81984309158531, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107.99430608967114, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 60.70940112329333, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.269e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110.16090294306335, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108.60963171194075, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109.35468888780946, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.44086337247836, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 60.24883311804743, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.248e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108.51844341405754, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.93064194406534, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 107.80020946727149, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.02724179913562, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 59.72685230909204, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.225e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.82165001203632, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.17992853133049, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.14902088738319, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.07330160113179, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 59.18547489414941, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.201e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105.08703076125636, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.328917540931, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 104.38815408906908, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.12609586674262, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 58.58886712732081, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.173e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103.28903264500107, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.5212057093207, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 102.94592324011916, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.10967475894662, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.99961608955286, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.148e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.40462320423973, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 100.05627805049087, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 101.09313819837439, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.95858141474966, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.29467753135579, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.119e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 99.34986323266298, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.15052072804752, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 98.84428378097596, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 94.65325167821052, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 56.52841125602272, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.087e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 97.25877716497521, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.05368894500998, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 96.41018116364205, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 92.18402522231173, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 55.69998575881009, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.059e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 95.0537145472073, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 93.78512841063588, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 94.11844645594428, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.00507952310281, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.79495880896327, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.020e+02, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 92.70703336145047, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.33865051946742, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91.78101112820062, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88.780591185812, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.92200549387927, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.803e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 90.20263091453123, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88.7140265624783, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89.30438770948572, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 86.20009720054594, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.929735684069115, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.389e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 87.52949826868019, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 85.91495141489452, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 86.64831651864034, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 83.4061076360307, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 51.91330014193889, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.051e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 84.68128908468816, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 82.94679106307433, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 83.81209935351649, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 80.1484241125026, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50.675033284688766, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.678e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 81.65677524788956, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 79.80890727526477, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 80.76439681541677, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 76.57006923209258, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 49.10134575876333, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 78.4552376935975, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 76.57172409555122, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 77.52535156654861, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 73.41198916827022, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.08112307176718, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.369e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 75.04118421886821, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 73.24864279136406, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 74.24965785912366, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70.19036622966664, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45.038745027239074, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.830e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 71.6169442345562, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70.02189231779286, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70.91050743974148, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.63449797960368, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 43.58094506053855, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.264e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 68.13588509325058, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.40501755027458, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 67.1890499113077, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 58.834514431666236, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.198922110125835, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.674e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64.57456524154043, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.34604376797145, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 58.31126998688907, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.270419750660324, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 37.11936867585935, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.999e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 51.8978503702815, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47.97831915660609, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.05441786733746, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 49.22380890716693, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 36.315562681301586, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.080e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 44.04907159358811, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.11629803431657, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.88106118017146, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 43.7312923989403, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.5513161233542, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.289e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42.684150471016835, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.523208341038924, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.454319507689235, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.72700046324297, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.36506066798981, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.156e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.37575806088273, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.55181039944901, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.51083516808643, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.78388606751463, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.19244885538109, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.382e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.102998687302716, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.397898584627086, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.066548274564923, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.925775282160004, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.157010546904942, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.652e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.5260897863825, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.13857430953638, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.227444427917078, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.092871129551213, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.810554069460053, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.014e+01, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17.042148902361618, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.744079411028792, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.714675312423253, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.26125874255436, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.644518709540975, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.841e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.981610308161436, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.0366802110405615, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.791920601699871, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.2680693839423895, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.972123192247807, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.426284691432841, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.271262003808204, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.744352494377267, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6918141949520304, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.247031365448862, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.283e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.666914337537833, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.019083059932825, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.644526402217508, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.207145336402391, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.79890764731968, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.346e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.444908369138716, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.78346352789049, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.341700515159403, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7411610405907254, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.66932003472185, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.808e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.1840428699639745, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5493674221750666, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.183221552149234, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.458379413451496, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.12700793284516, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.025e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9862228310896057, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2132798881955864, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9422555953233314, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.441685925361071, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.599127807383269, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.396e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.854860775752229, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1529425388034724, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9271656168104982, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3345342566883573, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.104202772682044, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.263e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.444306029393374, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8911803134618026, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2089405567415383, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.169082607005464, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4479645199427296, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.112e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.821786996262574, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.324384776105603, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0038475250536862, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.464173558787053, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.441295614513649, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.047e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.812533547719511, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0974110153920833, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.217737349599247, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.268644263690021, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.619753502005949, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.960e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.755999744627104, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8870222714691067, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9653224674486864, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3071453433930742, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2768566725428627, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.858e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6101425136666876, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.769155804976151, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8339387449489095, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.540343848082216, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.2332497125028965, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.756e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.523042074269199, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.614192422527367, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7193842333784914, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.176645466154696, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.733260119353815, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.618e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4624447766946673, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.465298369706886, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5527047960523532, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.28012513678442, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.122894890232018, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.447e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4616320388799977, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5219915392322036, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6241017231122896, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4158545466422083, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.604672035756849, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.155e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4937253580881134, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.647333707400662, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7971870317437606, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6104365368451568, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.605340289879194, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.052e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5441322973265414, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8557715353925914, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9910554222663563, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.775037519662021, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.716822814064784, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.806e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.647307810839777, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0558284908258884, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.1439666766299865, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9278225349679587, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9793753035390296, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.757e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.8530502840605436, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2409359209951845, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.28321052979652, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0943112253311256, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43210363305965416, tolerance: 0.1673386602593049\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.747e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0674774387784396, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4288237321065935, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4552017481900066, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6106328972020947, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+00, tolerance: 7.185e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2664298587624216, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.316884435620864, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.1539414158256704, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4915460633838507, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.406064647792618, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.180719712015559, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.041923237566834, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8189461198111303, tolerance: 0.5813374991937063\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.10305512574962, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9402600426049048, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1033984772037115, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4101377967980966, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2933413935677436, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4338824932368652, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9074496098810414, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8461427339261718, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.951553494522841, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7006079555957285, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6786800189893256, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8039355810562938, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8119241686164287, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.722860046103051, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7322467726102104, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.812643606682343, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.735713455688142, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7499856479893765, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8389165438625241, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7613934511687148, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7889003435443556, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7851909586617012, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7690058738053267, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7448509542714419, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7414826926963087, tolerance: 0.6599224447669702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7212908489707388, tolerance: 0.6599318841514538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\envs\\DiBanBen\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alpha_lasso =lambdas\n",
    "coefs_lasso = []\n",
    "\n",
    "for i in alpha_lasso:\n",
    "    lasso_model = LassoCV(alphas=[i], random_state=100, normalize=True).fit(X, y)\n",
    "    coefs_lasso.append(lasso_model.coef_)\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# ax = plt.gca()\n",
    "# ax.plot(np.log(lambdas), coefs_lasso)\n",
    "# # ax.set_xscale('log')\n",
    "# plt.axis('tight')\n",
    "# plt.xlabel('Log Lambdas')\n",
    "# # plt.xlim((-10,0))\n",
    "# plt.ylabel('coefficients')\n",
    "# plt.title('(a)Lasso regression coefficients Vs alpha',y=-0.1)\n",
    "# plt.savefig('lasso1.jpg', dpi=600, bbox_inches='tight',pad_inches=0)#bbox_inches='tight',pad_inches=0解决图片空白边缘\n",
    "# plt.savefig('lasso1.svg', dpi=600, bbox_inches='tight',pad_inches=0)#bbox_inches='tight',pad_inches=0解决图片空白边缘\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.0011572 , -0.01268751, -0.19517721, -0.00232506, -0.00095865,\n",
       "        -0.02189267,  0.00001518, -0.00007647,  0.00003597, -0.00016014,\n",
       "        -0.00528631, -0.00001163,  0.00215662,  0.00709615,  0.00119132,\n",
       "        -0.00887771, -0.00106456,  0.00170027, -0.00280541, -0.00283531,\n",
       "         0.00325714, -1.78757765, -4.77413968,  4.29230745,  0.01517614,\n",
       "        -0.00000036, -0.00154459, -0.027606  ,  0.04584322,  0.05022409,\n",
       "         0.02432787, -0.05613505,  0.02529971,  0.00026255, -0.04323616,\n",
       "        -2.49930068, -4.35172699, -4.16099799,  2.21055382, -1.12430398,\n",
       "         3.99104722,  1.71065248,  0.35534885, -0.02126251,  0.0525133 ,\n",
       "         0.30755428,  0.06824135, -0.09719541, -0.01102796, -0.00094249,\n",
       "         0.15615256,  0.44456835]),\n",
       " array([-0.00115711, -0.01268753, -0.19517696, -0.00232504, -0.00095865,\n",
       "        -0.02189272,  0.00001518, -0.00007646,  0.00003597, -0.00016014,\n",
       "        -0.0052863 , -0.00001163,  0.00215662,  0.00709614,  0.00119132,\n",
       "        -0.00887771, -0.00106456,  0.00170027, -0.00280528, -0.00283532,\n",
       "         0.00325701, -1.7872718 , -4.77414312,  4.29200497,  0.0151759 ,\n",
       "        -0.00000036, -0.00154455, -0.02760603,  0.0458428 ,  0.05022399,\n",
       "         0.02432787, -0.05613412,  0.02529919,  0.00026218, -0.04323574,\n",
       "        -2.49923121, -4.35172481, -4.16099894,  2.21051466, -1.1242822 ,\n",
       "         3.99101044,  1.71063302,  0.355349  , -0.02126221,  0.05251299,\n",
       "         0.30755412,  0.06824095, -0.0971957 , -0.01102797, -0.00094248,\n",
       "         0.15615241,  0.44456755]),\n",
       " array([-0.00115702, -0.01268756, -0.19517669, -0.00232503, -0.00095865,\n",
       "        -0.02189277,  0.00001518, -0.00007645,  0.00003597, -0.00016014,\n",
       "        -0.00528628, -0.00001163,  0.00215661,  0.00709612,  0.00119132,\n",
       "        -0.00887772, -0.00106456,  0.00170027, -0.00280513, -0.00283534,\n",
       "         0.00325688, -1.78694403, -4.7741468 ,  4.2916808 ,  0.01517565,\n",
       "        -0.00000036, -0.00154451, -0.02760605,  0.04584235,  0.05022389,\n",
       "         0.02432786, -0.05613312,  0.02529864,  0.00026178, -0.04323528,\n",
       "        -2.49915675, -4.35172247, -4.16099995,  2.21047269, -1.12425886,\n",
       "         3.99097103,  1.71061215,  0.35534915, -0.02126188,  0.05251266,\n",
       "         0.30755395,  0.06824052, -0.09719602, -0.01102797, -0.00094248,\n",
       "         0.15615225,  0.4445667 ]),\n",
       " array([-0.00115691, -0.01268758, -0.1951764 , -0.00232502, -0.00095864,\n",
       "        -0.02189283,  0.00001518, -0.00007643,  0.00003597, -0.00016013,\n",
       "        -0.00528626, -0.00001163,  0.00215661,  0.00709611,  0.00119131,\n",
       "        -0.00887772, -0.00106456,  0.00170027, -0.00280498, -0.00283536,\n",
       "         0.00325674, -1.78659277, -4.77415074,  4.2913334 ,  0.01517539,\n",
       "        -0.00000036, -0.00154446, -0.02760607,  0.04584187,  0.05022378,\n",
       "         0.02432785, -0.05613206,  0.02529806,  0.00026135, -0.0432348 ,\n",
       "        -2.49907696, -4.35171997, -4.16100104,  2.21042771, -1.12423385,\n",
       "         3.99092879,  1.7105898 ,  0.35534932, -0.02126154,  0.0525123 ,\n",
       "         0.30755376,  0.06824006, -0.09719635, -0.01102798, -0.00094247,\n",
       "         0.15615208,  0.44456578]),\n",
       " array([-0.00115681, -0.01268761, -0.19517609, -0.002325  , -0.00095864,\n",
       "        -0.0218929 ,  0.00001518, -0.00007642,  0.00003597, -0.00016013,\n",
       "        -0.00528624, -0.00001163,  0.0021566 ,  0.00709609,  0.00119131,\n",
       "        -0.00887772, -0.00106456,  0.00170027, -0.00280481, -0.00283538,\n",
       "         0.00325659, -1.78621634, -4.77415497,  4.29096111,  0.0151751 ,\n",
       "        -0.00000036, -0.00154441, -0.0276061 ,  0.04584135,  0.05022366,\n",
       "         0.02432784, -0.05613091,  0.02529743,  0.00026089, -0.04323427,\n",
       "        -2.49899145, -4.35171728, -4.16100221,  2.21037951, -1.12420704,\n",
       "         3.99088352,  1.71056584,  0.35534949, -0.02126116,  0.05251192,\n",
       "         0.30755357,  0.06823957, -0.09719671, -0.01102799, -0.00094246,\n",
       "         0.15615189,  0.4445648 ]),\n",
       " array([-0.00115669, -0.01268764, -0.19517576, -0.00232499, -0.00095864,\n",
       "        -0.02189297,  0.00001518, -0.00007641,  0.00003597, -0.00016013,\n",
       "        -0.00528622, -0.00001163,  0.0021566 ,  0.00709607,  0.00119131,\n",
       "        -0.00887772, -0.00106456,  0.00170026, -0.00280463, -0.0028354 ,\n",
       "         0.00325643, -1.78581293, -4.7741595 ,  4.29056214,  0.01517479,\n",
       "        -0.00000036, -0.00154436, -0.02760613,  0.0458408 ,  0.05022354,\n",
       "         0.02432783, -0.05612968,  0.02529675,  0.0002604 , -0.04323372,\n",
       "        -2.49889981, -4.3517144 , -4.16100346,  2.21032786, -1.12417831,\n",
       "         3.99083501,  1.71054016,  0.35534968, -0.02126076,  0.05251151,\n",
       "         0.30755335,  0.06823904, -0.0971971 , -0.011028  , -0.00094245,\n",
       "         0.15615169,  0.44456375]),\n",
       " array([-0.00115656, -0.01268767, -0.1951754 , -0.00232497, -0.00095864,\n",
       "        -0.02189304,  0.00001518, -0.00007639,  0.00003597, -0.00016013,\n",
       "        -0.0052862 , -0.00001163,  0.00215659,  0.00709605,  0.00119131,\n",
       "        -0.00887772, -0.00106456,  0.00170026, -0.00280444, -0.00283542,\n",
       "         0.00325625, -1.78538061, -4.77416436,  4.29013457,  0.01517446,\n",
       "        -0.00000036, -0.00154431, -0.02760616,  0.04584021,  0.0502234 ,\n",
       "         0.02432782, -0.05612837,  0.02529602,  0.00025987, -0.04323312,\n",
       "        -2.4988016 , -4.35171132, -4.16100479,  2.2102725 , -1.12414753,\n",
       "         3.99078302,  1.71051264,  0.35534988, -0.02126033,  0.05251107,\n",
       "         0.30755313,  0.06823848, -0.09719751, -0.01102801, -0.00094245,\n",
       "         0.15615147,  0.44456262]),\n",
       " array([-0.00115643, -0.0126877 , -0.19517502, -0.00232495, -0.00095863,\n",
       "        -0.02189312,  0.00001518, -0.00007638,  0.00003597, -0.00016013,\n",
       "        -0.00528618, -0.00001163,  0.00215659,  0.00709603,  0.00119131,\n",
       "        -0.00887773, -0.00106456,  0.00170026, -0.00280423, -0.00283544,\n",
       "         0.00325607, -1.78491731, -4.77416956,  4.28967636,  0.01517411,\n",
       "        -0.00000036, -0.00154425, -0.02760619,  0.04583958,  0.05022325,\n",
       "         0.0243278 , -0.05612696,  0.02529525,  0.00025931, -0.04323247,\n",
       "        -2.49869635, -4.35170801, -4.16100623,  2.21021318, -1.12411454,\n",
       "         3.99072731,  1.71048315,  0.3553501 , -0.02125988,  0.0525106 ,\n",
       "         0.30755288,  0.06823788, -0.09719796, -0.01102802, -0.00094244,\n",
       "         0.15615124,  0.44456141]),\n",
       " array([-0.00115629, -0.01268773, -0.19517462, -0.00232493, -0.00095863,\n",
       "        -0.02189321,  0.00001518, -0.00007636,  0.00003597, -0.00016013,\n",
       "        -0.00528616, -0.00001163,  0.00215658,  0.00709601,  0.0011913 ,\n",
       "        -0.00887773, -0.00106456,  0.00170026, -0.00280401, -0.00283547,\n",
       "         0.00325587, -1.7844208 , -4.77417514,  4.28918531,  0.01517373,\n",
       "        -0.00000036, -0.00154418, -0.02760622,  0.0458389 ,  0.0502231 ,\n",
       "         0.02432779, -0.05612545,  0.02529442,  0.0002587 , -0.04323178,\n",
       "        -2.49858357, -4.35170447, -4.16100777,  2.21014961, -1.12407918,\n",
       "         3.99066761,  1.71045155,  0.35535033, -0.02125938,  0.0525101 ,\n",
       "         0.30755262,  0.06823723, -0.09719843, -0.01102803, -0.00094243,\n",
       "         0.156151  ,  0.44456011]),\n",
       " array([-0.00115613, -0.01268777, -0.19517418, -0.00232491, -0.00095862,\n",
       "        -0.0218933 ,  0.00001517, -0.00007634,  0.00003597, -0.00016013,\n",
       "        -0.00528613, -0.00001163,  0.00215658,  0.00709599,  0.0011913 ,\n",
       "        -0.00887773, -0.00106456,  0.00170026, -0.00280378, -0.00283549,\n",
       "         0.00325565, -1.78388871, -4.77418111,  4.28865906,  0.01517332,\n",
       "        -0.00000036, -0.00154411, -0.02760626,  0.04583817,  0.05022293,\n",
       "         0.02432778, -0.05612383,  0.02529352,  0.00025805, -0.04323105,\n",
       "        -2.49846269, -4.35170068, -4.16100942,  2.21008147, -1.12404129,\n",
       "         3.99060362,  1.71041768,  0.35535058, -0.02125886,  0.05250955,\n",
       "         0.30755234,  0.06823654, -0.09719894, -0.01102804, -0.00094242,\n",
       "         0.15615073,  0.44455873]),\n",
       " array([-0.00115597, -0.01268781, -0.19517371, -0.00232489, -0.00095862,\n",
       "        -0.0218934 ,  0.00001517, -0.00007632,  0.00003597, -0.00016013,\n",
       "        -0.0052861 , -0.00001163,  0.00215657,  0.00709596,  0.0011913 ,\n",
       "        -0.00887773, -0.00106456,  0.00170026, -0.00280353, -0.00283552,\n",
       "         0.00325542, -1.78331849, -4.77418752,  4.28809511,  0.01517289,\n",
       "        -0.00000036, -0.00154404, -0.0276063 ,  0.04583739,  0.05022275,\n",
       "         0.02432776, -0.05612209,  0.02529257,  0.00025736, -0.04323025,\n",
       "        -2.49833316, -4.35169661, -4.16101118,  2.21000846, -1.12400069,\n",
       "         3.99053505,  1.71038138,  0.35535085, -0.02125829,  0.05250898,\n",
       "         0.30755204,  0.06823579, -0.09719949, -0.01102805, -0.0009424 ,\n",
       "         0.15615045,  0.44455724]),\n",
       " array([-0.00115579, -0.01268786, -0.19517321, -0.00232486, -0.00095861,\n",
       "        -0.0218935 ,  0.00001517, -0.0000763 ,  0.00003597, -0.00016013,\n",
       "        -0.00528607, -0.00001163,  0.00215657,  0.00709594,  0.0011913 ,\n",
       "        -0.00887774, -0.00106456,  0.00170026, -0.00280325, -0.00283555,\n",
       "         0.00325518, -1.7827074 , -4.77419438,  4.28749073,  0.01517242,\n",
       "        -0.00000036, -0.00154396, -0.02760634,  0.04583656,  0.05022256,\n",
       "         0.02432774, -0.05612023,  0.02529155,  0.00025661, -0.04322941,\n",
       "        -2.49819434, -4.35169225, -4.16101307,  2.20993021, -1.12395717,\n",
       "         3.99046157,  1.71034249,  0.35535114, -0.02125769,  0.05250836,\n",
       "         0.30755172,  0.06823499, -0.09720007, -0.01102806, -0.00094239,\n",
       "         0.15615015,  0.44455565]),\n",
       " array([-0.00115561, -0.0126879 , -0.19517267, -0.00232484, -0.00095861,\n",
       "        -0.02189361,  0.00001517, -0.00007628,  0.00003597, -0.00016013,\n",
       "        -0.00528604, -0.00001163,  0.00215656,  0.00709591,  0.00119129,\n",
       "        -0.00887774, -0.00106456,  0.00170025, -0.00280296, -0.00283558,\n",
       "         0.00325491, -1.78205251, -4.77420173,  4.28684304,  0.01517192,\n",
       "        -0.00000036, -0.00154388, -0.02760638,  0.04583566,  0.05022235,\n",
       "         0.02432773, -0.05611824,  0.02529045,  0.00025581, -0.0432285 ,\n",
       "        -2.49804558, -4.35168758, -4.1610151 ,  2.20984636, -1.12391054,\n",
       "         3.99038282,  1.7103008 ,  0.35535145, -0.02125704,  0.05250769,\n",
       "         0.30755137,  0.06823414, -0.0972007 , -0.01102808, -0.00094238,\n",
       "         0.15614982,  0.44455394]),\n",
       " array([-0.0011554 , -0.01268795, -0.1951721 , -0.00232481, -0.0009586 ,\n",
       "        -0.02189373,  0.00001517, -0.00007626,  0.00003597, -0.00016013,\n",
       "        -0.00528601, -0.00001163,  0.00215655,  0.00709588,  0.00119129,\n",
       "        -0.00887775, -0.00106456,  0.00170025, -0.00280265, -0.00283562,\n",
       "         0.00325463, -1.78135069, -4.77420961,  4.28614893,  0.01517139,\n",
       "        -0.00000036, -0.00154379, -0.02760643,  0.0458347 ,  0.05022213,\n",
       "         0.02432771, -0.0561161 ,  0.02528927,  0.00025496, -0.04322753,\n",
       "        -2.49788615, -4.35168257, -4.16101727,  2.2097565 , -1.12386057,\n",
       "         3.99029842,  1.71025613,  0.35535178, -0.02125634,  0.05250698,\n",
       "         0.307551  ,  0.06823322, -0.09720137, -0.01102809, -0.00094237,\n",
       "         0.15614948,  0.44455211]),\n",
       " array([-0.00115519, -0.01268801, -0.19517148, -0.00232478, -0.0009586 ,\n",
       "        -0.02189386,  0.00001517, -0.00007623,  0.00003597, -0.00016013,\n",
       "        -0.00528597, -0.00001163,  0.00215654,  0.00709585,  0.00119128,\n",
       "        -0.00887775, -0.00106456,  0.00170025, -0.00280232, -0.00283566,\n",
       "         0.00325433, -1.78059857, -4.77421806,  4.28540508,  0.01517082,\n",
       "        -0.00000036, -0.00154369, -0.02760648,  0.04583367,  0.0502219 ,\n",
       "         0.02432769, -0.05611381,  0.02528801,  0.00025404, -0.04322648,\n",
       "        -2.4977153 , -4.3516772 , -4.1610196 ,  2.20966019, -1.12380701,\n",
       "         3.99020798,  1.71020826,  0.35535213, -0.0212556 ,  0.05250621,\n",
       "         0.30755061,  0.06823224, -0.09720209, -0.01102811, -0.00094235,\n",
       "         0.1561491 ,  0.44455015]),\n",
       " array([-0.00115495, -0.01268806, -0.19517082, -0.00232474, -0.00095859,\n",
       "        -0.021894  ,  0.00001517, -0.0000762 ,  0.00003597, -0.00016013,\n",
       "        -0.00528593, -0.00001163,  0.00215653,  0.00709581,  0.00119128,\n",
       "        -0.00887775, -0.00106456,  0.00170025, -0.00280196, -0.0028357 ,\n",
       "         0.00325401, -1.77979255, -4.77422711,  4.28460792,  0.0151702 ,\n",
       "        -0.00000036, -0.00154359, -0.02760654,  0.04583257,  0.05022164,\n",
       "         0.02432767, -0.05611136,  0.02528666,  0.00025306, -0.04322536,\n",
       "        -2.4975322 , -4.35167145, -4.1610221 ,  2.20955699, -1.12374961,\n",
       "         3.99011105,  1.71015695,  0.35535251, -0.0212548 ,  0.0525054 ,\n",
       "         0.30755018,  0.06823119, -0.09720286, -0.01102812, -0.00094233,\n",
       "         0.1561487 ,  0.44454804]),\n",
       " array([-0.0011547 , -0.01268813, -0.19517011, -0.00232471, -0.00095858,\n",
       "        -0.02189415,  0.00001517, -0.00007617,  0.00003597, -0.00016013,\n",
       "        -0.00528589, -0.00001163,  0.00215652,  0.00709577,  0.00119128,\n",
       "        -0.00887776, -0.00106456,  0.00170025, -0.00280158, -0.00283574,\n",
       "         0.00325366, -1.77892876, -4.77423681,  4.28375363,  0.01516954,\n",
       "        -0.00000036, -0.00154347, -0.0276066 ,  0.04583139,  0.05022137,\n",
       "         0.02432764, -0.05610873,  0.02528521,  0.00025201, -0.04322417,\n",
       "        -2.49733598, -4.35166529, -4.16102478,  2.20944638, -1.1236881 ,\n",
       "         3.99000718,  1.71010197,  0.35535291, -0.02125394,  0.05250452,\n",
       "         0.30754973,  0.06823007, -0.09720369, -0.01102814, -0.00094232,\n",
       "         0.15614828,  0.44454579]),\n",
       " array([-0.00115444, -0.01268819, -0.19516935, -0.00232467, -0.00095858,\n",
       "        -0.02189431,  0.00001517, -0.00007614,  0.00003597, -0.00016012,\n",
       "        -0.00528584, -0.00001163,  0.00215651,  0.00709573,  0.00119127,\n",
       "        -0.00887776, -0.00106456,  0.00170024, -0.00280117, -0.00283579,\n",
       "         0.00325329, -1.77800307, -4.77424721,  4.28283811,  0.01516884,\n",
       "        -0.00000036, -0.00154335, -0.02760666,  0.04583013,  0.05022108,\n",
       "         0.02432762, -0.05610592,  0.02528366,  0.00025088, -0.04322288,\n",
       "        -2.4971257 , -4.35165869, -4.16102764,  2.20932785, -1.12362219,\n",
       "         3.98989587,  1.71004305,  0.35535335, -0.02125302,  0.05250358,\n",
       "         0.30754924,  0.06822886, -0.09720458, -0.01102816, -0.0009423 ,\n",
       "         0.15614782,  0.44454338]),\n",
       " array([-0.00115415, -0.01268826, -0.19516853, -0.00232463, -0.00095857,\n",
       "        -0.02189448,  0.00001516, -0.00007611,  0.00003597, -0.00016012,\n",
       "        -0.00528579, -0.00001163,  0.0021565 ,  0.00709569,  0.00119127,\n",
       "        -0.00887777, -0.00106456,  0.00170024, -0.00280073, -0.00283584,\n",
       "         0.00325289, -1.77701104, -4.77425835,  4.28185698,  0.01516808,\n",
       "        -0.00000036, -0.00154323, -0.02760673,  0.04582877,  0.05022077,\n",
       "         0.02432759, -0.0561029 ,  0.025282  ,  0.00024967, -0.0432215 ,\n",
       "        -2.49690034, -4.35165161, -4.16103071,  2.20920083, -1.12355155,\n",
       "         3.98977657,  1.70997991,  0.35535381, -0.02125204,  0.05250257,\n",
       "         0.30754872,  0.06822756, -0.09720553, -0.01102818, -0.00094228,\n",
       "         0.15614732,  0.44454079]),\n",
       " array([-0.00115385, -0.01268834, -0.19516766, -0.00232459, -0.00095856,\n",
       "        -0.02189466,  0.00001516, -0.00007607,  0.00003596, -0.00016012,\n",
       "        -0.00528574, -0.00001163,  0.00215649,  0.00709565,  0.00119126,\n",
       "        -0.00887777, -0.00106456,  0.00170024, -0.00280026, -0.00283589,\n",
       "         0.00325246, -1.7759479 , -4.77427029,  4.28080553,  0.01516727,\n",
       "        -0.00000036, -0.00154309, -0.0276068 ,  0.04582732,  0.05022043,\n",
       "         0.02432756, -0.05609966,  0.02528022,  0.00024837, -0.04322003,\n",
       "        -2.49665884, -4.35164403, -4.16103401,  2.2090647 , -1.12347584,\n",
       "         3.98964873,  1.70991224,  0.35535431, -0.02125099,  0.05250149,\n",
       "         0.30754816,  0.06822618, -0.09720654, -0.0110282 , -0.00094226,\n",
       "         0.1561468 ,  0.44453802]),\n",
       " array([-0.00115352, -0.01268842, -0.19516673, -0.00232455, -0.00095855,\n",
       "        -0.02189486,  0.00001516, -0.00007603,  0.00003596, -0.00016012,\n",
       "        -0.00528569, -0.00001164,  0.00215648,  0.0070956 ,  0.00119126,\n",
       "        -0.00887778, -0.00106456,  0.00170023, -0.00279975, -0.00283595,\n",
       "         0.003252  , -1.77480858, -4.77428308,  4.27967873,  0.0151664 ,\n",
       "        -0.00000036, -0.00154294, -0.02760688,  0.04582576,  0.05022008,\n",
       "         0.02432753, -0.05609619,  0.02527831,  0.00024698, -0.04321845,\n",
       "        -2.49640003, -4.3516359 , -4.16103753,  2.20891882, -1.12339471,\n",
       "         3.98951172,  1.70983972,  0.35535485, -0.02124986,  0.05250033,\n",
       "         0.30754756,  0.06822469, -0.09720763, -0.01102823, -0.00094223,\n",
       "         0.15614623,  0.44453505]),\n",
       " array([-0.00115316, -0.0126885 , -0.19516572, -0.0023245 , -0.00095854,\n",
       "        -0.02189507,  0.00001516, -0.00007599,  0.00003596, -0.00016012,\n",
       "        -0.00528563, -0.00001164,  0.00215647,  0.00709554,  0.00119125,\n",
       "        -0.00887779, -0.00106457,  0.00170023, -0.00279921, -0.00283601,\n",
       "         0.00325151, -1.7735876 , -4.7742968 ,  4.27847117,  0.01516547,\n",
       "        -0.00000036, -0.00154279, -0.02760697,  0.04582409,  0.05021969,\n",
       "         0.0243275 , -0.05609248,  0.02527626,  0.00024549, -0.04321676,\n",
       "        -2.49612267, -4.35162719, -4.16104132,  2.20876248, -1.12330777,\n",
       "         3.9893649 ,  1.709762  ,  0.35535542, -0.02124865,  0.0524991 ,\n",
       "         0.30754691,  0.0682231 , -0.0972088 , -0.01102825, -0.00094221,\n",
       "         0.15614563,  0.44453186]),\n",
       " array([-0.00115279, -0.0126886 , -0.19516465, -0.00232444, -0.00095853,\n",
       "        -0.02189529,  0.00001516, -0.00007595,  0.00003596, -0.00016012,\n",
       "        -0.00528556, -0.00001164,  0.00215645,  0.00709549,  0.00119124,\n",
       "        -0.00887779, -0.00106457,  0.00170023, -0.00279863, -0.00283607,\n",
       "         0.00325098, -1.77227912, -4.77431149,  4.27717707,  0.01516447,\n",
       "        -0.00000036, -0.00154262, -0.02760706,  0.0458223 ,  0.05021928,\n",
       "         0.02432747, -0.0560885 ,  0.02527407,  0.0002439 , -0.04321494,\n",
       "        -2.49582543, -4.35161785, -4.16104537,  2.20859494, -1.12321459,\n",
       "         3.98920755,  1.70967871,  0.35535603, -0.02124735,  0.05249777,\n",
       "         0.30754622,  0.06822139, -0.09721006, -0.01102828, -0.00094218,\n",
       "         0.15614498,  0.44452845]),\n",
       " array([-0.00115238, -0.0126887 , -0.1951635 , -0.00232439, -0.00095852,\n",
       "        -0.02189553,  0.00001515, -0.0000759 ,  0.00003596, -0.00016012,\n",
       "        -0.00528549, -0.00001164,  0.00215644,  0.00709543,  0.00119124,\n",
       "        -0.0088778 , -0.00106457,  0.00170022, -0.00279801, -0.00283614,\n",
       "         0.00325042, -1.77087687, -4.77432724,  4.27579023,  0.0151634 ,\n",
       "        -0.00000036, -0.00154244, -0.02760715,  0.04582038,  0.05021884,\n",
       "         0.02432743, -0.05608423,  0.02527172,  0.00024219, -0.043213  ,\n",
       "        -2.49550689, -4.35160785, -4.16104971,  2.20841539, -1.12311474,\n",
       "         3.98903893,  1.70958946,  0.35535669, -0.02124596,  0.05249634,\n",
       "         0.30754548,  0.06821956, -0.0972114 , -0.01102831, -0.00094216,\n",
       "         0.15614428,  0.44452479]),\n",
       " array([-0.00115195, -0.0126888 , -0.19516226, -0.00232433, -0.00095851,\n",
       "        -0.02189579,  0.00001515, -0.00007585,  0.00003596, -0.00016011,\n",
       "        -0.00528542, -0.00001164,  0.00215642,  0.00709536,  0.00119123,\n",
       "        -0.00887781, -0.00106457,  0.00170022, -0.00279734, -0.00283622,\n",
       "         0.00324982, -1.76937411, -4.77434411,  4.27430399,  0.01516226,\n",
       "        -0.00000036, -0.00154224, -0.02760726,  0.04581833,  0.05021837,\n",
       "         0.02432739, -0.05607966,  0.0252692 ,  0.00024035, -0.04321091,\n",
       "        -2.49516552, -4.35159713, -4.16105436,  2.20822297, -1.12300773,\n",
       "         3.98885822,  1.7094938 ,  0.3553574 , -0.02124447,  0.05249482,\n",
       "         0.30754469,  0.0682176 , -0.09721284, -0.01102834, -0.00094213,\n",
       "         0.15614354,  0.44452088]),\n",
       " array([-0.00115149, -0.01268892, -0.19516094, -0.00232426, -0.00095849,\n",
       "        -0.02189607,  0.00001515, -0.00007579,  0.00003596, -0.00016011,\n",
       "        -0.00528534, -0.00001164,  0.0021564 ,  0.00709529,  0.00119122,\n",
       "        -0.00887782, -0.00106457,  0.00170021, -0.00279663, -0.0028363 ,\n",
       "         0.00324917, -1.76776366, -4.7743622 ,  4.27271124,  0.01516103,\n",
       "        -0.00000036, -0.00154204, -0.02760737,  0.04581613,  0.05021786,\n",
       "         0.02432735, -0.05607476,  0.0252665 ,  0.00023839, -0.04320868,\n",
       "        -2.49479968, -4.35158564, -4.16105935,  2.20801676, -1.12289305,\n",
       "         3.98866457,  1.7093913 ,  0.35535815, -0.02124288,  0.05249318,\n",
       "         0.30754384,  0.0682155 , -0.09721438, -0.01102838, -0.00094209,\n",
       "         0.15614274,  0.44451668]),\n",
       " array([-0.00115099, -0.01268904, -0.19515953, -0.00232419, -0.00095848,\n",
       "        -0.02189636,  0.00001515, -0.00007574,  0.00003596, -0.00016011,\n",
       "        -0.00528525, -0.00001164,  0.00215638,  0.00709522,  0.00119121,\n",
       "        -0.00887783, -0.00106457,  0.00170021, -0.00279586, -0.00283639,\n",
       "         0.00324847, -1.76603779, -4.77438158,  4.27100434,  0.01515972,\n",
       "        -0.00000036, -0.00154181, -0.02760748,  0.04581377,  0.05021732,\n",
       "         0.0243273 , -0.0560695 ,  0.02526361,  0.00023629, -0.04320628,\n",
       "        -2.49440763, -4.35157333, -4.1610647 ,  2.20779577, -1.12277016,\n",
       "         3.98845703,  1.70928144,  0.35535897, -0.02124117,  0.05249143,\n",
       "         0.30754293,  0.06821325, -0.09721603, -0.01102841, -0.00094206,\n",
       "         0.15614188,  0.44451217]),\n",
       " array([-0.00115045, -0.01268917, -0.19515801, -0.00232412, -0.00095846,\n",
       "        -0.02189668,  0.00001514, -0.00007567,  0.00003596, -0.00016011,\n",
       "        -0.00528516, -0.00001164,  0.00215636,  0.00709514,  0.0011912 ,\n",
       "        -0.00887784, -0.00106457,  0.0017002 , -0.00279504, -0.00283648,\n",
       "         0.00324773, -1.76418824, -4.77440235,  4.26917511,  0.01515831,\n",
       "        -0.00000036, -0.00154158, -0.02760761,  0.04581124,  0.05021674,\n",
       "         0.02432725, -0.05606387,  0.02526051,  0.00023403, -0.04320372,\n",
       "        -2.49398748, -4.35156013, -4.16107043,  2.20755895, -1.12263845,\n",
       "         3.98823462,  1.70916372,  0.35535983, -0.02123933,  0.05248955,\n",
       "         0.30754196,  0.06821084, -0.0972178 , -0.01102845, -0.00094202,\n",
       "         0.15614097,  0.44450735]),\n",
       " array([-0.00114988, -0.01268931, -0.19515638, -0.00232404, -0.00095845,\n",
       "        -0.02189702,  0.00001514, -0.00007561,  0.00003596, -0.00016011,\n",
       "        -0.00528507, -0.00001164,  0.00215634,  0.00709505,  0.00119119,\n",
       "        -0.00887785, -0.00106457,  0.0017002 , -0.00279416, -0.00283658,\n",
       "         0.00324693, -1.76220612, -4.77442461,  4.26721478,  0.01515679,\n",
       "        -0.00000036, -0.00154132, -0.02760775,  0.04580853,  0.05021611,\n",
       "         0.0243272 , -0.05605784,  0.02525719,  0.00023161, -0.04320097,\n",
       "        -2.49353722, -4.35154599, -4.16107656,  2.20730515, -1.12249731,\n",
       "         3.98799627,  1.70903755,  0.35536076, -0.02123737,  0.05248754,\n",
       "         0.30754092,  0.06820825, -0.0972197 , -0.01102849, -0.00094198,\n",
       "         0.15613998,  0.44450218]),\n",
       " array([-0.00114927, -0.01268946, -0.19515464, -0.00232395, -0.00095843,\n",
       "        -0.02189739,  0.00001514, -0.00007553,  0.00003596, -0.0001601 ,\n",
       "        -0.00528496, -0.00001165,  0.00215632,  0.00709496,  0.00119118,\n",
       "        -0.00887786, -0.00106457,  0.00170019, -0.00279322, -0.00283669,\n",
       "         0.00324608, -1.76008196, -4.77444847,  4.26511396,  0.01515518,\n",
       "        -0.00000036, -0.00154105, -0.02760789,  0.04580563,  0.05021544,\n",
       "         0.02432714, -0.05605138,  0.02525363,  0.00022902, -0.04319802,\n",
       "        -2.49305469, -4.35153084, -4.16108314,  2.20703317, -1.12234605,\n",
       "         3.98774083,  1.70890235,  0.35536176, -0.02123526,  0.05248538,\n",
       "         0.3075398 ,  0.06820548, -0.09722173, -0.01102854, -0.00094194,\n",
       "         0.15613893,  0.44449664]),\n",
       " array([-0.00114861, -0.01268963, -0.19515277, -0.00232386, -0.00095841,\n",
       "        -0.02189778,  0.00001513, -0.00007546,  0.00003596, -0.0001601 ,\n",
       "        -0.00528485, -0.00001165,  0.00215629,  0.00709486,  0.00119117,\n",
       "        -0.00887787, -0.00106457,  0.00170018, -0.00279221, -0.0028368 ,\n",
       "         0.00324516, -1.75780556, -4.77447403,  4.26286258,  0.01515344,\n",
       "        -0.00000036, -0.00154075, -0.02760805,  0.04580252,  0.05021473,\n",
       "         0.02432708, -0.05604445,  0.02524982,  0.00022625, -0.04319486,\n",
       "        -2.49253757, -4.3515146 , -4.16109019,  2.20674169, -1.12218395,\n",
       "         3.98746709,  1.70875745,  0.35536283, -0.02123301,  0.05248307,\n",
       "         0.3075386 ,  0.06820251, -0.09722391, -0.01102858, -0.00094189,\n",
       "         0.1561378 ,  0.44449071]),\n",
       " array([-0.00114791, -0.0126898 , -0.19515077, -0.00232377, -0.00095839,\n",
       "        -0.0218982 ,  0.00001513, -0.00007537,  0.00003596, -0.0001601 ,\n",
       "        -0.00528473, -0.00001165,  0.00215627,  0.00709476,  0.00119116,\n",
       "        -0.00887788, -0.00106457,  0.00170018, -0.00279113, -0.00283692,\n",
       "         0.00324418, -1.75536602, -4.77450143,  4.26044985,  0.01515158,\n",
       "        -0.00000036, -0.00154044, -0.02760822,  0.04579918,  0.05021396,\n",
       "         0.02432702, -0.05603703,  0.02524573,  0.00022327, -0.04319148,\n",
       "        -2.4919834 , -4.3514972 , -4.16109775,  2.20642932, -1.12201024,\n",
       "         3.98717374,  1.70860217,  0.35536398, -0.02123059,  0.05248059,\n",
       "         0.30753731,  0.06819933, -0.09722625, -0.01102864, -0.00094184,\n",
       "         0.15613659,  0.44448434]),\n",
       " array([-0.00114716, -0.01268999, -0.19514862, -0.00232366, -0.00095837,\n",
       "        -0.02189865,  0.00001512, -0.00007528,  0.00003595, -0.0001601 ,\n",
       "        -0.0052846 , -0.00001165,  0.00215624,  0.00709464,  0.00119115,\n",
       "        -0.0088779 , -0.00106457,  0.00170017, -0.00278997, -0.00283705,\n",
       "         0.00324313, -1.75275177, -4.77453072,  4.25786426,  0.01514959,\n",
       "        -0.00000036, -0.0015401 , -0.0276084 ,  0.04579561,  0.05021314,\n",
       "         0.02432694, -0.05602907,  0.02524135,  0.00022008, -0.04318785,\n",
       "        -2.49138945, -4.35147861, -4.16110586,  2.20609454, -1.121824  ,\n",
       "         3.98685932,  1.70843571,  0.3553652 , -0.021228  ,  0.05247794,\n",
       "         0.30753594,  0.06819592, -0.09722875, -0.01102869, -0.00094179,\n",
       "         0.15613529,  0.44447752]),\n",
       " array([-0.00114635, -0.01269018, -0.19514632, -0.00232355, -0.00095835,\n",
       "        -0.02189913,  0.00001512, -0.00007519,  0.00003595, -0.00016009,\n",
       "        -0.00528447, -0.00001165,  0.00215621,  0.00709452,  0.00119113,\n",
       "        -0.00887791, -0.00106457,  0.00170016, -0.00278873, -0.00283719,\n",
       "         0.003242  , -1.74995003, -4.77456218,  4.25509331,  0.01514745,\n",
       "        -0.00000036, -0.00153974, -0.02760859,  0.04579178,  0.05021226,\n",
       "         0.02432687, -0.05602054,  0.02523665,  0.00021667, -0.04318396,\n",
       "        -2.49075299, -4.35145863, -4.16111453,  2.20573579, -1.12162449,\n",
       "         3.9865224 ,  1.70825738,  0.35536652, -0.02122522,  0.05247509,\n",
       "         0.30753446,  0.06819227, -0.09723143, -0.01102875, -0.00094174,\n",
       "         0.15613391,  0.44447021]),\n",
       " array([-0.00114548, -0.0126904 , -0.19514386, -0.00232343, -0.00095833,\n",
       "        -0.02189964,  0.00001512, -0.00007509,  0.00003595, -0.00016009,\n",
       "        -0.00528432, -0.00001166,  0.00215618,  0.00709439,  0.00119112,\n",
       "        -0.00887793, -0.00106458,  0.00170015, -0.00278739, -0.00283734,\n",
       "         0.0032408 , -1.7469475 , -4.77459589,  4.25212377,  0.01514516,\n",
       "        -0.00000036, -0.00153935, -0.0276088 ,  0.04578767,  0.05021131,\n",
       "         0.02432679, -0.05601141,  0.02523162,  0.00021301, -0.0431798 ,\n",
       "        -2.49007092, -4.35143721, -4.16112383,  2.20535134, -1.12141068,\n",
       "         3.98616134,  1.70806626,  0.35536793, -0.02122225,  0.05247204,\n",
       "         0.30753288,  0.06818835, -0.09723431, -0.01102881, -0.00094168,\n",
       "         0.15613242,  0.44446238]),\n",
       " array([-0.00114456, -0.01269063, -0.19514121, -0.0023233 , -0.0009583 ,\n",
       "        -0.0219002 ,  0.00001511, -0.00007498,  0.00003595, -0.00016009,\n",
       "        -0.00528416, -0.00001166,  0.00215614,  0.00709425,  0.0011911 ,\n",
       "        -0.00887794, -0.00106458,  0.00170014, -0.00278597, -0.0028375 ,\n",
       "         0.0032395 , -1.74372979, -4.77463202,  4.24894141,  0.01514271,\n",
       "        -0.00000036, -0.00153894, -0.02760902,  0.04578327,  0.0502103 ,\n",
       "         0.0243267 , -0.05600162,  0.02522623,  0.00020908, -0.04317534,\n",
       "        -2.48933997, -4.35141427, -4.1611338 ,  2.20493932, -1.12118154,\n",
       "         3.9857744 ,  1.70786144,  0.35536944, -0.02121906,  0.05246878,\n",
       "         0.30753118,  0.06818416, -0.09723739, -0.01102888, -0.00094161,\n",
       "         0.15613082,  0.44445399]),\n",
       " array([-0.00114356, -0.01269087, -0.19513838, -0.00232316, -0.00095827,\n",
       "        -0.02190079,  0.0000151 , -0.00007486,  0.00003595, -0.00016008,\n",
       "        -0.00528399, -0.00001166,  0.0021561 ,  0.0070941 ,  0.00119109,\n",
       "        -0.00887796, -0.00106458,  0.00170013, -0.00278444, -0.00283768,\n",
       "         0.00323811, -1.74028147, -4.77467074,  4.24553099,  0.01514008,\n",
       "        -0.00000036, -0.00153849, -0.02760925,  0.04577856,  0.05020922,\n",
       "         0.02432661, -0.05599112,  0.02522045,  0.00020488, -0.04317055,\n",
       "        -2.48855663, -4.35138967, -4.16114448,  2.20449778, -1.12093599,\n",
       "         3.98535973,  1.70764194,  0.35537106, -0.02121564,  0.05246528,\n",
       "         0.30752937,  0.06817966, -0.09724069, -0.01102895, -0.00094154,\n",
       "         0.15612911,  0.444445  ]),\n",
       " array([-0.0011425 , -0.01269113, -0.19513535, -0.00232301, -0.00095824,\n",
       "        -0.02190142,  0.0000151 , -0.00007473,  0.00003595, -0.00016008,\n",
       "        -0.00528381, -0.00001166,  0.00215606,  0.00709394,  0.00119107,\n",
       "        -0.00887798, -0.00106458,  0.00170012, -0.00278277, -0.00283786,\n",
       "         0.0032366 , -1.73658664, -4.77471232,  4.24187681,  0.01513726,\n",
       "        -0.00000036, -0.00153802, -0.02760951,  0.04577351,  0.05020806,\n",
       "         0.02432651, -0.05597987,  0.02521426,  0.00020037, -0.04316543,\n",
       "        -2.48771708, -4.35136314, -4.16115582,  2.20402457, -1.12067295,\n",
       "         3.98491532,  1.70740671,  0.35537279, -0.02121198,  0.05246152,\n",
       "         0.30752742,  0.06817484, -0.09724423, -0.01102903, -0.00094147,\n",
       "         0.15612728,  0.44443536]),\n",
       " array([-0.00114135, -0.01269141, -0.1951321 , -0.00232285, -0.00095821,\n",
       "        -0.0219021 ,  0.00001509, -0.0000746 ,  0.00003595, -0.00016007,\n",
       "        -0.00528362, -0.00001167,  0.00215602,  0.00709377,  0.00119105,\n",
       "        -0.008878  , -0.00106458,  0.00170011, -0.00278101, -0.00283806,\n",
       "         0.00323501, -1.73262639, -4.7747568 ,  4.23796007,  0.01513424,\n",
       "        -0.00000036, -0.00153751, -0.02760978,  0.04576809,  0.05020681,\n",
       "         0.02432641, -0.05596782,  0.02520762,  0.00019554, -0.04315993,\n",
       "        -2.48681744, -4.35133488, -4.16116808,  2.20351747, -1.12039095,\n",
       "         3.98443908,  1.70715462,  0.35537465, -0.02120805,  0.0524575 ,\n",
       "         0.30752533,  0.06816967, -0.09724802, -0.01102911, -0.00094139,\n",
       "         0.15612531,  0.44442504]),\n",
       " array([-0.00114013, -0.01269172, -0.19512861, -0.00232268, -0.00095818,\n",
       "        -0.02190283,  0.00001509, -0.00007446,  0.00003594, -0.00016007,\n",
       "        -0.00528341, -0.00001167,  0.00215597,  0.00709359,  0.00119103,\n",
       "        -0.00887802, -0.00106458,  0.0017001 , -0.00277913, -0.00283827,\n",
       "         0.0032333 , -1.72838232, -4.77480446,  4.23376264,  0.01513101,\n",
       "        -0.00000036, -0.00153696, -0.02761007,  0.04576229,  0.05020548,\n",
       "         0.0243263 , -0.05595491,  0.02520051,  0.00019037, -0.04315404,\n",
       "        -2.48585332, -4.3513046 , -4.16118121,  2.20297403, -1.12008873,\n",
       "         3.98392871,  1.70688447,  0.35537664, -0.02120385,  0.05245319,\n",
       "         0.3075231 ,  0.06816414, -0.09725208, -0.0110292 , -0.0009413 ,\n",
       "         0.15612321,  0.44441397]),\n",
       " array([-0.00113882, -0.01269204, -0.19512488, -0.0023225 , -0.00095814,\n",
       "        -0.02190361,  0.00001508, -0.0000743 ,  0.00003594, -0.00016006,\n",
       "        -0.00528318, -0.00001167,  0.00215592,  0.00709339,  0.00119101,\n",
       "        -0.00887805, -0.00106459,  0.00170009, -0.00277711, -0.0028385 ,\n",
       "         0.00323147, -1.72383408, -4.77485554,  4.22926438,  0.01512754,\n",
       "        -0.00000036, -0.00153638, -0.02761039,  0.04575607,  0.05020404,\n",
       "         0.02432617, -0.05594107,  0.02519289,  0.00018482, -0.04314773,\n",
       "        -2.4848201 , -4.35127214, -4.16119529,  2.20239164, -1.11976486,\n",
       "         3.98338177,  1.70659496,  0.35537878, -0.02119934,  0.05244857,\n",
       "         0.3075207 ,  0.0681582 , -0.09725643, -0.0110293 , -0.00094121,\n",
       "         0.15612095,  0.4444021 ]),\n",
       " array([-0.00113741, -0.01269239, -0.19512088, -0.00232231, -0.0009581 ,\n",
       "        -0.02190445,  0.00001507, -0.00007414,  0.00003594, -0.00016006,\n",
       "        -0.00528295, -0.00001168,  0.00215587,  0.00709318,  0.00119098,\n",
       "        -0.00887807, -0.00106459,  0.00170007, -0.00277495, -0.00283875,\n",
       "         0.0032295 , -1.71895988, -4.77491028,  4.22444375,  0.01512383,\n",
       "        -0.00000036, -0.00153575, -0.02761072,  0.04574941,  0.05020251,\n",
       "         0.02432604, -0.05592623,  0.02518472,  0.00017888, -0.04314097,\n",
       "        -2.48371283, -4.35123736, -4.16121038,  2.20176752, -1.11941777,\n",
       "         3.98279562,  1.7062847 ,  0.35538107, -0.02119451,  0.05244363,\n",
       "         0.30751813,  0.06815185, -0.0972611 , -0.0110294 , -0.00094112,\n",
       "         0.15611854,  0.44438939]),\n",
       " array([-0.00113591, -0.01269276, -0.19511659, -0.0023221 , -0.00095806,\n",
       "        -0.02190535,  0.00001506, -0.00007396,  0.00003594, -0.00016005,\n",
       "        -0.00528269, -0.00001168,  0.00215581,  0.00709296,  0.00119096,\n",
       "        -0.0088781 , -0.00106459,  0.00170006, -0.00277263, -0.00283901,\n",
       "         0.0032274 , -1.71373636, -4.77496894,  4.21927763,  0.01511984,\n",
       "        -0.00000036, -0.00153507, -0.02761108,  0.04574227,  0.05020087,\n",
       "         0.0243259 , -0.05591034,  0.02517597,  0.00017251, -0.04313373,\n",
       "        -2.48252621, -4.35120009, -4.16122655,  2.20109866, -1.11904581,\n",
       "         3.98216747,  1.7059522 ,  0.35538352, -0.02118933,  0.05243832,\n",
       "         0.30751538,  0.06814503, -0.0972661 , -0.01102951, -0.00094101,\n",
       "         0.15611595,  0.44437577]),\n",
       " array([-0.00113429, -0.01269315, -0.195112  , -0.00232187, -0.00095801,\n",
       "        -0.02190631,  0.00001505, -0.00007377,  0.00003593, -0.00016004,\n",
       "        -0.00528241, -0.00001169,  0.00215575,  0.00709271,  0.00119093,\n",
       "        -0.00887813, -0.00106459,  0.00170004, -0.00277014, -0.00283929,\n",
       "         0.00322515, -1.70813848, -4.7750318 ,  4.21374127,  0.01511558,\n",
       "        -0.00000036, -0.00153435, -0.02761146,  0.04573461,  0.05019911,\n",
       "         0.02432576, -0.0558933 ,  0.02516658,  0.00016568, -0.04312596,\n",
       "        -2.48125454, -4.35116014, -4.16124387,  2.20038187, -1.11864719,\n",
       "         3.9814943 ,  1.70559587,  0.35538615, -0.02118379,  0.05243264,\n",
       "         0.30751243,  0.06813773, -0.09727146, -0.01102962, -0.0009409 ,\n",
       "         0.15611317,  0.44436117]),\n",
       " array([-0.00113256, -0.01269358, -0.19510707, -0.00232163, -0.00095797,\n",
       "        -0.02190734,  0.00001504, -0.00007356,  0.00003593, -0.00016004,\n",
       "        -0.00528212, -0.00001169,  0.00215569,  0.00709246,  0.0011909 ,\n",
       "        -0.00887816, -0.00106459,  0.00170002, -0.00276748, -0.00283959,\n",
       "         0.00322273, -1.70213942, -4.77509917,  4.20780813,  0.015111  ,\n",
       "        -0.00000035, -0.00153358, -0.02761188,  0.04572641,  0.05019722,\n",
       "         0.0243256 , -0.05587504,  0.02515653,  0.00015836, -0.04311764,\n",
       "        -2.47989174, -4.35111734, -4.16126244,  2.19961371, -1.11822   ,\n",
       "         3.98077289,  1.70521401,  0.35538896, -0.02117784,  0.05242655,\n",
       "         0.30750927,  0.06812991, -0.0972772 , -0.01102975, -0.00094078,\n",
       "         0.1561102 ,  0.44434553]),\n",
       " array([-0.00113071, -0.01269404, -0.19510181, -0.00232137, -0.00095791,\n",
       "        -0.02190843,  0.00001503, -0.00007335,  0.00003593, -0.00016003,\n",
       "        -0.00528181, -0.0000117 ,  0.00215562,  0.00709218,  0.00119087,\n",
       "        -0.00887819, -0.0010646 ,  0.00170001, -0.00276462, -0.0028399 ,\n",
       "         0.00322012, -1.69570958, -4.77516968,  4.20144839,  0.0151061 ,\n",
       "        -0.00000035, -0.00153275, -0.02761231,  0.04571763,  0.05019522,\n",
       "         0.02432543, -0.05585548,  0.02514576,  0.00015052, -0.04310872,\n",
       "        -2.47843231, -4.3510728 , -4.16128289,  2.19879083, -1.11776126,\n",
       "         3.98000001,  1.70480502,  0.35539198, -0.02117147,  0.05242001,\n",
       "         0.30750591,  0.06812152, -0.09728336, -0.01102988, -0.00094065,\n",
       "         0.15610701,  0.44432866]),\n",
       " array([-0.00112873, -0.01269453, -0.19509617, -0.0023211 , -0.00095786,\n",
       "        -0.02190961,  0.00001502, -0.00007311,  0.00003593, -0.00016002,\n",
       "        -0.00528147, -0.0000117 ,  0.00215554,  0.00709188,  0.00119083,\n",
       "        -0.00887823, -0.0010646 ,  0.00169999, -0.00276156, -0.00284024,\n",
       "         0.00321735, -1.68881973, -4.77524677,  4.19463408,  0.01510085,\n",
       "        -0.00000035, -0.00153186, -0.02761279,  0.0457082 ,  0.05019306,\n",
       "         0.02432526, -0.05583445,  0.02513418,  0.00014207, -0.04309915,\n",
       "        -2.4768659 , -4.35102398, -4.16130475,  2.19790587, -1.1172696 ,\n",
       "         3.9791731 ,  1.70436593,  0.35539521, -0.02116464,  0.05241302,\n",
       "         0.30750227,  0.06811254, -0.09728995, -0.01103003, -0.00094051,\n",
       "         0.15610358,  0.44431064]),\n",
       " array([-0.0011266 , -0.01269505, -0.19509014, -0.0023208 , -0.0009578 ,\n",
       "        -0.02191087,  0.00001501, -0.00007286,  0.00003592, -0.00016001,\n",
       "        -0.00528111, -0.00001171,  0.00215546,  0.00709156,  0.0011908 ,\n",
       "        -0.00887827, -0.0010646 ,  0.00169996, -0.00275828, -0.00284061,\n",
       "         0.00321437, -1.68143589, -4.77532927,  4.18733112,  0.01509522,\n",
       "        -0.00000035, -0.00153091, -0.02761329,  0.04569809,  0.05019076,\n",
       "         0.02432509, -0.0558119 ,  0.02512177,  0.000133  , -0.0430889 ,\n",
       "        -2.47518703, -4.35097191, -4.16132834,  2.19695704, -1.11674252,\n",
       "         3.97828727,  1.70389535,  0.35539867, -0.02115713,  0.05240532,\n",
       "         0.3074982 ,  0.06810271, -0.09729722, -0.01103018, -0.00094037,\n",
       "         0.15609993,  0.44429135]),\n",
       " array([-0.00112431, -0.01269562, -0.19508366, -0.00232049, -0.00095774,\n",
       "        -0.02191223,  0.000015  , -0.00007259,  0.00003592, -0.00016   ,\n",
       "        -0.00528072, -0.00001171,  0.00215538,  0.00709122,  0.00119076,\n",
       "        -0.00887831, -0.00106461,  0.00169994, -0.00275477, -0.002841  ,\n",
       "         0.00321118, -1.67352376, -4.77541754,  4.1795049 ,  0.01508919,\n",
       "        -0.00000035, -0.00152989, -0.02761383,  0.04568724,  0.05018819,\n",
       "         0.0243247 , -0.0557877 ,  0.02510868,  0.00012325, -0.0430779 ,\n",
       "        -2.47338733, -4.35092047, -4.16135292,  2.19593874, -1.11617788,\n",
       "         3.97733886,  1.70339062,  0.35540238, -0.02114927,  0.05239726,\n",
       "         0.30749402,  0.06809238, -0.0973048 , -0.01103035, -0.00094021,\n",
       "         0.156096  ,  0.44427068]),\n",
       " array([-0.00112187, -0.01269622, -0.19507671, -0.00232015, -0.00095767,\n",
       "        -0.02191368,  0.00001498, -0.0000723 ,  0.00003591, -0.00015999,\n",
       "        -0.0052803 , -0.00001172,  0.00215528,  0.00709085,  0.00119072,\n",
       "        -0.00887835, -0.00106461,  0.00169992, -0.002751  , -0.00284143,\n",
       "         0.00320777, -1.66504394, -4.77551255,  4.17111812,  0.01508272,\n",
       "        -0.00000035, -0.0015288 , -0.02761441,  0.04567565,  0.05018552,\n",
       "         0.02432446, -0.05576187,  0.02509447,  0.00011289, -0.04306614,\n",
       "        -2.4714607 , -4.35086049, -4.1613793 ,  2.19485215, -1.11557372,\n",
       "         3.97631963,  1.70285071,  0.35540636, -0.02114085,  0.05238864,\n",
       "         0.30748954,  0.06808131, -0.09731294, -0.01103053, -0.00094004,\n",
       "         0.15609179,  0.44424855]),\n",
       " array([-0.00111925, -0.01269686, -0.19506926, -0.00231978, -0.0009576 ,\n",
       "        -0.02191524,  0.00001497, -0.00007199,  0.00003591, -0.00015998,\n",
       "        -0.00527986, -0.00001173,  0.00215519,  0.00709046,  0.00119067,\n",
       "        -0.0088784 , -0.00106461,  0.00169989, -0.00274697, -0.00284188,\n",
       "         0.00320411, -1.6559564 , -4.77561428,  4.16213025,  0.0150758 ,\n",
       "        -0.00000035, -0.00152763, -0.02761504,  0.04566322,  0.05018266,\n",
       "         0.02432421, -0.0557342 ,  0.02507925,  0.0001018 , -0.04305353,\n",
       "        -2.46939614, -4.35079649, -4.16140777,  2.19368773, -1.11492609,\n",
       "         3.9752275 ,  1.70227211,  0.35541063, -0.02113183,  0.05237939,\n",
       "         0.30748474,  0.06806944, -0.09732165, -0.01103072, -0.00093986,\n",
       "         0.15608729,  0.44422482]),\n",
       " array([-0.00111644, -0.01269756, -0.19506127, -0.00231939, -0.00095752,\n",
       "        -0.02191691,  0.00001495, -0.00007166,  0.00003591, -0.00015997,\n",
       "        -0.00527938, -0.00001174,  0.00215508,  0.00709004,  0.00119062,\n",
       "        -0.00887845, -0.00106462,  0.00169986, -0.00274265, -0.00284236,\n",
       "         0.00320018, -1.64621756, -4.77572338,  4.15249827,  0.01506837,\n",
       "        -0.00000035, -0.00152638, -0.02761571,  0.0456499 ,  0.0501796 ,\n",
       "         0.02432394, -0.05570454,  0.02506294,  0.0000899 , -0.04304001,\n",
       "        -2.46718349, -4.35072767, -4.16143811,  2.19243982, -1.1142322 ,\n",
       "         3.97405698,  1.70165204,  0.3554152 , -0.02112216,  0.05236949,\n",
       "         0.3074796 ,  0.06805672, -0.09733099, -0.01103092, -0.00093966,\n",
       "         0.15608246,  0.44419941]),\n",
       " array([-0.00111343, -0.0126983 , -0.19505271, -0.00231897, -0.00095743,\n",
       "        -0.0219187 ,  0.00001494, -0.00007131,  0.0000359 , -0.00015996,\n",
       "        -0.00527887, -0.00001174,  0.00215497,  0.00708959,  0.00119057,\n",
       "        -0.00887851, -0.00106462,  0.00169983, -0.00273801, -0.00284289,\n",
       "         0.00319598, -1.63578128, -4.77584016,  4.14217621,  0.01506041,\n",
       "        -0.00000035, -0.00152503, -0.02761642,  0.04563563,  0.05017631,\n",
       "         0.02432364, -0.05567275,  0.02504546,  0.00007716, -0.04302553,\n",
       "        -2.46481182, -4.35065376, -4.1614704 ,  2.19110233, -1.11348855,\n",
       "         3.97280223,  1.70098734,  0.35542009, -0.0211118 ,  0.05235887,\n",
       "         0.30747409,  0.06804309, -0.097341  , -0.01103114, -0.00093945,\n",
       "         0.15607728,  0.44417214]),\n",
       " array([-0.0011102 , -0.01269909, -0.19504354, -0.00231853, -0.00095734,\n",
       "        -0.02192061,  0.00001492, -0.00007093,  0.0000359 , -0.00015995,\n",
       "        -0.00527832, -0.00001175,  0.00215484,  0.0070891 ,  0.00119052,\n",
       "        -0.00887856, -0.00106463,  0.0016998 , -0.00273305, -0.00284344,\n",
       "         0.00319147, -1.62459655, -4.77596546,  4.13111417,  0.01505189,\n",
       "        -0.00000035, -0.00152359, -0.02761719,  0.04562033,  0.05017279,\n",
       "         0.02432333, -0.05563869,  0.02502673,  0.0000635 , -0.04301001,\n",
       "        -2.4622706 , -4.35057466, -4.16150521,  2.18966912, -1.11269165,\n",
       "         3.97145787,  1.70027519,  0.35542534, -0.0211007 ,  0.05234749,\n",
       "         0.30746818,  0.06802848, -0.09735172, -0.01103137, -0.00093923,\n",
       "         0.15607174,  0.44414295]),\n",
       " array([-0.00110674, -0.01269995, -0.19503371, -0.00231804, -0.00095725,\n",
       "        -0.02192267,  0.0000149 , -0.00007052,  0.00003589, -0.00015993,\n",
       "        -0.00527773, -0.00001176,  0.00215471,  0.00708858,  0.00119046,\n",
       "        -0.00887863, -0.00106463,  0.00169977, -0.00272772, -0.00284404,\n",
       "         0.00318664, -1.61261023, -4.77609974,  4.11925934,  0.01504275,\n",
       "        -0.00000035, -0.00152205, -0.02761801,  0.04560394,  0.05016902,\n",
       "         0.024323  , -0.05560219,  0.02500665,  0.00004886, -0.04299338,\n",
       "        -2.45954727, -4.35048989, -4.16154251,  2.1881332 , -1.11183765,\n",
       "         3.97001717,  1.69951201,  0.35543096, -0.0210888 ,  0.0523353 ,\n",
       "         0.30746185,  0.06801283, -0.09736322, -0.01103162, -0.00093899,\n",
       "         0.15606579,  0.44411167]),\n",
       " array([-0.00110304, -0.01270086, -0.19502317, -0.00231753, -0.00095714,\n",
       "        -0.02192487,  0.00001488, -0.00007009,  0.00003588, -0.00015992,\n",
       "        -0.0052771 , -0.00001177,  0.00215457,  0.00708803,  0.0011904 ,\n",
       "        -0.00887869, -0.00106464,  0.00169973, -0.00272202, -0.00284468,\n",
       "         0.00318147, -1.59976487, -4.77624365,  4.1065549 ,  0.01503296,\n",
       "        -0.00000035, -0.00152039, -0.02761889,  0.04558637,  0.05016497,\n",
       "         0.02432264, -0.05556307,  0.02498513,  0.00003317, -0.04297555,\n",
       "        -2.45662876, -4.35039905, -4.16158248,  2.18648721, -1.11092244,\n",
       "         3.96847322,  1.69869413,  0.35543699, -0.02107604,  0.05232223,\n",
       "         0.30745506,  0.06799605, -0.09737554, -0.01103189, -0.00093873,\n",
       "         0.15605942,  0.44407814]),\n",
       " array([-0.00109907, -0.01270184, -0.19501188, -0.00231698, -0.00095703,\n",
       "        -0.02192723,  0.00001486, -0.00006962,  0.00003588, -0.0001599 ,\n",
       "        -0.00527643, -0.00001178,  0.00215442,  0.00708743,  0.00119033,\n",
       "        -0.00887877, -0.00106464,  0.00169969, -0.00271591, -0.00284537,\n",
       "         0.00317592, -1.58599893, -4.77639787,  4.09293996,  0.01502246,\n",
       "        -0.00000035, -0.00151862, -0.02761984,  0.04556755,  0.05016064,\n",
       "         0.02432225, -0.05552114,  0.02496208,  0.00001636, -0.04295645,\n",
       "        -2.45350108, -4.3503017 , -4.16162531,  2.18472325, -1.10994163,\n",
       "         3.96681862,  1.69781763,  0.35544345, -0.02106238,  0.05230822,\n",
       "         0.30744779,  0.06797807, -0.09738874, -0.01103218, -0.00093846,\n",
       "         0.1560526 ,  0.44404221]),\n",
       " array([-0.00109482, -0.01270289, -0.19499978, -0.00231639, -0.00095692,\n",
       "        -0.02192976,  0.00001483, -0.00006912,  0.00003587, -0.00015989,\n",
       "        -0.0052757 , -0.00001179,  0.00215426,  0.0070868 ,  0.00119026,\n",
       "        -0.00887884, -0.00106465,  0.00169965, -0.00270936, -0.00284611,\n",
       "         0.00316998, -1.57124641, -4.77656315,  4.07834928,  0.01501122,\n",
       "        -0.00000035, -0.00151672, -0.02762085,  0.04554738,  0.050156  ,\n",
       "         0.02432184, -0.05547621,  0.02493737, -0.00000098, -0.04293629,\n",
       "        -2.45016653, -4.35017776, -4.16166605,  2.18282748, -1.10888615,\n",
       "         3.96504035,  1.69688329,  0.35545071, -0.02104774,  0.0522932 ,\n",
       "         0.30744003,  0.06795887, -0.09740292, -0.0110325 , -0.00093818,\n",
       "         0.15604506,  0.44400101]),\n",
       " array([-0.00109017, -0.0127041 , -0.1949869 , -0.00231575, -0.00095679,\n",
       "        -0.02193247,  0.00001481, -0.00006858,  0.00003586, -0.00015987,\n",
       "        -0.00527495, -0.00001181,  0.00215409,  0.00708611,  0.00119018,\n",
       "        -0.00887893, -0.00106465,  0.0016996 , -0.00270234, -0.00284689,\n",
       "         0.0031636 , -1.55543948, -4.7767428 ,  4.06271562,  0.01499916,\n",
       "        -0.00000035, -0.00151468, -0.02762194,  0.04552569,  0.05015075,\n",
       "         0.02432146, -0.05542813,  0.02491092, -0.00001955, -0.04291469,\n",
       "        -2.44659502, -4.35003763, -4.16170973,  2.1807958 , -1.10775453,\n",
       "         3.9631345 ,  1.69588244,  0.35545837, -0.02103206,  0.0522771 ,\n",
       "         0.30743179,  0.06793826, -0.09741811, -0.01103284, -0.00093789,\n",
       "         0.156037  ,  0.44395753]),\n",
       " array([-0.00108517, -0.01270541, -0.19497308, -0.00231508, -0.00095665,\n",
       "        -0.02193536,  0.00001478, -0.000068  ,  0.00003586, -0.00015985,\n",
       "        -0.00527414, -0.00001182,  0.00215391,  0.00708537,  0.0011901 ,\n",
       "        -0.00887901, -0.00106466,  0.00169955, -0.00269481, -0.00284774,\n",
       "         0.00315678, -1.53849731, -4.77693672,  4.0459628 ,  0.01498624,\n",
       "        -0.00000035, -0.0015125 , -0.02762311,  0.04550242,  0.050145  ,\n",
       "         0.02432106, -0.0553766 ,  0.02488258, -0.00003941, -0.04289156,\n",
       "        -2.44276844, -4.34988652, -4.16175605,  2.17861859, -1.10654226,\n",
       "         3.96109161,  1.69481024,  0.35546654, -0.02101525,  0.05225986,\n",
       "         0.30742299,  0.06791615, -0.09743438, -0.01103321, -0.00093758,\n",
       "         0.15602835,  0.44391091]),\n",
       " array([-0.00107981, -0.01270682, -0.19495828, -0.00231435, -0.00095651,\n",
       "        -0.02193846,  0.00001475, -0.00006739,  0.00003585, -0.00015983,\n",
       "        -0.00527327, -0.00001183,  0.00215371,  0.00708457,  0.00119001,\n",
       "        -0.00887911, -0.00106467,  0.0016995 , -0.00268675, -0.00284865,\n",
       "         0.00314946, -1.5203403 , -4.77714524,  4.02801035,  0.01497239,\n",
       "        -0.00000035, -0.00151016, -0.02762436,  0.04547746,  0.05013883,\n",
       "         0.02432064, -0.05532139,  0.02485221, -0.00006064, -0.04286679,\n",
       "        -2.43866811, -4.34972474, -4.16180546,  2.17628534, -1.10524316,\n",
       "         3.95890174,  1.69366142,  0.35547535, -0.02099675,  0.05224087,\n",
       "         0.30741314,  0.06789198, -0.0974523 , -0.01103361, -0.00093725,\n",
       "         0.1560191 ,  0.44386081]),\n",
       " array([-0.00107395, -0.01270843, -0.19494245, -0.00231357, -0.00095635,\n",
       "        -0.02194176,  0.00001472, -0.00006672,  0.00003584, -0.0001598 ,\n",
       "        -0.00527236, -0.00001185,  0.0021535 ,  0.00708371,  0.00118991,\n",
       "        -0.00887921, -0.00106468,  0.00169945, -0.00267811, -0.00284962,\n",
       "         0.00314162, -1.50087928, -4.77737712,  4.00877951,  0.01495754,\n",
       "        -0.00000035, -0.00150765, -0.02762571,  0.04545054,  0.05013184,\n",
       "         0.02432031, -0.05526231,  0.0248197 , -0.00008252, -0.04284064,\n",
       "        -2.43429827, -4.34952259, -4.16185285,  2.17378089, -1.1038477 ,\n",
       "         3.95654942,  1.6924375 ,  0.35548512, -0.02097743,  0.05222103,\n",
       "         0.30740321,  0.06786656, -0.09747101, -0.01103404, -0.00093693,\n",
       "         0.15600891,  0.44380456]),\n",
       " array([-0.00106778, -0.01271006, -0.19492542, -0.00231273, -0.00095619,\n",
       "        -0.02194532,  0.00001469, -0.00006602,  0.00003583, -0.00015978,\n",
       "        -0.00527137, -0.00001186,  0.00215327,  0.0070828 ,  0.00118981,\n",
       "        -0.00887932, -0.00106468,  0.00169939, -0.00266885, -0.00285066,\n",
       "         0.00313322, -1.48002434, -4.77762031,  3.98816507,  0.01494164,\n",
       "        -0.00000035, -0.00150496, -0.02762715,  0.04542181,  0.05012475,\n",
       "         0.02431987, -0.05519893,  0.02478483, -0.00010675, -0.04281226,\n",
       "        -2.42959391, -4.34933314, -4.16190939,  2.17110214, -1.10235652,\n",
       "         3.95403436,  1.69111971,  0.35549532, -0.02095672,  0.05219976,\n",
       "         0.30739239,  0.0678393 , -0.09749106, -0.0110345 , -0.00093655,\n",
       "         0.15599823,  0.44374648]),\n",
       " array([-0.00106102, -0.0127119 , -0.19490718, -0.00231184, -0.00095601,\n",
       "        -0.0219491 ,  0.00001465, -0.00006526,  0.00003582, -0.00015976,\n",
       "        -0.00527033, -0.00001188,  0.00215303,  0.00708181,  0.0011897 ,\n",
       "        -0.00887944, -0.00106469,  0.00169932, -0.00265893, -0.00285177,\n",
       "         0.00312421, -1.45767182, -4.77789054,  3.96608315,  0.01492459,\n",
       "        -0.00000035, -0.00150209, -0.02762869,  0.04539079,  0.05011675,\n",
       "         0.02431952, -0.0551311 ,  0.02474751, -0.00013171, -0.04278228,\n",
       "        -2.42457894, -4.34909843, -4.16196487,  2.16822841, -1.10075564,\n",
       "         3.95133341,  1.68971568,  0.35550656, -0.02093454,  0.05217696,\n",
       "         0.30738096,  0.06781011, -0.0975125 , -0.01103498, -0.00093618,\n",
       "         0.15598653,  0.44368138]),\n",
       " array([-0.00105375, -0.01271388, -0.1948876 , -0.00231088, -0.00095582,\n",
       "        -0.02195315,  0.00001461, -0.00006444,  0.00003581, -0.00015973,\n",
       "        -0.00526921, -0.00001189,  0.00215277,  0.00708075,  0.00118959,\n",
       "        -0.00887957, -0.0010647 ,  0.00169926, -0.00264829, -0.00285296,\n",
       "         0.00311456, -1.4337166 , -4.77818255,  3.94242094,  0.01490631,\n",
       "        -0.00000035, -0.001499  , -0.02763035,  0.04535746,  0.05010815,\n",
       "         0.02431918, -0.05505841,  0.02470752, -0.00015834, -0.0427502 ,\n",
       "        -2.41920647, -4.34884315, -4.16202418,  2.16515024, -1.09904192,\n",
       "         3.9484387 ,  1.68821223,  0.3555186 , -0.02091078,  0.05215252,\n",
       "         0.30736867,  0.06777882, -0.09753547, -0.01103549, -0.00093578,\n",
       "         0.15597399,  0.44361134]),\n",
       " array([-0.00104599, -0.01271597, -0.1948666 , -0.00230985, -0.00095562,\n",
       "        -0.02195748,  0.00001457, -0.00006357,  0.0000358 , -0.0001597 ,\n",
       "        -0.00526801, -0.00001191,  0.00215249,  0.00707962,  0.00118946,\n",
       "        -0.00887971, -0.00106472,  0.00169918, -0.0026369 , -0.00285424,\n",
       "         0.00310421, -1.40804651, -4.77849282,  3.91706114,  0.01488673,\n",
       "        -0.00000035, -0.00149569, -0.02763213,  0.04532177,  0.05009905,\n",
       "         0.02431878, -0.05498048,  0.02466465, -0.00018716, -0.04271569,\n",
       "        -2.41343982, -4.34857932, -4.16208956,  2.16185338, -1.09720719,\n",
       "         3.94533804,  1.68659849,  0.35553137, -0.02088532,  0.05212632,\n",
       "         0.30735544,  0.06774528, -0.09756007, -0.01103604, -0.00093534,\n",
       "         0.15596065,  0.44353711]),\n",
       " array([-0.00103764, -0.01271822, -0.19484406, -0.00230875, -0.0009554 ,\n",
       "        -0.02196212,  0.00001453, -0.00006263,  0.00003579, -0.00015967,\n",
       "        -0.00526673, -0.00001193,  0.00215219,  0.0070784 ,  0.00118933,\n",
       "        -0.00887986, -0.00106473,  0.00169911, -0.00262469, -0.00285561,\n",
       "         0.00309313, -1.3805346 , -4.77882879,  3.88988642,  0.01486575,\n",
       "        -0.00000035, -0.00149215, -0.02763403,  0.04528341,  0.05008922,\n",
       "         0.0243184 , -0.054897  ,  0.02461872, -0.00021776, -0.04267882,\n",
       "        -2.40726734, -4.34828915, -4.16215939,  2.15832139, -1.09524189,\n",
       "         3.94201503,  1.6848722 ,  0.35554513, -0.02085803,  0.05209825,\n",
       "         0.30734127,  0.06770934, -0.09758641, -0.01103662, -0.00093487,\n",
       "         0.15594629,  0.44345695]),\n",
       " array([-0.00102851, -0.01272073, -0.19481988, -0.00230757, -0.00095517,\n",
       "        -0.02196705,  0.00001448, -0.00006163,  0.00003577, -0.00015963,\n",
       "        -0.00526538, -0.00001195,  0.00215187,  0.00707709,  0.00118919,\n",
       "        -0.00888002, -0.00106474,  0.00169902, -0.0026116 , -0.00285707,\n",
       "         0.00308124, -1.35104676, -4.77920021,  3.86077533,  0.01484325,\n",
       "        -0.00000035, -0.00148835, -0.02763608,  0.04524188,  0.05007828,\n",
       "         0.02431817, -0.05480765,  0.02456956, -0.00024937, -0.0426398 ,\n",
       "        -2.40068109, -4.34794634, -4.16223119,  2.15453753, -1.09313641,\n",
       "         3.93844974,  1.68303311,  0.35556013, -0.02082804,  0.05206733,\n",
       "         0.30732547,  0.06767004, -0.09761535, -0.01103724, -0.0009344 ,\n",
       "         0.1559307 ,  0.4433682 ]),\n",
       " array([-0.00101881, -0.01272335, -0.19479392, -0.00230631, -0.00095492,\n",
       "        -0.02197235,  0.00001443, -0.00006056,  0.00003576, -0.0001596 ,\n",
       "        -0.00526393, -0.00001198,  0.00215153,  0.00707569,  0.00118904,\n",
       "        -0.0088802 , -0.00106476,  0.00169893, -0.00259758, -0.00285864,\n",
       "         0.00306851, -1.31944832, -4.77959248,  3.82957241,  0.01481915,\n",
       "        -0.00000035, -0.00148428, -0.02763827,  0.04519749,  0.05006682,\n",
       "         0.02431786, -0.05471184,  0.02451683, -0.00028384, -0.04259772,\n",
       "        -2.39360597, -4.34759811, -4.16231072,  2.15048454, -1.09088251,\n",
       "         3.93463203,  1.68105729,  0.35557601, -0.02079667,  0.05203502,\n",
       "         0.30730916,  0.0676287 , -0.09764559, -0.0110379 , -0.00093387,\n",
       "         0.15591411,  0.4432749 ]),\n",
       " array([-0.00100838, -0.01272617, -0.19476608, -0.00230495, -0.00095466,\n",
       "        -0.02197801,  0.00001437, -0.00005941,  0.00003574, -0.00015956,\n",
       "        -0.00526237, -0.000012  ,  0.00215116,  0.00707419,  0.00118888,\n",
       "        -0.00888039, -0.00106477,  0.00169884, -0.00258255, -0.00286032,\n",
       "         0.00305486, -1.28558662, -4.78001352,  3.79613515,  0.01479332,\n",
       "        -0.00000035, -0.00147992, -0.02764061,  0.04514979,  0.0500545 ,\n",
       "         0.02431753, -0.05460915,  0.02446034, -0.0003206 , -0.04255267,\n",
       "        -2.38602521, -4.3472223 , -4.16239559,  2.14614225, -1.08846841,\n",
       "         3.93053974,  1.67894131,  0.35559301, -0.02076306,  0.05200039,\n",
       "         0.3072917 ,  0.06758439, -0.09767798, -0.0110386 , -0.0009333 ,\n",
       "         0.15589632,  0.44317471]),\n",
       " array([-0.00099695, -0.01272931, -0.19473614, -0.0023035 , -0.00095438,\n",
       "        -0.02198401,  0.00001431, -0.00005817,  0.00003573, -0.00015952,\n",
       "        -0.00526074, -0.00001203,  0.00215076,  0.00707257,  0.0011887 ,\n",
       "        -0.00888059, -0.00106479,  0.00169873, -0.00256645, -0.00286212,\n",
       "         0.00304023, -1.24929228, -4.78047969,  3.76031571,  0.01476563,\n",
       "        -0.00000035, -0.00147525, -0.02764313,  0.04509797,  0.05004084,\n",
       "         0.02431746, -0.05449928,  0.02439987, -0.00035832, -0.04250508,\n",
       "        -2.37793954, -4.34678031, -4.16248399,  2.14149358, -1.08588497,\n",
       "         3.92615055,  1.67669047,  0.35561152, -0.02072709,  0.05196326,\n",
       "         0.3072731 ,  0.06753689, -0.09771258, -0.01103935, -0.00093273,\n",
       "         0.15587694,  0.44306401]),\n",
       " array([-0.00098466, -0.01273267, -0.194704  , -0.00230194, -0.00095407,\n",
       "        -0.02199043,  0.00001425, -0.00005685,  0.00003571, -0.00015947,\n",
       "        -0.00525901, -0.00001205,  0.00215034,  0.00707083,  0.00118852,\n",
       "        -0.00888081, -0.00106481,  0.00169862, -0.00254919, -0.00286405,\n",
       "         0.00302455, -1.21039658, -4.78098017,  3.72192975,  0.01473595,\n",
       "        -0.00000035, -0.00147024, -0.02764583,  0.04504217,  0.05002622,\n",
       "         0.02431744, -0.05438154,  0.02433508, -0.00039845, -0.04245417,\n",
       "        -2.36927716, -4.34630445, -4.16257892,  2.13651502, -1.08312033,\n",
       "         3.92144688,  1.67428237,  0.35563134, -0.02068855,  0.05192346,\n",
       "         0.30725316,  0.06748596, -0.09774963, -0.01104015, -0.00093211,\n",
       "         0.15585615,  0.44294556]),\n",
       " array([-0.00097157, -0.01273622, -0.19466961, -0.00230027, -0.00095375,\n",
       "        -0.02199731,  0.00001419, -0.00005543,  0.00003569, -0.00015942,\n",
       "        -0.00525713, -0.00001208,  0.00214989,  0.00706898,  0.00118832,\n",
       "        -0.00888104, -0.00106483,  0.0016985 , -0.0025307 , -0.00286612,\n",
       "         0.00300775, -1.16871627, -4.78150763,  3.68078444,  0.01470416,\n",
       "        -0.00000035, -0.00146487, -0.02764872,  0.04498251,  0.05001081,\n",
       "         0.0243173 , -0.0542553 ,  0.02426561, -0.00044206, -0.04239933,\n",
       "        -2.35997595, -4.34581632, -4.16268349,  2.13117993, -1.08015773,\n",
       "         3.91640862,  1.67169637,  0.35565239, -0.0206461 ,  0.05187968,\n",
       "         0.3072307 ,  0.06743027, -0.09779047, -0.01104101, -0.00093143,\n",
       "         0.15583409,  0.4428206 ]),\n",
       " array([-0.00095721, -0.01274016, -0.19463257, -0.00229849, -0.00095341,\n",
       "        -0.02200459,  0.00001411, -0.00005391,  0.00003567, -0.00015937,\n",
       "        -0.00525518, -0.00001211,  0.0021494 ,  0.00706699,  0.00118812,\n",
       "        -0.00888129, -0.00106485,  0.00169837, -0.00251088, -0.00286832,\n",
       "         0.00298973, -1.12404316, -4.78208949,  3.6367068 ,  0.01467008,\n",
       "        -0.00000035, -0.00145912, -0.02765183,  0.04491741,  0.04999381,\n",
       "         0.02431754, -0.05412023,  0.02419128, -0.00048647, -0.04234146,\n",
       "        -2.35005591, -4.34524764, -4.16279367,  2.12547199, -1.07699075,\n",
       "         3.911006  ,  1.6689497 ,  0.35567523, -0.02060181,  0.05183389,\n",
       "         0.30720778,  0.06737165, -0.09783297, -0.01104192, -0.00093074,\n",
       "         0.15580999,  0.44268326]),\n",
       " array([-0.00094184, -0.01274432, -0.19459303, -0.00229657, -0.00095304,\n",
       "        -0.02201236,  0.00001404, -0.00005228,  0.00003565, -0.00015932,\n",
       "        -0.00525309, -0.00001215,  0.00214888,  0.00706485,  0.00118789,\n",
       "        -0.00888156, -0.00106487,  0.00169823, -0.00248964, -0.0028707 ,\n",
       "         0.00297044, -1.0761754 , -4.78270221,  3.58945813,  0.01463354,\n",
       "        -0.00000035, -0.00145295, -0.02765514,  0.04484766,  0.04997535,\n",
       "         0.02431655, -0.05397474,  0.02411257, -0.00053521, -0.04227908,\n",
       "        -2.33939442, -4.34468834, -4.16291351,  2.11932656, -1.07359157,\n",
       "         3.9052359 ,  1.66599546,  0.35569944, -0.02055431,  0.05178482,\n",
       "         0.30718317,  0.06730882, -0.09787855, -0.0110429 , -0.00092998,\n",
       "         0.15578428,  0.4425383 ]),\n",
       " array([-0.00092528, -0.0127488 , -0.19455051, -0.00229452, -0.00095265,\n",
       "        -0.02202065,  0.00001395, -0.00005054,  0.00003562, -0.00015926,\n",
       "        -0.00525086, -0.00001219,  0.00214833,  0.00706256,  0.00118765,\n",
       "        -0.00888185, -0.00106489,  0.00169808, -0.00246688, -0.00287324,\n",
       "         0.00294976, -1.02487039, -4.78336255,  3.53882618,  0.0145944 ,\n",
       "        -0.00000034, -0.00144635, -0.0276587 ,  0.04477246,  0.04995602,\n",
       "         0.02431675, -0.05381958,  0.02402728, -0.00058601, -0.04221264,\n",
       "        -2.32799603, -4.34405248, -4.1630447 ,  2.11277484, -1.06995856,\n",
       "         3.89903648,  1.6628467 ,  0.35572548, -0.02050215,  0.05173094,\n",
       "         0.30715562,  0.06724018, -0.0979286 , -0.01104396, -0.00092917,\n",
       "         0.1557567 ,  0.44238283]),\n",
       " array([-0.00090734, -0.01275367, -0.19450477, -0.00229232, -0.00095223,\n",
       "        -0.02202949,  0.00001387, -0.00004867,  0.0000356 , -0.0001592 ,\n",
       "        -0.00524852, -0.00001222,  0.00214773,  0.00706011,  0.0011874 ,\n",
       "        -0.00888215, -0.00106492,  0.00169792, -0.0024425 , -0.00287596,\n",
       "         0.00292759, -0.96988711, -4.78407696,  3.4845737 ,  0.01455244,\n",
       "        -0.00000034, -0.00143927, -0.02766254,  0.04469079,  0.04993512,\n",
       "         0.02431726, -0.05365345,  0.02393594, -0.00063879, -0.04214202,\n",
       "        -2.31580651, -4.34334575, -4.1631839 ,  2.10576208, -1.06607519,\n",
       "         3.89239075,  1.65949107,  0.35575349, -0.02044746,  0.0516744 ,\n",
       "         0.30712722,  0.06716771, -0.097981  , -0.01104509, -0.00092832,\n",
       "         0.15572685,  0.44221514]),\n",
       " array([-0.00088796, -0.01275892, -0.19445571, -0.00228996, -0.00095178,\n",
       "        -0.02203888,  0.00001377, -0.00004666,  0.00003557, -0.00015913,\n",
       "        -0.00524606, -0.00001227,  0.00214709,  0.00705748,  0.00118712,\n",
       "        -0.00888248, -0.00106495,  0.00169775, -0.00241636, -0.00287886,\n",
       "         0.00290382, -0.91096555, -4.78484123,  3.42643439,  0.01450748,\n",
       "        -0.00000034, -0.00143168, -0.02766665,  0.04460241,  0.04991268,\n",
       "         0.02431795, -0.0534755 ,  0.0238381 , -0.00069419, -0.04206671,\n",
       "        -2.30275706, -4.34258096, -4.16333517,  2.09825341, -1.06191883,\n",
       "         3.88526895,  1.65590844,  0.35578348, -0.02038899,  0.05161393,\n",
       "         0.30709692,  0.06709011, -0.09803696, -0.0110463 , -0.00092741,\n",
       "         0.15569472,  0.44203556]),\n",
       " array([-0.00086704, -0.01276455, -0.1944031 , -0.00228744, -0.0009513 ,\n",
       "        -0.02204887,  0.00001367, -0.00004452,  0.00003554, -0.00015906,\n",
       "        -0.00524344, -0.00001231,  0.0021464 ,  0.00705466,  0.00118683,\n",
       "        -0.00888283, -0.00106498,  0.00169756, -0.00238835, -0.00288197,\n",
       "         0.00287835, -0.84781855, -4.78565861,  3.36412464,  0.0144593 ,\n",
       "        -0.00000034, -0.00142354, -0.02767106,  0.04450683,  0.04988866,\n",
       "         0.02431885, -0.05328491,  0.02373331, -0.00075243, -0.04198636,\n",
       "        -2.28878558, -4.34175603, -4.16349995,  2.09021394, -1.05747212,\n",
       "         3.87763887,  1.65208428,  0.35581562, -0.02032464,  0.0515474 ,\n",
       "         0.30706287,  0.0670052 , -0.09809856, -0.01104761, -0.00092644,\n",
       "         0.15566026,  0.44184401]),\n",
       " array([-0.00084435, -0.01277067, -0.1943467 , -0.00228473, -0.00095079,\n",
       "        -0.02205933,  0.00001356, -0.00004221,  0.00003551, -0.00015898,\n",
       "        -0.00524073, -0.00001236,  0.00214566,  0.00705164,  0.00118652,\n",
       "        -0.00888319, -0.00106501,  0.00169736, -0.00235782, -0.00288515,\n",
       "         0.00285036, -0.7801468 , -4.78652253,  3.29735105,  0.01440766,\n",
       "        -0.00000034, -0.00141483, -0.02767577,  0.04440263,  0.04986288,\n",
       "         0.02432029, -0.05308098,  0.02362114, -0.00081225, -0.04190115,\n",
       "        -2.27386058, -4.34085588, -4.16368327,  2.08161587, -1.05271307,\n",
       "         3.86946577,  1.64802016,  0.35585009, -0.02025742,  0.05147784,\n",
       "         0.30702822,  0.06691579, -0.09816278, -0.01104901, -0.00092544,\n",
       "         0.15562285,  0.4416377 ]),\n",
       " array([-0.0008198 , -0.0127772 , -0.19428607, -0.00228183, -0.00095024,\n",
       "        -0.02207055,  0.00001344, -0.00003974,  0.00003548, -0.0001589 ,\n",
       "        -0.00523786, -0.00001242,  0.00214487,  0.00704841,  0.00118619,\n",
       "        -0.00888358, -0.00106504,  0.00169715, -0.00232562, -0.00288869,\n",
       "         0.00282104, -0.7076252 , -4.78745901,  3.22579098,  0.01435232,\n",
       "        -0.00000034, -0.00140549, -0.02768087,  0.04428974,  0.04983516,\n",
       "         0.02432195, -0.0528625 ,  0.023501  , -0.00087487, -0.04181026,\n",
       "        -2.25786971, -4.33987649, -4.16388005,  2.07240723, -1.04762845,\n",
       "         3.86070758,  1.64368094,  0.35588698, -0.02018541,  0.05140335,\n",
       "         0.3069908 ,  0.06681987, -0.09823153, -0.01105052, -0.00092435,\n",
       "         0.15558264,  0.44141885]),\n",
       " array([-0.00079341, -0.01278416, -0.19422114, -0.00227871, -0.00094966,\n",
       "        -0.02208254,  0.00001332, -0.0000371 ,  0.00003545, -0.00015881,\n",
       "        -0.00523482, -0.00001247,  0.00214402,  0.00704495,  0.00118583,\n",
       "        -0.00888399, -0.00106507,  0.00169692, -0.00229111, -0.00289251,\n",
       "         0.00278963, -0.62989868, -4.78845773,  3.14908973,  0.01429301,\n",
       "        -0.00000034, -0.00139548, -0.02768635,  0.04416784,  0.04980559,\n",
       "         0.02432395, -0.05262854,  0.0233723 , -0.00094091, -0.04171318,\n",
       "        -2.24074526, -4.33882804, -4.16409685,  2.06254754, -1.04218898,\n",
       "         3.85132947,  1.6390486 ,  0.35592649, -0.02010614,  0.0513214 ,\n",
       "         0.3069487 ,  0.06671492, -0.09830728, -0.01105214, -0.00092317,\n",
       "         0.15553955,  0.44118656]),\n",
       " array([-0.000765  , -0.0127916 , -0.19415151, -0.00227538, -0.00094903,\n",
       "        -0.02209528,  0.00001318, -0.00003427,  0.00003541, -0.00015872,\n",
       "        -0.00523161, -0.00001254,  0.00214312,  0.00704125,  0.00118545,\n",
       "        -0.00888442, -0.00106511,  0.00169667, -0.00225413, -0.00289658,\n",
       "         0.00275595, -0.5466112 , -4.78952066,  3.06689181,  0.01422945,\n",
       "        -0.00000034, -0.00138475, -0.02769224,  0.04403614,  0.04977395,\n",
       "         0.02432618, -0.05237789,  0.02323446, -0.00101051, -0.04160943,\n",
       "        -2.22239892, -4.33770499, -4.16433277,  2.05198665, -1.0363644 ,\n",
       "         3.84128038,  1.63409731,  0.35596868, -0.0200233 ,  0.05123575,\n",
       "         0.30690546,  0.06660448, -0.09838635, -0.01105389, -0.0009219 ,\n",
       "         0.15549316,  0.44093901]),\n",
       " array([-0.00073391, -0.01279963, -0.19407699, -0.00227181, -0.00094837,\n",
       "        -0.0221086 ,  0.00001304, -0.00003123,  0.00003537, -0.00015862,\n",
       "        -0.00522832, -0.00001261,  0.00214214,  0.00703728,  0.00118505,\n",
       "        -0.00888488, -0.00106515,  0.0016964 , -0.00221449, -0.00290092,\n",
       "         0.00271984, -0.45735744, -4.79065294,  2.97879505,  0.01416133,\n",
       "        -0.00000034, -0.00137325, -0.02769856,  0.04389118,  0.04973894,\n",
       "         0.02432715, -0.05210853,  0.02308885, -0.00108116, -0.04149962,\n",
       "        -2.20277809, -4.33652001, -4.16459559,  2.04064268, -1.03013765,\n",
       "         3.83055104,  1.62884149,  0.35601388, -0.01993219,  0.05114153,\n",
       "         0.30685693,  0.06648355, -0.09847323, -0.01105577, -0.00092057,\n",
       "         0.15544294,  0.44067536]),\n",
       " array([-0.0007005 , -0.01280817, -0.19399691, -0.00226798, -0.00094766,\n",
       "        -0.02212284,  0.00001288, -0.00002797,  0.00003533, -0.00015851,\n",
       "        -0.00522484, -0.00001269,  0.0021411 ,  0.00703303,  0.00118461,\n",
       "        -0.00888536, -0.00106519,  0.00169612, -0.00217202, -0.00290558,\n",
       "         0.00268114, -0.36170381, -4.79186361,  2.88438259,  0.01408833,\n",
       "        -0.00000033, -0.00136093, -0.02770539,  0.04373468,  0.04970242,\n",
       "         0.02433052, -0.05182131,  0.02293104, -0.00115428, -0.04138258,\n",
       "        -2.18178691, -4.33519415, -4.16488439,  2.02854785, -1.02348517,\n",
       "         3.81902746,  1.62324133,  0.35606223, -0.01983695,  0.05104307,\n",
       "         0.30680695,  0.06635623, -0.09856398, -0.01105779, -0.00091914,\n",
       "         0.15538891,  0.44039542]),\n",
       " array([-0.00066428, -0.0128173 , -0.19391111, -0.00226387, -0.00094691,\n",
       "        -0.02213787,  0.00001272, -0.00002448,  0.00003528, -0.00015839,\n",
       "        -0.00522122, -0.00001277,  0.00213997,  0.00702847,  0.00118414,\n",
       "        -0.00888586, -0.00106523,  0.00169582, -0.0021265 , -0.00291055,\n",
       "         0.00263965, -0.25919227, -4.79315281,  2.78319317,  0.01401011,\n",
       "        -0.00000033, -0.00134772, -0.02771275,  0.043564  ,  0.04966328,\n",
       "         0.02433464, -0.05151393,  0.02276213, -0.00122879, -0.0412583 ,\n",
       "        -2.15933463, -4.3337571 , -4.16520664,  2.01560734, -1.01637708,\n",
       "         3.80668933,  1.61728944,  0.35611398, -0.01973215,  0.05093479,\n",
       "         0.30675074,  0.06621691, -0.09866385, -0.01105996, -0.0009176 ,\n",
       "         0.15533075,  0.44009867]),\n",
       " array([-0.00062528, -0.01282702, -0.19381911, -0.00225946, -0.0009461 ,\n",
       "        -0.02215386,  0.00001254, -0.00002074,  0.00003523, -0.00015827,\n",
       "        -0.00521739, -0.00001286,  0.00213877,  0.00702358,  0.00118364,\n",
       "        -0.00888639, -0.00106527,  0.00169549, -0.00207773, -0.0029159 ,\n",
       "         0.0025952 , -0.1493327 , -4.79452742,  2.67474284,  0.01392628,\n",
       "        -0.00000033, -0.00133357, -0.02772067,  0.04337913,  0.04962169,\n",
       "         0.02433984, -0.05118508,  0.02258052, -0.0013061 , -0.04112548,\n",
       "        -2.13528923, -4.33220102, -4.16555796,  2.00176289, -1.00876626,\n",
       "         3.79346121,  1.61092941,  0.35616934, -0.01961966,  0.05081859,\n",
       "         0.30669017,  0.06606732, -0.09877107, -0.0110623 , -0.00091595,\n",
       "         0.15526831,  0.4397832 ]),\n",
       " array([-0.00058288, -0.01283741, -0.19372022, -0.00225474, -0.00094523,\n",
       "        -0.02217072,  0.00001235, -0.00001673,  0.00003518, -0.00015813,\n",
       "        -0.00521342, -0.00001296,  0.00213747,  0.00701834,  0.0011831 ,\n",
       "        -0.00888695, -0.00106532,  0.00169515, -0.00202546, -0.00292163,\n",
       "         0.00254757, -0.03159843, -4.79599663,  2.55851534,  0.01383643,\n",
       "        -0.00000033, -0.0013184 , -0.02772924,  0.0431763 ,  0.04957742,\n",
       "         0.02434704, -0.05083373,  0.02238506, -0.00138246, -0.04098453,\n",
       "        -2.10958236, -4.33047662, -4.16594449,  1.98697533, -1.00063305,\n",
       "         3.77927567,  1.60417069,  0.35622858, -0.01950208,  0.05069716,\n",
       "         0.30662775,  0.06590968, -0.09888294, -0.01106481, -0.00091419,\n",
       "         0.15520071,  0.43944825]),\n",
       " array([-0.00054317, -0.01284075, -0.193598  , -0.00224969, -0.00094442,\n",
       "        -0.02218371,  0.00001215, -0.00001243,  0.00003512, -0.000158  ,\n",
       "        -0.00520803, -0.00001318,  0.00213608,  0.0070155 ,  0.00118245,\n",
       "        -0.0088875 , -0.00106538,  0.00169446, -0.00196425, -0.00292749,\n",
       "         0.00249585,  0.01582939, -4.77771609,  2.49065561,  0.01373858,\n",
       "        -0.00000033, -0.00130216, -0.02772698,  0.04294594,  0.0495264 ,\n",
       "         0.02436138, -0.05046258,  0.02218033, -0.00146544, -0.0408361 ,\n",
       "        -2.08165946, -4.32749754, -4.1663783 ,  1.97126484, -0.99188599,\n",
       "         3.76401479,  1.59677202,  0.35632009, -0.01936463,  0.05056305,\n",
       "         0.30651888,  0.0657421 , -0.09900752, -0.01106658, -0.00091183,\n",
       "         0.15514538,  0.43963666]),\n",
       " array([-0.00049773, -0.01284381, -0.1934669 , -0.00224423, -0.00094351,\n",
       "        -0.02219615,  0.00001193, -0.00000784,  0.00003506, -0.00015785,\n",
       "        -0.00520413, -0.0000133 ,  0.00213453,  0.00701423,  0.00118172,\n",
       "        -0.00888875, -0.00106541,  0.00169357, -0.00190111, -0.00293252,\n",
       "         0.0024387 ,  0.03964787, -4.77480354,  2.4632989 ,  0.01363397,\n",
       "        -0.00000033, -0.00128473, -0.02772817,  0.04269795,  0.04947416,\n",
       "         0.02437112, -0.05005847,  0.02196207, -0.00155115, -0.04067756,\n",
       "        -2.05187635, -4.32508084, -4.16670206,  1.9543068 , -0.98240914,\n",
       "         3.74751553,  1.58907685,  0.35641606, -0.01922374,  0.05041708,\n",
       "         0.30645876,  0.06556517, -0.09914385, -0.01107043, -0.00090975,\n",
       "         0.15507948,  0.43926653]),\n",
       " array([-0.00044673, -0.01285335, -0.19333519, -0.00223836, -0.0009425 ,\n",
       "        -0.0222107 ,  0.00001169, -0.00000294,  0.000035  , -0.00015767,\n",
       "        -0.00520056, -0.00001343,  0.00213285,  0.00701133,  0.001181  ,\n",
       "        -0.0088897 , -0.00106547,  0.00169286, -0.00183562, -0.00293804,\n",
       "         0.0023782 ,  0.06429766, -4.77396709,  2.43774602,  0.01352291,\n",
       "        -0.00000032, -0.00126605, -0.02773608,  0.04243004,  0.04942392,\n",
       "         0.02438367, -0.04962698,  0.02172535, -0.00163051, -0.04051051,\n",
       "        -2.02034598, -4.3227692 , -4.16710382,  1.93621575, -0.97232201,\n",
       "         3.7298551 ,  1.58100768,  0.35649678, -0.019074  ,  0.0502642 ,\n",
       "         0.3063751 ,  0.06536666, -0.09928731, -0.01107445, -0.00090779,\n",
       "         0.15499792,  0.4388732 ]),\n",
       " array([-0.00039212, -0.01286424, -0.19319764, -0.00223206, -0.00094142,\n",
       "        -0.02222652,  0.00001144,  0.00000121,  0.00003498, -0.00015745,\n",
       "        -0.00519922, -0.00001351,  0.00213111,  0.00700616,  0.00118031,\n",
       "        -0.0088903 , -0.00106554,  0.0016922 , -0.00176583, -0.00294397,\n",
       "         0.00231335,  0.08841129, -4.77367488,  2.413448  ,  0.01340407,\n",
       "        -0.00000032, -0.00124604, -0.02774604,  0.04214171,  0.04937033,\n",
       "         0.02439709, -0.04916559,  0.02147228, -0.00171088, -0.04033232,\n",
       "        -1.98668211, -4.32057481, -4.16760509,  1.91683504, -0.96150659,\n",
       "         3.71099919,  1.57242096,  0.35657338, -0.01891276,  0.05010028,\n",
       "         0.30627158,  0.06515384, -0.09944068, -0.01107833, -0.00090609,\n",
       "         0.15491477,  0.43846743]),\n",
       " array([-0.0003366 , -0.01287585, -0.19304917, -0.00222522, -0.00094032,\n",
       "        -0.0222426 ,  0.00001117,  0.00000401,  0.00003502, -0.00015717,\n",
       "        -0.00519919, -0.00001353,  0.00212929,  0.00699975,  0.00117966,\n",
       "        -0.00889057, -0.00106565,  0.00169164, -0.0016914 , -0.00295068,\n",
       "         0.00224423,  0.11254248, -4.77375366,  2.38967511,  0.01327644,\n",
       "        -0.00000032, -0.00122459, -0.02775567,  0.04182336,  0.04931737,\n",
       "         0.02441499, -0.0486747 ,  0.02120285, -0.00178705, -0.04014405,\n",
       "        -1.95073101, -4.31832946, -4.16818599,  1.89612608, -0.94990607,\n",
       "         3.69080106,  1.56338904,  0.35664354, -0.01873978,  0.04992343,\n",
       "         0.30614382,  0.06492961, -0.09960306, -0.01108084, -0.00090461,\n",
       "         0.15483448,  0.43802989]),\n",
       " array([-0.00027987, -0.01288926, -0.19289039, -0.00221779, -0.00093919,\n",
       "        -0.02225874,  0.00001088,  0.00000681,  0.00003506, -0.00015688,\n",
       "        -0.00519661, -0.00001355,  0.00212737,  0.00699389,  0.00117901,\n",
       "        -0.00889098, -0.00106578,  0.00169108, -0.00161157, -0.00295816,\n",
       "         0.00217045,  0.13605172, -4.77368553,  2.36634324,  0.0131395 ,\n",
       "        -0.00000031, -0.0012016 , -0.02776499,  0.0414762 ,  0.04925943,\n",
       "         0.02443251, -0.04814817,  0.02091896, -0.00186706, -0.03994264,\n",
       "        -1.91223441, -4.31593809, -4.1688305 ,  1.87389571, -0.93743601,\n",
       "         3.66919658,  1.55377913,  0.35671791, -0.01855353,  0.04973266,\n",
       "         0.30600674,  0.06468787, -0.09977748, -0.0110839 , -0.00090297,\n",
       "         0.15474811,  0.43755724]),\n",
       " array([-0.00024342, -0.01290292, -0.19272608, -0.00220981, -0.00093777,\n",
       "        -0.02228434,  0.00001057,  0.0000096 ,  0.00003511, -0.0001566 ,\n",
       "        -0.0051892 , -0.00001357,  0.00212545,  0.00699118,  0.00117844,\n",
       "        -0.00889187, -0.00106589,  0.00169036, -0.00152463, -0.00296587,\n",
       "         0.00208994,  0.15837877, -4.7733419 ,  2.34405271,  0.01299329,\n",
       "        -0.00000031, -0.001177  , -0.02777642,  0.04108243,  0.04920702,\n",
       "         0.0244728 , -0.04759881,  0.02061726, -0.00194102, -0.03972615,\n",
       "        -1.87127245, -4.3129899 , -4.16972801,  1.85075452, -0.92451836,\n",
       "         3.64606603,  1.54368391,  0.35679792, -0.01835356,  0.04952578,\n",
       "         0.30587057,  0.06443172, -0.09996508, -0.01108761, -0.00090125,\n",
       "         0.15466564,  0.43698258]),\n",
       " array([-0.00022204, -0.01291707, -0.19255627, -0.00220118, -0.00093613,\n",
       "        -0.02231645,  0.00001024,  0.00001228,  0.00003518, -0.00015632,\n",
       "        -0.00517803, -0.00001359,  0.0021235 ,  0.00699098,  0.00117792,\n",
       "        -0.00889308, -0.001066  ,  0.00168949, -0.00143187, -0.00297336,\n",
       "         0.00200348,  0.17928758, -4.77263638,  2.32293851,  0.01283703,\n",
       "        -0.00000031, -0.00115068, -0.02778965,  0.04064151,  0.04915755,\n",
       "         0.02452858, -0.04701452,  0.0202965 , -0.00201511, -0.03949145,\n",
       "        -1.82745757, -4.30972357, -4.1708646 ,  1.82618157, -0.91079926,\n",
       "         3.62138459,  1.53290767,  0.35688108, -0.01813986,  0.04930295,\n",
       "         0.3057346 ,  0.06415999, -0.10016542, -0.01109206, -0.00089959,\n",
       "         0.15458458,  0.43630851]),\n",
       " array([-0.00020898, -0.01293162, -0.19237798, -0.00219175, -0.00093432,\n",
       "        -0.0223534 ,  0.00000989,  0.00001489,  0.00003524, -0.00015603,\n",
       "        -0.00516416, -0.00001362,  0.00212147,  0.00699277,  0.00117744,\n",
       "        -0.00889452, -0.0010661 ,  0.0016885 , -0.00133268, -0.00298185,\n",
       "         0.00191168,  0.19739109, -4.77166974,  2.30435531,  0.01266988,\n",
       "        -0.00000031, -0.00112247, -0.02780532,  0.04015408,  0.04910438,\n",
       "         0.02459187, -0.04639281,  0.01996121, -0.00208512, -0.03924191,\n",
       "        -1.78069387, -4.30608502, -4.17216654,  1.80006922, -0.89628758,\n",
       "         3.59499791,  1.52152885,  0.35696643, -0.01791088,  0.04906324,\n",
       "         0.30559459,  0.06386872, -0.10037963, -0.01109739, -0.00089782,\n",
       "         0.15450123,  0.43556157]),\n",
       " array([-0.00019479, -0.01294699, -0.19218884, -0.00218137, -0.00093244,\n",
       "        -0.02239112,  0.00000951,  0.00001741,  0.00003532, -0.00015573,\n",
       "        -0.00514958, -0.00001369,  0.00211931,  0.00699551,  0.00117701,\n",
       "        -0.00889597, -0.00106619,  0.00168744, -0.00122623, -0.00299099,\n",
       "         0.00181332,  0.21212827, -4.77038833,  2.28878095,  0.01249093,\n",
       "        -0.0000003 , -0.00109225, -0.02782327,  0.03962521,  0.04904887,\n",
       "         0.02466666, -0.04573115,  0.01959834, -0.00214878, -0.0389782 ,\n",
       "        -1.73079707, -4.30191699, -4.17358596,  1.77229683, -0.88071424,\n",
       "         3.56658403,  1.5094974 ,  0.35705159, -0.0176655 ,  0.04880603,\n",
       "         0.30544536,  0.06355429, -0.10060772, -0.01110374, -0.000896  ,\n",
       "         0.15441131,  0.43476414]),\n",
       " array([-0.00017471, -0.01296333, -0.19198843, -0.0021699 , -0.00093054,\n",
       "        -0.02242774,  0.00000911,  0.0000198 ,  0.0000354 , -0.00015539,\n",
       "        -0.00513512, -0.0000138 ,  0.00211701,  0.00699875,  0.00117662,\n",
       "        -0.00889733, -0.00106627,  0.00168633, -0.00111201, -0.00300084,\n",
       "         0.00170793,  0.22233843, -4.76874993,  2.2772704 ,  0.0122993 ,\n",
       "        -0.0000003 , -0.00105987, -0.02784351,  0.03904506,  0.04898471,\n",
       "         0.02474184, -0.04502191,  0.0192148 , -0.00219881, -0.03870098,\n",
       "        -1.67753687, -4.29743385, -4.17512518,  1.74254076, -0.86391814,\n",
       "         3.53610256,  1.49679131,  0.35713524, -0.01739583,  0.04852334,\n",
       "         0.3052783 ,  0.06320756, -0.10085683, -0.0111112 , -0.00089418,\n",
       "         0.15431242,  0.43392639]),\n",
       " array([-0.0001498 , -0.01298086, -0.1917768 , -0.00215721, -0.00092861,\n",
       "        -0.02246356,  0.00000868,  0.000022  ,  0.00003549, -0.00015504,\n",
       "        -0.00512029, -0.00001397,  0.00211457,  0.00700252,  0.00117626,\n",
       "        -0.00889855, -0.00106634,  0.00168517, -0.00098739, -0.00301086,\n",
       "         0.00159232,  0.22746368, -4.76669722,  2.27039046,  0.01209405,\n",
       "        -0.0000003 , -0.00102516, -0.02786613,  0.03840973,  0.04891529,\n",
       "         0.02482612, -0.04426445,  0.01880219, -0.00223249, -0.03840964,\n",
       "        -1.62075833, -4.29246611, -4.17684183,  1.71078102, -0.84578103,\n",
       "         3.50336128,  1.4833842 ,  0.35721718, -0.01711332,  0.04822678,\n",
       "         0.30510593,  0.06284001, -0.10111591, -0.01111978, -0.00089246,\n",
       "         0.15420394,  0.43303187]),\n",
       " array([-0.00012065, -0.01299952, -0.19155314, -0.00214314, -0.00092662,\n",
       "        -0.02250047,  0.00000822,  0.00002398,  0.00003558, -0.00015465,\n",
       "        -0.0051045 , -0.00001417,  0.00211198,  0.00700688,  0.00117593,\n",
       "        -0.00889973, -0.00106642,  0.00168396, -0.00085579, -0.00302227,\n",
       "         0.00147116,  0.23060851, -4.76433725,  2.26506807,  0.01187417,\n",
       "        -0.00000029, -0.00098796, -0.02789147,  0.03771261,  0.04883916,\n",
       "         0.02491976, -0.04345535,  0.01835981, -0.00224712, -0.03810305,\n",
       "        -1.56014791, -4.2869458 , -4.17876134,  1.67687156, -0.82628098,\n",
       "         3.46822504,  1.46921858,  0.35729834, -0.01680251,  0.04790051,\n",
       "         0.30491414,  0.06243642, -0.10140049, -0.01112949, -0.00089066,\n",
       "         0.1540871 ,  0.43207816]),\n",
       " array([-0.00008873, -0.01301967, -0.19131766, -0.00212749, -0.00092455,\n",
       "        -0.02253885,  0.00000773,  0.00002562,  0.00003568, -0.00015424,\n",
       "        -0.00508711, -0.00001441,  0.00210923,  0.00701197,  0.00117563,\n",
       "        -0.00890087, -0.00106651,  0.00168268, -0.00071461, -0.00303449,\n",
       "         0.00134128,  0.23182029, -4.7615695 ,  2.26119495,  0.01163861,\n",
       "        -0.00000029, -0.00094809, -0.0279193 ,  0.03694593,  0.04875391,\n",
       "         0.02501735, -0.04258732,  0.01789147, -0.00224326, -0.0377783 ,\n",
       "        -1.4953718 , -4.28105771, -4.18090572,  1.64050975, -0.80522781,\n",
       "         3.43060581,  1.45416617,  0.3573788 , -0.01647651,  0.04755771,\n",
       "         0.30471799,  0.06201006, -0.10169788, -0.01114038, -0.00088891,\n",
       "         0.15396118,  0.43104504]),\n",
       " array([-0.00005626, -0.01304157, -0.19107041, -0.0021101 , -0.0009224 ,\n",
       "        -0.02257975,  0.00000721,  0.00002693,  0.00003578, -0.00015381,\n",
       "        -0.00506718, -0.00001468,  0.00210633,  0.00701788,  0.00117535,\n",
       "        -0.00890197, -0.00106661,  0.0016813 , -0.00056065, -0.00304688,\n",
       "         0.00119879,  0.23202536, -4.75833407,  2.25789646,  0.01138632,\n",
       "        -0.00000028, -0.00090538, -0.02794956,  0.03610451,  0.04866663,\n",
       "         0.02513449, -0.04166588,  0.01738268, -0.00221193, -0.03743439,\n",
       "        -1.42632644, -4.27450992, -4.18335108,  1.60193379, -0.78253044,\n",
       "         3.3901279 ,  1.43821093,  0.35746011, -0.01611752,  0.04717998,\n",
       "         0.30450444,  0.06154368, -0.10202637, -0.01115251, -0.00088729,\n",
       "         0.15382789,  0.4299152 ]),\n",
       " array([-0.00002672, -0.0130648 , -0.19081287, -0.00209095, -0.00092004,\n",
       "        -0.02262758,  0.00000665,  0.00002812,  0.00003588, -0.00015335,\n",
       "        -0.00504374, -0.00001496,  0.00210329,  0.00702475,  0.00117503,\n",
       "        -0.00890323, -0.00106675,  0.00167982, -0.00039804, -0.00306061,\n",
       "         0.00104904,  0.23196702, -4.75483495,  2.25454687,  0.01111596,\n",
       "        -0.00000028, -0.00085958, -0.02798228,  0.0351736 ,  0.04856808,\n",
       "         0.02525536, -0.04067829,  0.01685326, -0.00215435, -0.03707143,\n",
       "        -1.35256894, -4.26771214, -4.18622978,  1.56060003, -0.75833599,\n",
       "         3.34709909,  1.42138432,  0.35754547, -0.015731  ,  0.04677289,\n",
       "         0.30427927,  0.06104443, -0.10238151, -0.01116548, -0.00088551,\n",
       "         0.15368735,  0.42868974]),\n",
       " array([-0.00000543, -0.01308864, -0.19054534, -0.00206992, -0.00091741,\n",
       "        -0.02268505,  0.00000606,  0.00002924,  0.00003597, -0.00015287,\n",
       "        -0.00501559, -0.00001524,  0.00210009,  0.00703277,  0.00117464,\n",
       "        -0.0089047 , -0.00106693,  0.00167822, -0.00022367, -0.00307505,\n",
       "         0.00088821,  0.23182259, -4.75106267,  2.25109652,  0.01082629,\n",
       "        -0.00000027, -0.0008105 , -0.02801756,  0.03415031,  0.04846494,\n",
       "         0.02539832, -0.03963118,  0.01628789, -0.00206525, -0.03668721,\n",
       "        -1.273958  , -4.26021153, -4.18962783,  1.51678498, -0.73262003,\n",
       "         3.30112794,  1.40363945,  0.35763699, -0.01531457,  0.04633376,\n",
       "         0.30404435,  0.06051122, -0.10276658, -0.01117924, -0.00088363,\n",
       "         0.15354185,  0.42734318]),\n",
       " array([-0.        , -0.01310852, -0.19027243, -0.00204689, -0.00091441,\n",
       "        -0.02275397,  0.00000543,  0.00003028,  0.00003605, -0.00015234,\n",
       "        -0.00498229, -0.00001558,  0.00209675,  0.00704244,  0.00117413,\n",
       "        -0.0089063 , -0.00106715,  0.00167651, -0.00003341, -0.00308925,\n",
       "         0.00071135,  0.23164266, -4.74680844,  2.24736844,  0.01051588,\n",
       "        -0.00000027, -0.00075789, -0.02805521,  0.03302329,  0.0483525 ,\n",
       "         0.02556608, -0.03852279,  0.0156891 , -0.00194042, -0.03628011,\n",
       "        -1.1902871 , -4.25163783, -4.19365048,  1.47031986, -0.70528799,\n",
       "         3.2520979 ,  1.38495984,  0.35773922, -0.01486565,  0.04585972,\n",
       "         0.30380421,  0.05994298, -0.10318565, -0.01119415, -0.0008818 ,\n",
       "         0.15339388,  0.42585647]),\n",
       " array([-0.00003683, -0.01319104, -0.1899797 , -0.0020215 , -0.00091184,\n",
       "        -0.02280005,  0.00000475,  0.00003151,  0.00003612, -0.000152  ,\n",
       "        -0.00494919, -0.00001502,  0.00209309,  0.00704038,  0.0011738 ,\n",
       "        -0.00890055, -0.00106663,  0.00167384,  0.00000125, -0.00296154,\n",
       "         0.00053888,  0.24667512, -4.73686743,  2.23773957,  0.01017468,\n",
       "        -0.00000026, -0.00070168, -0.02805251,  0.03176446,  0.04839579,\n",
       "         0.0257659 , -0.03734142,  0.0150578 , -0.00178464, -0.03584404,\n",
       "        -1.10170893, -4.25132005, -4.19779622,  1.42037321, -0.67445574,\n",
       "         3.19882414,  1.36678668,  0.35770258, -0.01437153,  0.04533775,\n",
       "         0.30363513,  0.05937535, -0.10367575, -0.01122185, -0.00089895,\n",
       "         0.15325476,  0.42341413]),\n",
       " array([-0.00003505, -0.01327056, -0.18966536, -0.00199343, -0.00091003,\n",
       "        -0.02279772,  0.00000401,  0.00003305,  0.00003617, -0.00015185,\n",
       "        -0.00492442, -0.00001453,  0.00208954,  0.00703321,  0.0011722 ,\n",
       "        -0.0088914 , -0.00106507,  0.00167035,  0.00000544, -0.00280125,\n",
       "         0.00036812,  0.26049778, -4.71772919,  2.21882438,  0.00980897,\n",
       "        -0.00000025, -0.0006416 , -0.02804229,  0.030415  ,  0.04843904,\n",
       "         0.02596993, -0.03606294,  0.01436301, -0.00157825, -0.03539166,\n",
       "        -1.00901992, -4.25036045, -4.2016661 ,  1.36695697, -0.64030504,\n",
       "         3.14055197,  1.34844229,  0.35784728, -0.01385063,  0.04477616,\n",
       "         0.3035357 ,  0.05874766, -0.10418114, -0.01124171, -0.00092066,\n",
       "         0.1530364 ,  0.42112432]),\n",
       " array([-0.00001495, -0.01334668, -0.18934251, -0.00196265, -0.00090814,\n",
       "        -0.02278398,  0.00000321,  0.00003467,  0.00003621, -0.00015169,\n",
       "        -0.00490173, -0.00001399,  0.00208584,  0.00702779,  0.00117013,\n",
       "        -0.00888409, -0.00106364,  0.00166636,  0.00000858, -0.00263898,\n",
       "         0.00019683,  0.27398973, -4.69694733,  2.19854724,  0.00941789,\n",
       "        -0.00000024, -0.00057725, -0.02803401,  0.02896045,  0.0484636 ,\n",
       "         0.02617477, -0.03468471,  0.01362426, -0.00133378, -0.03491688,\n",
       "        -0.91012189, -4.2491038 , -4.20579095,  1.3095171 , -0.60335741,\n",
       "         3.07807623,  1.32883733,  0.35800867, -0.01328998,  0.04416559,\n",
       "         0.30342047,  0.05807358, -0.10471404, -0.01126059, -0.00094269,\n",
       "         0.15280387,  0.41867031]),\n",
       " array([-0.0000036 , -0.01342391, -0.18901176, -0.00192916, -0.00090595,\n",
       "        -0.02277212,  0.00000237,  0.00003634,  0.00003624, -0.00015149,\n",
       "        -0.00487421, -0.00001339,  0.00208197,  0.00702536,  0.00116778,\n",
       "        -0.00887701, -0.00106235,  0.00166184,  0.0000105 , -0.00247409,\n",
       "         0.00002376,  0.28817558, -4.67526443,  2.17726933,  0.00900006,\n",
       "        -0.00000023, -0.00050839, -0.02802804,  0.02739959,  0.04848823,\n",
       "         0.02640234, -0.03321465,  0.01283184, -0.00107236, -0.03440496,\n",
       "        -0.80432755, -4.24759575, -4.21047309,  1.24828123, -0.56347991,\n",
       "         3.01113938,  1.30757039,  0.3581798 , -0.01267847,  0.04350091,\n",
       "         0.30329256,  0.05733388, -0.10529747, -0.01128145, -0.00096545,\n",
       "         0.15255865,  0.41601074]),\n",
       " array([-0.        , -0.01335365, -0.18862964, -0.00189342, -0.00090464,\n",
       "        -0.02278112,  0.00000148,  0.0000379 ,  0.00003624, -0.00015101,\n",
       "        -0.00485782, -0.00001357,  0.00207901,  0.00704921,  0.00116379,\n",
       "        -0.00887203, -0.00106173,  0.0016529 , -0.        , -0.00239522,\n",
       "        -0.        ,  0.26754329, -4.6240149 ,  2.125292  ,  0.008543  ,\n",
       "        -0.00000021, -0.00043478, -0.02796205,  0.02572519,  0.04826112,\n",
       "         0.02664744, -0.03165015,  0.0120019 , -0.00080978, -0.03386169,\n",
       "        -0.68937491, -4.23405052, -4.21614246,  1.18274227, -0.52060467,\n",
       "         2.9398349 ,  1.28329407,  0.35889104, -0.01203295,  0.04270446,\n",
       "         0.3039396 ,  0.05669703, -0.1059217 , -0.0113213 , -0.00097652,\n",
       "         0.15246023,  0.41345608]),\n",
       " array([-0.        , -0.01335973, -0.18829442, -0.00185487, -0.00090127,\n",
       "        -0.02285338,  0.00000054,  0.00003908,  0.00003624, -0.00015041,\n",
       "        -0.00483104, -0.00001333,  0.00207549,  0.00708434,  0.00116188,\n",
       "        -0.00887565, -0.00106211,  0.00164727, -0.        , -0.00238529,\n",
       "        -0.        ,  0.26521445, -4.60389023,  2.10429234,  0.00806668,\n",
       "        -0.0000002 , -0.0003554 , -0.02797488,  0.02394856,  0.04812327,\n",
       "         0.02690314, -0.02997511,  0.01111822, -0.00055712, -0.0332657 ,\n",
       "        -0.56410683, -4.22560601, -4.22367601,  1.11289333, -0.47553108,\n",
       "         2.86505993,  1.25500263,  0.35913495, -0.01133013,  0.04192304,\n",
       "         0.30382578,  0.05586917, -0.10657674, -0.01134469, -0.00097871,\n",
       "         0.15230833,  0.41064981]),\n",
       " array([-0.        , -0.01337816, -0.18796564, -0.00181279, -0.00089758,\n",
       "        -0.02294133, -0.0000001 ,  0.00002881,  0.00003622, -0.0001502 ,\n",
       "        -0.00483755, -0.0000137 ,  0.00207192,  0.00710474,  0.00116114,\n",
       "        -0.00887602, -0.00106206,  0.00164245, -0.        , -0.00237885,\n",
       "        -0.        ,  0.26421021, -4.5877627 ,  2.08723155,  0.00755697,\n",
       "        -0.00000019, -0.00027035, -0.02800329,  0.02207054,  0.04798406,\n",
       "         0.02715684, -0.02816966,  0.01016858, -0.00032198, -0.03260498,\n",
       "        -0.42926285, -4.21897406, -4.23246706,  1.03741916, -0.4263439 ,\n",
       "         2.78564116,  1.22419437,  0.35941455, -0.01056179,  0.04108448,\n",
       "         0.30350169,  0.05496005, -0.10727833, -0.01136593, -0.0009835 ,\n",
       "         0.15213985,  0.40803086]),\n",
       " array([-0.        , -0.01340076, -0.18758199, -0.00176574, -0.00089416,\n",
       "        -0.02302663, -0.00000032,  0.00000356,  0.00003613, -0.00015037,\n",
       "        -0.00487749, -0.0000148 ,  0.00206764,  0.0071234 ,  0.0011612 ,\n",
       "        -0.00887909, -0.00106197,  0.00163804, -0.        , -0.00237288,\n",
       "        -0.        ,  0.26368379, -4.57046405,  2.0683263 ,  0.00700892,\n",
       "        -0.00000018, -0.00017918, -0.02802776,  0.0200786 ,  0.04785895,\n",
       "         0.02741759, -0.02624143,  0.00914335, -0.00009685, -0.0318789 ,\n",
       "        -0.2845938 , -4.21337391, -4.24192336,  0.95655026, -0.37210567,\n",
       "         2.69988022,  1.19106198,  0.35971232, -0.00973127,  0.04016517,\n",
       "         0.30296654,  0.05400523, -0.10799014, -0.01137716, -0.00098987,\n",
       "         0.15197647,  0.40540703]),\n",
       " array([-0.        , -0.01339889, -0.18721059, -0.00171511, -0.00089004,\n",
       "        -0.02313444, -0.00000052, -0.        ,  0.00003503, -0.00015091,\n",
       "        -0.00486309, -0.00001768,  0.00206194,  0.00715252,  0.00115959,\n",
       "        -0.00888316, -0.00106178,  0.00163267, -0.        , -0.00236127,\n",
       "        -0.        ,  0.2588009 , -4.54572199,  2.0431997 ,  0.00642603,\n",
       "        -0.00000017, -0.00008147, -0.02806991,  0.01821011,  0.04755453,\n",
       "         0.02749392, -0.02430362,  0.00811361, -0.        , -0.03096145,\n",
       "        -0.16539007, -4.19308286, -4.239702  ,  0.87540879, -0.32181961,\n",
       "         2.63902471,  1.14912522,  0.36019459, -0.00884422,  0.03919674,\n",
       "         0.30265213,  0.05293509, -0.10879679, -0.01141209, -0.00099075,\n",
       "         0.15165103,  0.4022767 ]),\n",
       " array([-0.        , -0.0133831 , -0.18682655, -0.00166318, -0.00088468,\n",
       "        -0.02330393, -0.00000071, -0.        ,  0.00003374, -0.00015128,\n",
       "        -0.00484435, -0.00002016,  0.00205538,  0.00715925,  0.00115622,\n",
       "        -0.00888534, -0.00106234,  0.0016278 , -0.        , -0.00235013,\n",
       "        -0.        ,  0.25378702, -4.52288357,  2.02048911,  0.00592517,\n",
       "        -0.00000015, -0.        , -0.02801859,  0.01634964,  0.04719678,\n",
       "         0.0274646 , -0.02227722,  0.0070447 , -0.        , -0.02989402,\n",
       "        -0.05198823, -4.17178135, -4.23291114,  0.79060816, -0.27273369,\n",
       "         2.59015713,  1.10035013,  0.36076593, -0.00785566,  0.03813211,\n",
       "         0.30231223,  0.05181102, -0.10974116, -0.01144272, -0.00098849,\n",
       "         0.15134575,  0.39921872]),\n",
       " array([-0.        , -0.01340324, -0.18617778, -0.00160958, -0.00087883,\n",
       "        -0.02345502, -0.0000009 , -0.        ,  0.00003242, -0.00015167,\n",
       "        -0.00478554, -0.00002273,  0.00204661,  0.00717197,  0.00115267,\n",
       "        -0.00889146, -0.00106362,  0.00162173, -0.        , -0.0023378 ,\n",
       "        -0.        ,  0.25397359, -4.49481934,  1.99280724,  0.00589751,\n",
       "        -0.00000014, -0.        , -0.02790045,  0.01454561,  0.04655537,\n",
       "         0.02717273, -0.02020734,  0.00594813, -0.        , -0.02861023,\n",
       "        -0.        , -4.10784836, -4.18531336,  0.71330484, -0.23364465,\n",
       "         2.57918498,  1.03323594,  0.36176634, -0.00681214,  0.03697384,\n",
       "         0.30211935,  0.05058133, -0.11072731, -0.01149189, -0.00100606,\n",
       "         0.15094369,  0.3924119 ]),\n",
       " array([-0.        , -0.01340487, -0.18548792, -0.00155319, -0.00087231,\n",
       "        -0.02357823, -0.00000109, -0.        ,  0.00003102, -0.00015171,\n",
       "        -0.00471419, -0.0000251 ,  0.00203624,  0.00718689,  0.00114766,\n",
       "        -0.0088955 , -0.00106577,  0.00161429, -0.        , -0.00232906,\n",
       "        -0.        ,  0.25762166, -4.46437692,  1.96046726,  0.00587215,\n",
       "        -0.00000013, -0.        , -0.02778118,  0.01281624,  0.04584761,\n",
       "         0.02649992, -0.01828256,  0.00491621, -0.        , -0.02697733,\n",
       "        -0.        , -4.04317936, -4.12011141,  0.63734546, -0.20268788,\n",
       "         2.62382238,  0.95382637,  0.36276424, -0.00568287,  0.03575409,\n",
       "         0.30186647,  0.04922596, -0.11182521, -0.01154682, -0.00101704,\n",
       "         0.15044663,  0.38599115]),\n",
       " array([-0.        , -0.01341801, -0.18482953, -0.00149494, -0.00086557,\n",
       "        -0.02373095, -0.00000125, -0.        ,  0.00002953, -0.00015187,\n",
       "        -0.00464362, -0.00002707,  0.0020262 ,  0.00719192,  0.00114221,\n",
       "        -0.00889855, -0.00106815,  0.001607  , -0.        , -0.0023208 ,\n",
       "        -0.        ,  0.27516085, -4.43645952,  1.91631029,  0.00584232,\n",
       "        -0.00000011, -0.        , -0.02763601,  0.01100725,  0.04513575,\n",
       "         0.02577459, -0.01625936,  0.00382908, -0.        , -0.02523502,\n",
       "         0.        , -3.97945156, -4.05238266,  0.5562396 , -0.17202835,\n",
       "         2.67395735,  0.87041268,  0.36376616, -0.00445762,  0.03444146,\n",
       "         0.30149255,  0.04778937, -0.11302541, -0.01159617, -0.00102473,\n",
       "         0.14995662,  0.37977431]),\n",
       " array([-0.        , -0.01343337, -0.184122  , -0.00143415, -0.00085844,\n",
       "        -0.02389013, -0.00000139, -0.        ,  0.00002794, -0.00015205,\n",
       "        -0.00457026, -0.00002911,  0.0020155 ,  0.00719696,  0.0011364 ,\n",
       "        -0.00890201, -0.00107068,  0.00159917, -0.        , -0.00231213,\n",
       "        -0.        ,  0.28740059, -4.40698342,  1.87603   ,  0.0058099 ,\n",
       "        -0.0000001 , -0.        , -0.02747796,  0.00904998,  0.04437962,\n",
       "         0.02499639, -0.01407282,  0.00266952, -0.        , -0.02337191,\n",
       "         0.        , -3.91126749, -3.97942091,  0.4684889 , -0.14010439,\n",
       "         2.72877129,  0.7813465 ,  0.36484209, -0.00312993,  0.03302023,\n",
       "         0.30106409,  0.04623825, -0.11432532, -0.01164767, -0.00103273,\n",
       "         0.14943523,  0.37319156]),\n",
       " array([-0.        , -0.01345023, -0.18334661, -0.00137045, -0.00085084,\n",
       "        -0.0240537 , -0.0000015 , -0.        ,  0.00002624, -0.00015225,\n",
       "        -0.00449224, -0.00003129,  0.00200383,  0.00720245,  0.00113009,\n",
       "        -0.00890601, -0.00107342,  0.00159069, -0.        , -0.00230323,\n",
       "        -0.        ,  0.29720769, -4.37569302,  1.83660745,  0.00577521,\n",
       "        -0.00000008, -0.        , -0.02730884,  0.00695286,  0.04356541,\n",
       "         0.02414821, -0.01172898,  0.00144217, -0.        , -0.02137667,\n",
       "         0.        , -3.83783926, -3.9003857 ,  0.3744104 , -0.10686046,\n",
       "         2.78736937,  0.68611209,  0.36600384, -0.00169472,  0.03148581,\n",
       "         0.30058935,  0.04456209, -0.11573189, -0.01170262, -0.00104118,\n",
       "         0.14887638,  0.36620214]),\n",
       " array([-0.        , -0.01346413, -0.18251282, -0.00130357, -0.00084258,\n",
       "        -0.02423381, -0.00000158, -0.        ,  0.00002442, -0.00015248,\n",
       "        -0.00441424, -0.00003364,  0.0019914 ,  0.0072122 ,  0.00112313,\n",
       "        -0.00891025, -0.0010763 ,  0.00158158, -0.        , -0.00229363,\n",
       "        -0.        ,  0.30610665, -4.34277536,  1.79640813,  0.00573968,\n",
       "        -0.00000007, -0.        , -0.02713498,  0.00481205,  0.04274314,\n",
       "         0.02347372, -0.00931192,  0.00017042, -0.        , -0.01951383,\n",
       "         0.        , -3.7539523 , -3.81970437,  0.27785085, -0.08021056,\n",
       "         2.84589692,  0.59712849,  0.36721427, -0.00037227,  0.03006214,\n",
       "         0.30026882,  0.04298406, -0.11702409, -0.01175736, -0.00104903,\n",
       "         0.14826716,  0.35938559]),\n",
       " array([-0.        , -0.01344227, -0.18167344, -0.00122897, -0.00083407,\n",
       "        -0.024386  , -0.00000163, -0.00001017,  0.00002272, -0.00015247,\n",
       "        -0.00431719, -0.00003606,  0.00197871,  0.00725314,  0.00111515,\n",
       "        -0.00891791, -0.00107969,  0.00157046, -0.        , -0.00228084,\n",
       "        -0.        ,  0.31810985, -4.29454971,  1.74240825,  0.00570369,\n",
       "        -0.00000005, -0.        , -0.02694747,  0.00225821,  0.041427  ,\n",
       "         0.02252802, -0.00656032,  0.        , -0.        , -0.01847489,\n",
       "         0.        , -3.67559093, -3.75748665,  0.17138907, -0.0536315 ,\n",
       "         2.91545775,  0.5340609 ,  0.36870709, -0.        ,  0.02957214,\n",
       "         0.30110349,  0.04241935, -0.1174361 , -0.01180426, -0.00108671,\n",
       "         0.1474968 ,  0.34987795]),\n",
       " array([-0.        , -0.01342848, -0.18101211, -0.00114966, -0.00082624,\n",
       "        -0.02453964, -0.00000163, -0.00002368,  0.00002094, -0.00015253,\n",
       "        -0.00424336, -0.00003749,  0.00196734,  0.00729726,  0.0011071 ,\n",
       "        -0.00891874, -0.00108374,  0.00155925, -0.        , -0.00226759,\n",
       "        -0.        ,  0.32108441, -4.25392424,  1.6956938 ,  0.0056619 ,\n",
       "        -0.00000003, -0.        , -0.02673849,  0.        ,  0.04005258,\n",
       "         0.02124023, -0.00384324,  0.        , -0.        , -0.01727979,\n",
       "         0.        , -3.62322771, -3.70455072,  0.05152987, -0.02844806,\n",
       "         2.98729924,  0.48125035,  0.36972297, -0.        ,  0.02953115,\n",
       "         0.30197658,  0.04218979, -0.11746712, -0.01185159, -0.00108636,\n",
       "         0.14659619,  0.34631025]),\n",
       " array([ 0.        , -0.01354941, -0.18028286, -0.00106884, -0.00081975,\n",
       "        -0.02486972, -0.00000149, -0.00003769,  0.00001898, -0.00015304,\n",
       "        -0.00426382, -0.00003819,  0.00195702,  0.00726159,  0.00110277,\n",
       "        -0.0089017 , -0.00108692,  0.00155314, -0.        , -0.00225706,\n",
       "        -0.        ,  0.3131255 , -4.25544915,  1.68815939,  0.00560982,\n",
       "        -0.00000002, -0.        , -0.02651267, -0.        ,  0.03856602,\n",
       "         0.01958001, -0.00294122,  0.        , -0.        , -0.01572143,\n",
       "         0.        , -3.53941816, -3.60692783,  0.        , -0.0613534 ,\n",
       "         2.97415291,  0.4266198 ,  0.36971245, -0.        ,  0.02954832,\n",
       "         0.30245003,  0.04186235, -0.11742587, -0.01191098, -0.00104633,\n",
       "         0.14626057,  0.34847358]),\n",
       " array([ 0.        , -0.01364222, -0.17871079, -0.00098302, -0.00081099,\n",
       "        -0.025248  , -0.00000134, -0.0000517 ,  0.0000169 , -0.00015403,\n",
       "        -0.00422726, -0.00004157,  0.00193961,  0.00722032,  0.00109801,\n",
       "        -0.00890354, -0.00108708,  0.0015466 , -0.        , -0.00223568,\n",
       "        -0.        ,  0.31540724, -4.22398979,  1.64401652,  0.00556061,\n",
       "        -0.        ,  0.        , -0.02628555, -0.        ,  0.03709378,\n",
       "         0.01785061, -0.00264295,  0.        , -0.        , -0.0141045 ,\n",
       "         0.        , -3.40309673, -3.45729641,  0.        , -0.12303174,\n",
       "         2.92836455,  0.35444797,  0.37045399, -0.        ,  0.02951352,\n",
       "         0.30339   ,  0.04166736, -0.11742899, -0.01199158, -0.00103443,\n",
       "         0.14633855,  0.34516981]),\n",
       " array([ 0.        , -0.01373052, -0.17663642, -0.00088957, -0.00079821,\n",
       "        -0.02564621, -0.00000121, -0.0000659 ,  0.00001468, -0.00015472,\n",
       "        -0.00415647, -0.00004537,  0.00191582,  0.00719216,  0.00109055,\n",
       "        -0.00891535, -0.0010883 ,  0.00153664, -0.        , -0.00221764,\n",
       "        -0.        ,  0.31936355, -4.18498634,  1.59620154,  0.00551299,\n",
       "        -0.        ,  0.        , -0.02602876, -0.        ,  0.03545998,\n",
       "         0.01590448, -0.00235712,  0.        , -0.        , -0.01227941,\n",
       "         0.        , -3.26172071, -3.30055469,  0.        , -0.1766647 ,\n",
       "         2.87723441,  0.27268487,  0.37164912, -0.        ,  0.02947928,\n",
       "         0.30450164,  0.04144535, -0.11746629, -0.01207875, -0.00102502,\n",
       "         0.14623082,  0.34057604]),\n",
       " array([ 0.        , -0.01382333, -0.17427614, -0.00078891, -0.00078312,\n",
       "        -0.02605283, -0.00000111, -0.00008063,  0.00001231, -0.00015518,\n",
       "        -0.00407082, -0.00004922,  0.00188818,  0.00717021,  0.00108102,\n",
       "        -0.00893012, -0.00109044,  0.00152441, -0.        , -0.00220319,\n",
       "        -0.        ,  0.32890109, -4.1439371 ,  1.54377087,  0.00546428,\n",
       "        -0.        ,  0.        , -0.02576095, -0.        ,  0.03370036,\n",
       "         0.01372421, -0.00204708,  0.        , -0.        , -0.01022981,\n",
       "         0.        , -3.1164828 , -3.13483946,  0.        , -0.22626634,\n",
       "         2.82019834,  0.18173942,  0.37305593, -0.        ,  0.02946108,\n",
       "         0.30568633,  0.04116044, -0.11752994, -0.01217388, -0.00101478,\n",
       "         0.14600576,  0.33558144]),\n",
       " array([-0.        , -0.01391716, -0.17176138, -0.00068127, -0.00076671,\n",
       "        -0.0264699 , -0.00000102, -0.00009566,  0.00000978, -0.00015551,\n",
       "        -0.00397349, -0.00005315,  0.00185839,  0.00715346,  0.00107012,\n",
       "        -0.00894577, -0.00109315,  0.00151066, -0.        , -0.00218965,\n",
       "        -0.        ,  0.34418192, -4.10011578,  1.48385757,  0.00541355,\n",
       "        -0.        ,  0.        , -0.02547901, -0.        ,  0.03183866,\n",
       "         0.01145111, -0.00170872,  0.        , -0.        , -0.00809461,\n",
       "         0.        , -2.96531402, -2.96352125, -0.        , -0.27530336,\n",
       "         2.75843963,  0.08743114,  0.37457746, -0.        ,  0.02944918,\n",
       "         0.3069566 ,  0.04083183, -0.11760889, -0.01227538, -0.0010039 ,\n",
       "         0.14570217,  0.33040529]),\n",
       " array([-0.        , -0.01400141, -0.16920854, -0.00056707, -0.0007494 ,\n",
       "        -0.02690548, -0.00000097, -0.00011086,  0.00000711, -0.00015578,\n",
       "        -0.00387171, -0.0000571 ,  0.00182789,  0.00714777,  0.00105796,\n",
       "        -0.00896057, -0.00109628,  0.00149576, -0.        , -0.00217558,\n",
       "        -0.        ,  0.35755503, -4.05334894,  1.42291898,  0.00536208,\n",
       "        -0.        ,  0.        , -0.0251888 , -0.        ,  0.02997934,\n",
       "         0.00950011, -0.00132917,  0.        , -0.        , -0.00628086,\n",
       "         0.        , -2.80215918, -2.79550294, -0.        , -0.33459094,\n",
       "         2.69328444,  0.00880813,  0.37611373, -0.        ,  0.02943859,\n",
       "         0.30830009,  0.0404747 , -0.11769472, -0.01237674, -0.00099073,\n",
       "         0.14532023,  0.32598561]),\n",
       " array([-0.        , -0.01404153, -0.16666064, -0.00044439, -0.00072986,\n",
       "        -0.02733967, -0.00000087, -0.00012976,  0.00000427, -0.00015625,\n",
       "        -0.00374697, -0.00006115,  0.00179691,  0.00718632,  0.00104263,\n",
       "        -0.00897257, -0.00110041,  0.0014785 , -0.        , -0.00216106,\n",
       "        -0.        ,  0.37202332, -4.00383752,  1.35196106,  0.00531889,\n",
       "        -0.        ,  0.        , -0.02493346, -0.        ,  0.02866736,\n",
       "         0.00859368, -0.00100741,  0.        , -0.        , -0.0055218 ,\n",
       "         0.        , -2.65212133, -2.6645978 , -0.        , -0.41641321,\n",
       "         2.62920201,  0.        ,  0.37755986, -0.        ,  0.0294911 ,\n",
       "         0.30947046,  0.04005852, -0.11775793, -0.01248713, -0.00093397,\n",
       "         0.14457225,  0.33001052]),\n",
       " array([ 0.        , -0.01407731, -0.1639496 , -0.00031268, -0.00070905,\n",
       "        -0.02777845, -0.00000075, -0.00015082,  0.00000123, -0.00015681,\n",
       "        -0.00360103, -0.00006545,  0.00176391,  0.00723844,  0.0010258 ,\n",
       "        -0.0089848 , -0.00110505,  0.00145945, -0.        , -0.00214474,\n",
       "        -0.        ,  0.38429081, -3.94974973,  1.27577281,  0.00527426,\n",
       "        -0.        ,  0.        , -0.02466905,  0.        ,  0.02738859,\n",
       "         0.00769262, -0.00069768,  0.        ,  0.        , -0.00478886,\n",
       "         0.        , -2.5016083 , -2.53165845, -0.        , -0.50273435,\n",
       "         2.5609507 ,  0.        ,  0.37908086, -0.        ,  0.02956422,\n",
       "         0.31069382,  0.03959639, -0.11781918, -0.01261432, -0.00086197,\n",
       "         0.14369645,  0.33625476]),\n",
       " array([ 0.        , -0.01402041, -0.16066515, -0.00023511, -0.00068353,\n",
       "        -0.02808773, -0.0000007 , -0.0001713 ,  0.        , -0.00015679,\n",
       "        -0.00357594, -0.00006956,  0.00172591,  0.00723498,  0.00100447,\n",
       "        -0.00899814, -0.00110691,  0.00144155, -0.00003166, -0.00210364,\n",
       "        -0.        ,  0.39996888, -3.89816872,  1.20297185,  0.00522325,\n",
       "        -0.        ,  0.        , -0.02436807,  0.        ,  0.02596149,\n",
       "         0.00669651, -0.00034885,  0.        ,  0.        , -0.00397472,\n",
       "         0.00242222, -2.34231952, -2.39253258, -0.        , -0.59674153,\n",
       "         2.48981941,  0.        ,  0.38146542, -0.        ,  0.02957323,\n",
       "         0.31159563,  0.03917226, -0.11774848, -0.01266328, -0.00079551,\n",
       "         0.14264507,  0.34363547]),\n",
       " array([ 0.        , -0.01402793, -0.15658305, -0.00020642, -0.00066108,\n",
       "        -0.02808784, -0.00000067, -0.0001845 ,  0.        , -0.00015516,\n",
       "        -0.0035275 , -0.00007108,  0.00168116,  0.00727219,  0.00098391,\n",
       "        -0.0090128 , -0.00110972,  0.00141869, -0.00015041, -0.00198685,\n",
       "        -0.        ,  0.42808195, -3.83919915,  1.122458  ,  0.0051606 ,\n",
       "        -0.        ,  0.        , -0.02400371,  0.        ,  0.02459662,\n",
       "         0.00557764, -0.        ,  0.        ,  0.        , -0.00303994,\n",
       "         0.01348229, -2.18095803, -2.23835506, -0.        , -0.69740784,\n",
       "         2.40118357, -0.        ,  0.38371666, -0.        ,  0.0295852 ,\n",
       "         0.31259554,  0.03866254, -0.11766391, -0.01273698, -0.00073518,\n",
       "         0.1415261 ,  0.35150645]),\n",
       " array([-0.        , -0.01401374, -0.15249054, -0.00018535, -0.00064204,\n",
       "        -0.02792712, -0.00000062, -0.00019327,  0.        , -0.00015305,\n",
       "        -0.0033778 , -0.00007352,  0.00163548,  0.00732049,  0.00096326,\n",
       "        -0.00902119, -0.00111258,  0.00139653, -0.00020778, -0.00189097,\n",
       "        -0.00003207,  0.4458802 , -3.76719125,  1.02902605,  0.00510412,\n",
       "        -0.        ,  0.        , -0.02365254,  0.        ,  0.02333759,\n",
       "         0.00443011, -0.        ,  0.        ,  0.        , -0.00203796,\n",
       "         0.03486257, -2.01954961, -2.07574494, -0.        , -0.80563843,\n",
       "         2.31639862, -0.        ,  0.385741  , -0.        ,  0.02965238,\n",
       "         0.31390583,  0.03808246, -0.11769827, -0.01285675, -0.00067072,\n",
       "         0.14041969,  0.36091755]),\n",
       " array([-0.        , -0.01400794, -0.1481074 , -0.00017052, -0.00062296,\n",
       "        -0.02772088, -0.00000052, -0.00019824,  0.        , -0.00015072,\n",
       "        -0.00318157, -0.0000759 ,  0.00158663,  0.0073714 ,  0.00094174,\n",
       "        -0.00903261, -0.00111541,  0.00137415, -0.00027158, -0.00179888,\n",
       "        -0.00006225,  0.46850274, -3.69567084,  0.9355691 ,  0.00504689,\n",
       "        -0.        ,  0.        , -0.0232961 ,  0.        ,  0.02196582,\n",
       "         0.00321264, -0.        ,  0.        ,  0.        , -0.00097934,\n",
       "         0.09207391, -1.85567964, -1.91292587, -0.        , -0.91068866,\n",
       "         2.1909167 , -0.        ,  0.38774576, -0.        ,  0.02973188,\n",
       "         0.31522503,  0.03745394, -0.11772817, -0.01298086, -0.00059753,\n",
       "         0.1392834 ,  0.37103344]),\n",
       " array([-0.        , -0.01400115, -0.14348128, -0.00016061, -0.00060329,\n",
       "        -0.02752185, -0.00000041, -0.00019988,  0.        , -0.00014828,\n",
       "        -0.0029745 , -0.00007869,  0.00153544,  0.00741887,  0.00091978,\n",
       "        -0.00904583, -0.00111789,  0.0013514 , -0.00032038, -0.0017082 ,\n",
       "        -0.00010245,  0.48918646, -3.61869496,  0.83759553,  0.00498371,\n",
       "        -0.        ,  0.        , -0.02290635,  0.        ,  0.02042467,\n",
       "         0.00205022, -0.        ,  0.        ,  0.        , -0.        ,\n",
       "         0.17793972, -1.6779961 , -1.74694454, -0.        , -1.01645267,\n",
       "         2.03374019, -0.        ,  0.38992164, -0.        ,  0.02978416,\n",
       "         0.3167417 ,  0.03683378, -0.11776097, -0.0131087 , -0.00053081,\n",
       "         0.13815599,  0.38016007]),\n",
       " array([-0.        , -0.0139552 , -0.13912244, -0.00015506, -0.00058697,\n",
       "        -0.02729612, -0.00000024, -0.00020043, -0.        , -0.00014536,\n",
       "        -0.00275158, -0.00008173,  0.00148732,  0.00746734,  0.00089884,\n",
       "        -0.00906241, -0.00112015,  0.00132855, -0.00037303, -0.00162312,\n",
       "        -0.00012186,  0.50922815, -3.51725491,  0.72646925,  0.00490572,\n",
       "        -0.        ,  0.        , -0.02242664,  0.        ,  0.01833394,\n",
       "         0.00183764, -0.        ,  0.        ,  0.        , -0.        ,\n",
       "         0.26390848, -1.42614302, -1.57742321, -0.        , -1.12333979,\n",
       "         1.87532285, -0.        ,  0.39252104, -0.        ,  0.0296607 ,\n",
       "         0.31921264,  0.03637932, -0.11789527, -0.01321201, -0.0005582 ,\n",
       "         0.13725724,  0.37673482]),\n",
       " array([-0.        , -0.01390297, -0.13452589, -0.00015097, -0.00056997,\n",
       "        -0.02704897, -0.00000009, -0.00019891, -0.        , -0.0001422 ,\n",
       "        -0.00250737, -0.00008487,  0.00143654,  0.00752032,  0.00087657,\n",
       "        -0.00907939, -0.00112259,  0.00130436, -0.0004232 , -0.00153632,\n",
       "        -0.00014368,  0.52997255, -3.40905832,  0.60725331,  0.00482215,\n",
       "         0.        ,  0.        , -0.02191176,  0.        ,  0.01605766,\n",
       "         0.00154158,  0.        ,  0.0000743 ,  0.        , -0.        ,\n",
       "         0.35512816, -1.16212514, -1.39999936, -0.        , -1.23492388,\n",
       "         1.70774653, -0.        ,  0.39526175,  0.        ,  0.02952819,\n",
       "         0.32186283,  0.03589679, -0.11804055, -0.0133221 , -0.00058601,\n",
       "         0.1362889 ,  0.37318711]),\n",
       " array([-0.        , -0.01384509, -0.12967251, -0.00014602, -0.00055203,\n",
       "        -0.0267823 , -0.        , -0.00019567, -0.        , -0.00013879,\n",
       "        -0.00224002, -0.00008811,  0.00138284,  0.0075791 ,  0.00085277,\n",
       "        -0.00909663, -0.00112523,  0.00127852, -0.00048193, -0.00144206,\n",
       "        -0.00016265,  0.55305032, -3.29260071,  0.47740315,  0.00473236,\n",
       "         0.        ,  0.        , -0.02135796,  0.        ,  0.01355977,\n",
       "         0.00109785,  0.        ,  0.00028385,  0.        , -0.        ,\n",
       "         0.44840081, -0.88552559, -1.2129903 , -0.        , -1.351839  ,\n",
       "         1.53337369, -0.        ,  0.39816332,  0.        ,  0.02938481,\n",
       "         0.32471557,  0.03538003, -0.11819663, -0.01344037, -0.00061537,\n",
       "         0.13523256,  0.3694166 ]),\n",
       " array([-0.        , -0.01379956, -0.12454205, -0.00014015, -0.000533  ,\n",
       "        -0.02650583, -0.        , -0.0001898 , -0.        , -0.0001351 ,\n",
       "        -0.00195344, -0.00009137,  0.00132607,  0.0076419 ,  0.0008277 ,\n",
       "        -0.00911378, -0.00112823,  0.00125109, -0.00054783, -0.00133667,\n",
       "        -0.00018471,  0.57400128, -3.17256145,  0.34651135,  0.00463405,\n",
       "         0.        ,  0.        , -0.02076493,  0.00018361,  0.01076583,\n",
       "         0.00059057,  0.        ,  0.00051166,  0.        , -0.        ,\n",
       "         0.53195011, -0.58236474, -1.01041551, -0.        , -1.47698025,\n",
       "         1.35285968, -0.        ,  0.40125969,  0.        ,  0.0292239 ,\n",
       "         0.32777342,  0.0348049 , -0.11834814, -0.01356903, -0.00064736,\n",
       "         0.13402936,  0.36521303]),\n",
       " array([-0.        , -0.01378479, -0.11892741, -0.00013338, -0.00051179,\n",
       "        -0.02622732, -0.        , -0.00018371, -0.        , -0.00013111,\n",
       "        -0.00166304, -0.00009511,  0.00126392,  0.00770883,  0.00080143,\n",
       "        -0.00913175, -0.00113167,  0.00122157, -0.00061457, -0.00121342,\n",
       "        -0.00022555,  0.59864328, -3.05266193,  0.21460298,  0.00452705,\n",
       "         0.        ,  0.        , -0.02014308,  0.000701  ,  0.00755552,\n",
       "         0.        ,  0.        ,  0.00074899,  0.        , -0.        ,\n",
       "         0.60715125, -0.2400934 , -0.78602058,  0.        , -1.61234083,\n",
       "         1.15354352, -0.        ,  0.40468202,  0.        ,  0.02903841,\n",
       "         0.3310263 ,  0.03412487, -0.1184729 , -0.01371132, -0.00068557,\n",
       "         0.13259856,  0.3602317 ]),\n",
       " array([ 0.        , -0.01377929, -0.11288972, -0.00012577, -0.00048881,\n",
       "        -0.02592805, -0.        , -0.00017729, -0.        , -0.0001268 ,\n",
       "        -0.00135377, -0.00009871,  0.00119696,  0.00778054,  0.00077277,\n",
       "        -0.00915256, -0.00113532,  0.00118955, -0.00069061, -0.00107336,\n",
       "        -0.00027515,  0.62933648, -2.92458686,  0.0722559 ,  0.00441275,\n",
       "         0.        ,  0.        , -0.01947598,  0.00103573,  0.00753125,\n",
       "         0.        ,  0.        ,  0.00036834,  0.00022172,  0.        ,\n",
       "         0.68839398, -0.        , -0.54599495,  0.        , -1.75633607,\n",
       "         0.93876372, -0.        ,  0.40837391,  0.        ,  0.02884394,\n",
       "         0.33450316,  0.03339275, -0.11861437, -0.0138608 , -0.00072912,\n",
       "         0.13104865,  0.3546205 ]),\n",
       " array([ 0.        , -0.01379329, -0.10649645, -0.00011765, -0.00046408,\n",
       "        -0.02560232, -0.        , -0.00017056, -0.        , -0.00012229,\n",
       "        -0.00103438, -0.00010118,  0.00112613,  0.00787032,  0.00074127,\n",
       "        -0.00917949, -0.00113925,  0.00115378, -0.00076931, -0.00091658,\n",
       "        -0.00034061,  0.5854541 , -2.78723567,  0.        ,  0.00428976,\n",
       "         0.        ,  0.        , -0.01874578,  0.00143036,  0.01294316,\n",
       "         0.        ,  0.        ,  0.        ,  0.0003558 ,  0.        ,\n",
       "         0.77642535, -0.        , -0.30218925,  0.        , -1.90060263,\n",
       "         0.71383692, -0.        ,  0.41233831,  0.        ,  0.02864532,\n",
       "         0.33811219,  0.03262193, -0.11877716, -0.01400502, -0.0007769 ,\n",
       "         0.12934849,  0.34849429]),\n",
       " array([-0.        , -0.01368188, -0.10002595, -0.00011031, -0.00043931,\n",
       "        -0.02519468, -0.        , -0.00016285, -0.        , -0.00011723,\n",
       "        -0.00072717, -0.00010296,  0.00105351,  0.00798686,  0.00070574,\n",
       "        -0.00920458, -0.00114302,  0.00111394, -0.00085431, -0.00076063,\n",
       "        -0.0003944 ,  0.43886976, -2.62195909,  0.        ,  0.00416792,\n",
       "         0.        ,  0.        , -0.01794346,  0.00187417,  0.01749091,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "         0.86776984,  0.        , -0.09229345,  0.        , -2.04639754,\n",
       "         0.52638119, -0.        ,  0.41623626,  0.        ,  0.02844476,\n",
       "         0.3418503 ,  0.03188881, -0.1190109 , -0.01409252, -0.00082176,\n",
       "         0.12763719,  0.3426099 ]),\n",
       " array([-0.        , -0.01346325, -0.09382173, -0.00010565, -0.00041118,\n",
       "        -0.02483931, -0.        , -0.00015335, -0.        , -0.00011184,\n",
       "        -0.0004234 , -0.00010486,  0.00098145,  0.00812833,  0.00066668,\n",
       "        -0.00922634, -0.00114828,  0.00107293, -0.0009463 , -0.00061287,\n",
       "        -0.00042906,  0.27704477, -2.44326517,  0.        ,  0.00403167,\n",
       "         0.        ,  0.        , -0.01704101,  0.00201632,  0.0194358 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "         0.96370906,  0.        , -0.        ,  0.        , -2.07085705,\n",
       "         0.33777064,  0.        ,  0.42101689,  0.        ,  0.02818668,\n",
       "         0.34543261,  0.03123279, -0.11918793, -0.01411729, -0.00086671,\n",
       "         0.12501178,  0.33603215]),\n",
       " array([-0.        , -0.0132085 , -0.08816092, -0.00010368, -0.00038297,\n",
       "        -0.0243989 , -0.        , -0.00014261, -0.        , -0.00010608,\n",
       "        -0.00014013, -0.00010514,  0.000913  ,  0.00830143,  0.0006246 ,\n",
       "        -0.00923684, -0.00115625,  0.00102855, -0.00104313, -0.00046016,\n",
       "        -0.00046422,  0.10431452, -2.25554556,  0.        ,  0.00387453,\n",
       "         0.        ,  0.        , -0.01601071,  0.00223113,  0.01915222,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "         1.09112792,  0.        , -0.        ,  0.        , -1.99941876,\n",
       "         0.11769689,  0.        ,  0.42588083,  0.        ,  0.02789339,\n",
       "         0.34908118,  0.03052778, -0.11933393, -0.01412286, -0.00091542,\n",
       "         0.12182289,  0.3297766 ]),\n",
       " array([-0.        , -0.01286121, -0.08274264, -0.00010362, -0.00035095,\n",
       "        -0.02405902, -0.        , -0.00013044, -0.        , -0.00010038,\n",
       "        -0.        , -0.00010474,  0.00084657,  0.00850044,  0.00057822,\n",
       "        -0.00923565, -0.00116367,  0.00098323, -0.00115681, -0.00028353,\n",
       "        -0.00052399,  0.        , -2.1089563 ,  0.        ,  0.00372608,\n",
       "         0.        ,  0.        , -0.01499553,  0.00169624,  0.01866938,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "         1.16480401,  0.        , -0.        ,  0.        , -1.93037318,\n",
       "         0.        ,  0.        ,  0.43114684,  0.        ,  0.02769011,\n",
       "         0.35231643,  0.02994572, -0.11958867, -0.01414191, -0.00096003,\n",
       "         0.11856132,  0.32179384]),\n",
       " array([-0.        , -0.0125132 , -0.07782179, -0.00010424, -0.00031603,\n",
       "        -0.023844  , -0.        , -0.00011825, -0.        , -0.00009441,\n",
       "        -0.        , -0.00010368,  0.00078319,  0.00871106,  0.00053215,\n",
       "        -0.0092264 , -0.00117249,  0.00094052, -0.00131295, -0.00015969,\n",
       "        -0.00051952,  0.        , -2.0425045 ,  0.        ,  0.00356859,\n",
       "         0.        ,  0.        , -0.01395391,  0.00182607,  0.01825189,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         1.09495088,  0.        , -0.        ,  0.        , -1.86748872,\n",
       "         0.        ,  0.        ,  0.43626034,  0.        ,  0.02751565,\n",
       "         0.3543658 ,  0.02914796, -0.11971544, -0.01407096, -0.000989  ,\n",
       "         0.11464202,  0.3124493 ]),\n",
       " array([-0.        , -0.01215054, -0.07275501, -0.0001053 , -0.00028042,\n",
       "        -0.0236767 , -0.        , -0.00010561, -0.        , -0.00008833,\n",
       "        -0.        , -0.00010338,  0.00071827,  0.00892364,  0.00048655,\n",
       "        -0.00921279, -0.00118101,  0.00089851, -0.00146504, -0.00006359,\n",
       "        -0.00048717,  0.        , -1.97959558,  0.        ,  0.00339904,\n",
       "         0.        ,  0.        , -0.01284788,  0.00201529,  0.01784071,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         1.02162957,  0.        , -0.        ,  0.        , -1.80502047,\n",
       "         0.        ,  0.        ,  0.44147417,  0.        ,  0.02731758,\n",
       "         0.35644598,  0.02831794, -0.11980035, -0.01399996, -0.00101002,\n",
       "         0.11055211,  0.30304432]),\n",
       " array([-0.        , -0.01176552, -0.06737082, -0.00010425, -0.00024192,\n",
       "        -0.02359514, -0.        , -0.00009388, -0.        , -0.00008213,\n",
       "        -0.        , -0.00010377,  0.0006498 ,  0.0091374 ,  0.00043998,\n",
       "        -0.00920025, -0.00118949,  0.0008562 , -0.00161683, -0.        ,\n",
       "        -0.00041961,  0.        , -1.9180159 ,  0.        ,  0.00321727,\n",
       "         0.        ,  0.        , -0.0116679 ,  0.0022032 ,  0.01742542,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.94759692,  0.        , -0.        ,  0.        , -1.74244418,\n",
       "         0.        ,  0.        ,  0.44699511,  0.        ,  0.02709463,\n",
       "         0.35855111,  0.02748929, -0.11985501, -0.01391983, -0.00102295,\n",
       "         0.10625704,  0.29333924]),\n",
       " array([-0.        , -0.0113284 , -0.06160168, -0.00010023, -0.00019985,\n",
       "        -0.02354492, -0.        , -0.00008355, -0.        , -0.00007535,\n",
       "        -0.        , -0.00010459,  0.00057632,  0.00937606,  0.00039029,\n",
       "        -0.00919036, -0.00119925,  0.00081126, -0.00177498, -0.        ,\n",
       "        -0.00028124,  0.        , -1.85763952,  0.        ,  0.00302677,\n",
       "         0.        ,  0.        , -0.01042664,  0.00240751,  0.01700654,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.86888671,  0.        , -0.        ,  0.        , -1.67661094,\n",
       "         0.        ,  0.        ,  0.45291578,  0.        ,  0.02686356,\n",
       "         0.36072509,  0.02657587, -0.11990335, -0.01383355, -0.0010262 ,\n",
       "         0.10165815,  0.28344025]),\n",
       " array([-0.        , -0.01086116, -0.05539719, -0.00009296, -0.00015439,\n",
       "        -0.02349896, -0.        , -0.00007461, -0.        , -0.0000681 ,\n",
       "        -0.        , -0.0001055 ,  0.00049728,  0.0096318 ,  0.00033696,\n",
       "        -0.00918022, -0.0012098 ,  0.00076291, -0.00192563, -0.        ,\n",
       "        -0.00015222,  0.        , -1.79237121,  0.        ,  0.00282244,\n",
       "         0.        ,  0.        , -0.00909507,  0.00262583,  0.01655819,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.78407276,  0.        , -0.        ,  0.        , -1.60546256,\n",
       "         0.        ,  0.        ,  0.45928406,  0.        ,  0.02661571,\n",
       "         0.36305198,  0.02559544, -0.11995492, -0.01374064, -0.00103037,\n",
       "         0.09671922,  0.27276072]),\n",
       " array([-0.        , -0.01036233, -0.0487137 , -0.00008191, -0.00010511,\n",
       "        -0.02346276, -0.        , -0.00006732, -0.        , -0.00006036,\n",
       "        -0.        , -0.00010652,  0.00041214,  0.00990495,  0.00027968,\n",
       "        -0.00917015, -0.00122121,  0.00071089, -0.00206591, -0.        ,\n",
       "        -0.00003561,  0.        , -1.72174926,  0.        ,  0.00260311,\n",
       "         0.        ,  0.        , -0.0076656 ,  0.00285837,  0.01607892,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.69281231,  0.        , -0.        ,  0.        , -1.52866421,\n",
       "         0.        ,  0.        ,  0.46614636,  0.        ,  0.02634931,\n",
       "         0.36554036,  0.02454612, -0.12000957, -0.01363992, -0.00103556,\n",
       "         0.09141326,  0.26122996]),\n",
       " array([-0.        , -0.00983112, -0.04150564, -0.00006761, -0.00005158,\n",
       "        -0.02343783, -0.        , -0.00006118, -0.        , -0.00005209,\n",
       "        -0.        , -0.00010767,  0.0003203 ,  0.01019607,  0.00021811,\n",
       "        -0.00916039, -0.00123354,  0.00065499, -0.00212831, -0.        ,\n",
       "        -0.        , -0.        , -1.64504061, -0.        ,  0.00236806,\n",
       "         0.        ,  0.        , -0.00613266,  0.00310581,  0.01556717,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.59477372,  0.        , -0.        ,  0.        , -1.44595622,\n",
       "         0.        ,  0.        ,  0.47354309,  0.        ,  0.02606436,\n",
       "         0.36818963,  0.02341971, -0.12006675, -0.01352952, -0.00104205,\n",
       "         0.08570559,  0.24882768]),\n",
       " array([-0.        , -0.00918647, -0.03455571, -0.00007428, -0.00000614,\n",
       "        -0.02294376, -0.        , -0.00004065, -0.        , -0.00004182,\n",
       "        -0.        , -0.00010727,  0.00022909,  0.01059891,  0.0001508 ,\n",
       "        -0.00912787, -0.00124842,  0.00059231, -0.00215698, -0.        ,\n",
       "        -0.        , -0.        , -1.56695273, -0.        ,  0.00213111,\n",
       "         0.        ,  0.        , -0.00457893,  0.00345072,  0.01474908,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.47728917,  0.00938176, -0.        ,  0.        , -1.34908549,\n",
       "         0.        ,  0.        ,  0.48057813,  0.        ,  0.0258194 ,\n",
       "         0.37132481,  0.0219058 , -0.12020264, -0.01346732, -0.00105023,\n",
       "         0.07964361,  0.23642879]),\n",
       " array([-0.        , -0.00839251, -0.02995849, -0.00007708, -0.        ,\n",
       "        -0.02166226, -0.        , -0.00003128, -0.        , -0.00002971,\n",
       "        -0.        , -0.00010513,  0.00016458,  0.01122783,  0.00009315,\n",
       "        -0.00903063, -0.00126074,  0.00053865, -0.00215006, -0.        ,\n",
       "        -0.        , -0.        , -1.51591158, -0.        ,  0.00190626,\n",
       "         0.        ,  0.        , -0.00313058,  0.00372803,  0.01302874,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.34653869,  0.05077188, -0.        ,  0.        , -1.23732083,\n",
       "         0.        ,  0.        ,  0.48446402,  0.        ,  0.02557129,\n",
       "         0.37500068,  0.02014078, -0.12031994, -0.01353797, -0.00102982,\n",
       "         0.07432603,  0.22415734]),\n",
       " array([-0.        , -0.00749675, -0.02532932, -0.00009716, -0.        ,\n",
       "        -0.02005405, -0.        , -0.00000969, -0.        , -0.00001574,\n",
       "        -0.        , -0.00010134,  0.00009821,  0.01196167,  0.00002953,\n",
       "        -0.00891857, -0.00127481,  0.00047979, -0.00214079, -0.        ,\n",
       "        -0.        , -0.        , -1.4635749 , -0.        ,  0.00167362,\n",
       "         0.        ,  0.        , -0.00161764,  0.00401332,  0.01127147,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.20368836,  0.08919127, -0.        ,  0.        , -1.11436367,\n",
       "         0.        ,  0.        ,  0.4882196 ,  0.        ,  0.02533076,\n",
       "         0.37913569,  0.0181589 , -0.12050521, -0.01363506, -0.00100522,\n",
       "         0.06870633,  0.21120159]),\n",
       " array([-0.        , -0.00670442, -0.02023363, -0.00010241, -0.        ,\n",
       "        -0.01854519, -0.        , -0.00000192, -0.        , -0.00000187,\n",
       "        -0.        , -0.00010754,  0.00002518,  0.01271964,  0.        ,\n",
       "        -0.00880763, -0.00128741,  0.00042178, -0.00213235, -0.        ,\n",
       "        -0.        , -0.        , -1.40803663, -0.        ,  0.00141909,\n",
       "         0.        ,  0.        , -0.00009307,  0.00437458,  0.00923392,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.04997988,  0.14407451, -0.        ,  0.        , -0.98480419,\n",
       "         0.        , -0.        ,  0.48990688,  0.        ,  0.02508223,\n",
       "         0.38323813,  0.0158353 , -0.12045257, -0.01389594, -0.00098189,\n",
       "         0.06269497,  0.19837018]),\n",
       " array([-0.        , -0.00622583, -0.01802149, -0.00009696, -0.        ,\n",
       "        -0.01734536, -0.        , -0.00000726, -0.        , -0.        ,\n",
       "        -0.        , -0.000116  ,  0.        ,  0.01315218,  0.        ,\n",
       "        -0.00875608, -0.00128336,  0.00036028, -0.00211071, -0.        ,\n",
       "        -0.        , -0.        , -1.37419325, -0.        ,  0.001407  ,\n",
       "         0.        ,  0.        , -0.        ,  0.00366488,  0.00819407,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        ,  0.11021827, -0.        ,  0.        , -0.90062995,\n",
       "         0.        , -0.        ,  0.49044568,  0.        ,  0.02456806,\n",
       "         0.38566826,  0.01375348, -0.11958148, -0.01406929, -0.00095984,\n",
       "         0.05765355,  0.18337841]),\n",
       " array([-0.        , -0.00568012, -0.01765311, -0.00009588, -0.        ,\n",
       "        -0.01623496, -0.        , -0.00000954, -0.        , -0.        ,\n",
       "        -0.        , -0.00012408,  0.        ,  0.01328757,  0.        ,\n",
       "        -0.00868562, -0.00128419,  0.00029983, -0.00208908, -0.        ,\n",
       "        -0.        , -0.        , -1.35387715, -0.        ,  0.00141195,\n",
       "         0.        ,  0.        , -0.        ,  0.00250634,  0.00743936,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        ,  0.04348768, -0.        ,  0.        , -0.843178  ,\n",
       "         0.        , -0.        ,  0.49054129,  0.        ,  0.02401063,\n",
       "         0.38753352,  0.01146383, -0.1183937 , -0.01423225, -0.00093716,\n",
       "         0.05307069,  0.17185224]),\n",
       " array([-0.        , -0.0050039 , -0.01713806, -0.00008657, -0.        ,\n",
       "        -0.01500155, -0.        , -0.00001784, -0.        , -0.        ,\n",
       "        -0.        , -0.00013478,  0.        ,  0.01342395,  0.        ,\n",
       "        -0.00860968, -0.00128619,  0.00022956, -0.0020131 , -0.        ,\n",
       "        -0.        , -0.        , -1.20576153, -0.17721775,  0.00141812,\n",
       "         0.        ,  0.        , -0.        ,  0.00117382,  0.00598114,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        ,  0.        , -0.        ,  0.        , -0.77703016,\n",
       "         0.        , -0.        ,  0.49101265,  0.        ,  0.02325165,\n",
       "         0.39053445,  0.00902584, -0.11693741, -0.01442426, -0.00092807,\n",
       "         0.04813302,  0.16241428]),\n",
       " array([-0.        , -0.0043188 , -0.01650904, -0.00009296, -0.        ,\n",
       "        -0.01382641, -0.        , -0.00001416, -0.        , -0.        ,\n",
       "        -0.        , -0.00014506,  0.        ,  0.01352399,  0.        ,\n",
       "        -0.00852673, -0.0012873 ,  0.00015614, -0.00191129, -0.        ,\n",
       "        -0.        , -0.        , -1.02204308, -0.41525446,  0.00142476,\n",
       "         0.        ,  0.        , -0.        ,  0.        ,  0.00331566,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        ,  0.        , -0.        ,  0.        , -0.71926475,\n",
       "         0.        , -0.        ,  0.49162273,  0.        ,  0.02236216,\n",
       "         0.39418089,  0.00667267, -0.11536821, -0.01461214, -0.00091552,\n",
       "         0.04298956,  0.15309842]),\n",
       " array([-0.        , -0.00396997, -0.01590118, -0.000111  , -0.        ,\n",
       "        -0.01348484, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00014623,  0.        ,  0.01343622,  0.        ,\n",
       "        -0.0084191 , -0.00128211,  0.00009742, -0.00179895, -0.        ,\n",
       "        -0.        , -0.        , -0.92320626, -0.59026149,  0.00143212,\n",
       "         0.        ,  0.        , -0.        ,  0.        ,  0.00009678,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        ,  0.        , -0.        ,  0.        , -0.71916978,\n",
       "         0.        , -0.        ,  0.49162442,  0.        ,  0.02134468,\n",
       "         0.39854809,  0.0054856 , -0.1140508 , -0.01472556, -0.00087061,\n",
       "         0.03846771,  0.14064127]),\n",
       " array([-0.        , -0.00360955, -0.01527499, -0.00010959, -0.        ,\n",
       "        -0.01319167, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00014728,  0.        ,  0.01333761,  0.        ,\n",
       "        -0.00829935, -0.00127657,  0.00003555, -0.00167683, -0.        ,\n",
       "        -0.        , -0.        , -0.81554217, -0.78018319,  0.00144041,\n",
       "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        ,  0.        , -0.        ,  0.        , -0.71835563,\n",
       "         0.        , -0.        ,  0.4915463 ,  0.        ,  0.02025669,\n",
       "         0.40325139,  0.00428508, -0.11266669, -0.01484783, -0.00082051,\n",
       "         0.03371036,  0.12688734]),\n",
       " array([ 0.        , -0.00318   , -0.01499875, -0.0001073 , -0.        ,\n",
       "        -0.01294168, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00015551,  0.        ,  0.0131665 ,  0.        ,\n",
       "        -0.00815912, -0.00126955,  0.        , -0.00158408, -0.        ,\n",
       "        -0.        , -0.        , -0.72410863, -0.95223639,  0.00144318,\n",
       "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        ,  0.        , -0.        , -0.        , -0.72073752,\n",
       "        -0.        , -0.        ,  0.49121219,  0.        ,  0.01911392,\n",
       "         0.40756074,  0.00286817, -0.11101651, -0.01497408, -0.00076843,\n",
       "         0.02842501,  0.11419747]),\n",
       " array([ 0.        , -0.00265811, -0.01519093, -0.00010385, -0.        ,\n",
       "        -0.01275568, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.0001732 ,  0.        ,  0.01290064,  0.        ,\n",
       "        -0.00799576, -0.0012607 ,  0.        , -0.00153034, -0.        ,\n",
       "        -0.        , -0.        , -0.65532896, -1.09896585,  0.00143864,\n",
       "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.72747806,\n",
       "        -0.        , -0.        ,  0.49055386,  0.        ,  0.01791022,\n",
       "         0.41126402,  0.00120201, -0.10903248, -0.01509886, -0.00071429,\n",
       "         0.02253847,  0.10307112]),\n",
       " array([ 0.        , -0.00194195, -0.01542926, -0.00009989, -0.        ,\n",
       "        -0.01260586, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00019077,  0.        ,  0.01261579,  0.        ,\n",
       "        -0.00784885, -0.00125123,  0.        , -0.00146382, -0.        ,\n",
       "        -0.        , -0.12145293, -0.58874999, -1.12625221,  0.00143026,\n",
       "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.73449028,\n",
       "        -0.        , -0.        ,  0.48974881,  0.        ,  0.01662812,\n",
       "         0.41554722,  0.        , -0.10716346, -0.01518504, -0.00064657,\n",
       "         0.01673683,  0.0903231 ]),\n",
       " array([ 0.        , -0.0008661 , -0.01565462, -0.00009513, -0.        ,\n",
       "        -0.01257639, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00020653,  0.        ,  0.01231846,  0.        ,\n",
       "        -0.00775775, -0.00124213,  0.        , -0.00136575, -0.        ,\n",
       "        -0.        , -0.34607965, -0.4960017 , -1.08433549,  0.00141471,\n",
       "         0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.74177176,\n",
       "        -0.        , -0.        ,  0.48873434,  0.        ,  0.01524682,\n",
       "         0.42068783,  0.        , -0.10562279, -0.01516729, -0.00055238,\n",
       "         0.01164902,  0.0757597 ]),\n",
       " array([ 0.        , -0.        , -0.01590655, -0.00009035, -0.        ,\n",
       "        -0.01263603, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00021929,  0.        ,  0.01202222,  0.        ,\n",
       "        -0.00766392, -0.00123439,  0.        , -0.00127015, -0.        ,\n",
       "        -0.        , -0.60412199, -0.40405163, -1.01248063,  0.00140142,\n",
       "         0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.74984221,\n",
       "        -0.        , -0.        ,  0.48715078,  0.        ,  0.01375788,\n",
       "         0.42601279,  0.        , -0.10398738, -0.01508398, -0.00045668,\n",
       "         0.00615783,  0.06003585]),\n",
       " array([ 0.        , -0.        , -0.01626718, -0.00008635, -0.        ,\n",
       "        -0.01298704, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00022042,  0.        ,  0.01177922,  0.        ,\n",
       "        -0.00757603, -0.00123201,  0.        , -0.00119508, -0.        ,\n",
       "        -0.        , -0.84887042, -0.32905766, -0.93566256,  0.00139807,\n",
       "         0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.75891988,\n",
       "        -0.        , -0.        ,  0.48391726,  0.        ,  0.01213934,\n",
       "         0.43097985,  0.        , -0.10221705, -0.01482277, -0.00037365,\n",
       "         0.00016962,  0.04270191]),\n",
       " array([ 0.        , -0.        , -0.01665497, -0.00008207, -0.        ,\n",
       "        -0.01336177, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00022162,  0.        ,  0.01151926, -0.        ,\n",
       "        -0.00748178, -0.00122951,  0.        , -0.00111489, -0.        ,\n",
       "        -0.        , -1.09749893, -0.24841089, -0.86691753,  0.00139473,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.7686633 ,\n",
       "        -0.        , -0.        ,  0.4804859 ,  0.        ,  0.01040511,\n",
       "         0.43626187,  0.        , -0.10031381, -0.01453982, -0.00028581,\n",
       "         0.        ,  0.02407635]),\n",
       " array([ 0.        , -0.        , -0.01707298, -0.00007747, -0.        ,\n",
       "        -0.01376268, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00022291,  0.        ,  0.01124052, -0.        ,\n",
       "        -0.00738081, -0.00122682,  0.        , -0.00102893, -0.        ,\n",
       "        -0.        , -1.34007327, -0.16174771, -0.81740636,  0.00139118,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.77911591,\n",
       "        -0.        , -0.        ,  0.47680534,  0.        ,  0.0085463 ,\n",
       "         0.44192516,  0.        , -0.09827449, -0.01423608, -0.00019175,\n",
       "         0.        ,  0.00412988]),\n",
       " array([ 0.        , -0.        , -0.01751046, -0.0000725 , -0.        ,\n",
       "        -0.01419651, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00022422,  0.        ,  0.0109363 , -0.        ,\n",
       "        -0.00726991, -0.00122361,  0.        , -0.00093703, -0.        ,\n",
       "        -0.        , -1.58004441, -0.05915237, -0.79287421,  0.0013878 ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.79046981,\n",
       "        -0.        , -0.        ,  0.47278974,  0.        ,  0.00661057,\n",
       "         0.44785787,  0.        , -0.09615457, -0.01390246, -0.00010211,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.01793878, -0.00006721, -0.        ,\n",
       "        -0.01468342, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00022491,  0.        ,  0.01061327, -0.        ,\n",
       "        -0.00715218, -0.00121972,  0.        , -0.00084877, -0.        ,\n",
       "        -0.        , -0.47206301, -0.        , -2.07466935,  0.00138369,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.80241767,\n",
       "        -0.        , -0.        ,  0.46852212,  0.        ,  0.00451088,\n",
       "         0.45445093,  0.        , -0.09394847, -0.01353497, -0.00001207,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.01859851, -0.00006168, -0.        ,\n",
       "        -0.01514751, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00022501,  0.        ,  0.01026839, -0.        ,\n",
       "        -0.00702921, -0.00121436,  0.        , -0.00076781, -0.        ,\n",
       "        -0.        , -0.56318496, -0.        , -2.10931647,  0.00137957,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.81434861,\n",
       "        -0.        , -0.        ,  0.46349278,  0.        ,  0.00228089,\n",
       "         0.46147845,  0.        , -0.09165541, -0.01314562, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.01930305, -0.00005576, -0.        ,\n",
       "        -0.01564186, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.000225  ,  0.        ,  0.00989813, -0.        ,\n",
       "        -0.00689625, -0.00120843,  0.        , -0.0006832 , -0.        ,\n",
       "        -0.        , -0.60117783, -0.        , -2.20295667,  0.0013752 ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.82703333,\n",
       "        -0.        , -0.        ,  0.4581278 ,  0.        ,  0.        ,\n",
       "         0.46839022,  0.        , -0.08928024, -0.01272976, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.01987225, -0.00004978, -0.        ,\n",
       "        -0.01618195, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00022216,  0.        ,  0.00949794, -0.        ,\n",
       "        -0.00672016, -0.0011993 ,  0.        , -0.00065132, -0.        ,\n",
       "        -0.        , -0.65014302, -0.        , -2.18093307,  0.0013677 ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.83982492,\n",
       "        -0.        , -0.        ,  0.45345477,  0.        ,  0.        ,\n",
       "         0.46185505,  0.        , -0.08867525, -0.01229219, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.02048225, -0.00004337, -0.        ,\n",
       "        -0.01676074, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00021911,  0.        ,  0.00906907, -0.        ,\n",
       "        -0.00653144, -0.00118953, -0.        , -0.00061715, -0.        ,\n",
       "        -0.        , -0.70372532, -0.        , -2.15622396,  0.00135966,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.85353324,\n",
       "        -0.        , -0.        ,  0.44844682,  0.        ,  0.        ,\n",
       "         0.4548515 ,  0.        , -0.08802688, -0.01182327, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.02113596, -0.0000365 , -0.        ,\n",
       "        -0.01738102, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00021585,  0.        ,  0.00860946, -0.        ,\n",
       "        -0.0063292 , -0.00117905, -0.        , -0.00058053, -0.        ,\n",
       "        -0.        , -0.76113389, -0.        , -2.12975785,  0.00135105,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.868224  ,\n",
       "        -0.        , -0.        ,  0.44307996,  0.        ,  0.        ,\n",
       "         0.44734601,  0.        , -0.08733205, -0.01132075, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.02183652, -0.00002914, -0.        ,\n",
       "        -0.01804574, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00021235, -0.        ,  0.00811691, -0.        ,\n",
       "        -0.00611246, -0.00116782, -0.        , -0.0005413 , -0.        ,\n",
       "        -0.        , -0.82277038, -0.        , -2.10128147,  0.00134182,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.88396761,\n",
       "        -0.        , -0.        ,  0.43732848,  0.        ,  0.        ,\n",
       "         0.43930263, -0.        , -0.08658742, -0.01078221, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.02258729, -0.00002125, -0.        ,\n",
       "        -0.01875811, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.0002086 , -0.        ,  0.00758907, -0.        ,\n",
       "        -0.0058802 , -0.00115579, -0.        , -0.00049924, -0.        ,\n",
       "        -0.        , -0.88888781, -0.        , -2.07070069,  0.00133192,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.90083953,\n",
       "        -0.        , -0.        ,  0.4311648 ,  0.        ,  0.        ,\n",
       "         0.43068279, -0.        , -0.08578943, -0.01020507, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.02339187, -0.0000128 , -0.        ,\n",
       "        -0.01952153, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00020458, -0.        ,  0.0070234 , -0.        ,\n",
       "        -0.00563128, -0.00114289, -0.        , -0.00045418, -0.        ,\n",
       "        -0.        , -0.95892496, -0.        , -2.03874645,  0.00132132,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.91892063,\n",
       "        -0.        , -0.        ,  0.42455939, -0.        ,  0.        ,\n",
       "         0.42144521, -0.        , -0.08493425, -0.00958657, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        , -0.        , -0.02425411, -0.00000373, -0.        ,\n",
       "        -0.02033967, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00020028, -0.        ,  0.00641718, -0.        ,\n",
       "        -0.00536453, -0.00112907, -0.        , -0.00040589, -0.        ,\n",
       "        -0.        , -1.03383725, -0.        , -2.0046463 ,  0.00130996,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.93829756,\n",
       "        -0.        , -0.        ,  0.41748058, -0.        ,  0.        ,\n",
       "         0.41154559, -0.        , -0.08401778, -0.00892374, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.02506615, -0.        , -0.        ,\n",
       "        -0.02103634, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00019703, -0.        ,  0.00588024, -0.        ,\n",
       "        -0.00509084, -0.00111989, -0.        , -0.0003684 , -0.        ,\n",
       "        -0.        , -1.09824751, -0.        , -1.96582817,  0.00129174,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.95360312,\n",
       "        -0.        , -0.        ,  0.40976208, -0.        ,  0.        ,\n",
       "         0.40084717, -0.        , -0.08288212, -0.00836131, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.02586138, -0.        , -0.        ,\n",
       "        -0.02166231, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00019446, -0.        ,  0.00538032, -0.        ,\n",
       "        -0.0048057 , -0.00111381, -0.        , -0.00033778, -0.        ,\n",
       "        -0.        , -1.15666686, -0.        , -1.92268038,  0.00126815,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.96634807,\n",
       "        -0.        , -0.        ,  0.40140173, -0.        ,  0.        ,\n",
       "         0.38932218, -0.        , -0.08156225, -0.00785764, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.0267136 , -0.        , -0.        ,\n",
       "        -0.02233315, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.0001917 , -0.        ,  0.00484458, -0.        ,\n",
       "        -0.00450013, -0.0011073 , -0.        , -0.00030497, -0.        ,\n",
       "        -0.        , -1.21924729, -0.        , -1.87646597,  0.00124288,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.98000641,\n",
       "        -0.        , -0.        ,  0.39244221, -0.        ,  0.        ,\n",
       "         0.37697123, -0.        , -0.08014778, -0.00731787, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.0276269 , -0.        , -0.        ,\n",
       "        -0.02305205, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00018875, -0.        ,  0.00427044, -0.        ,\n",
       "        -0.00417266, -0.00110033, -0.        , -0.00026981, -0.        ,\n",
       "        -0.        , -1.2862089 , -0.        , -1.82704323,  0.0012158 ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.99464362,\n",
       "        -0.        , -0.        ,  0.38284058, -0.        ,  0.        ,\n",
       "         0.36373511, -0.        , -0.07863194, -0.00673942, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.02860565, -0.        , -0.        ,\n",
       "        -0.02382249, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.00018559, -0.        ,  0.00365516, -0.        ,\n",
       "        -0.00382171, -0.00109285, -0.        , -0.00023213, -0.        ,\n",
       "        -0.        , -1.35804913, -0.        , -1.77399888,  0.00118678,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.01032984,\n",
       "        -0.        , -0.        ,  0.37255083, -0.        ,  0.        ,\n",
       "         0.3495504 , -0.        , -0.07700746, -0.00611952, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.02965455, -0.        , -0.        ,\n",
       "        -0.02464813, -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.0001822 , -0.        ,  0.00299577, -0.        ,\n",
       "        -0.00344562, -0.00108484, -0.        , -0.00019175, -0.        ,\n",
       "        -0.        , -1.43497437, -0.        , -1.71721655,  0.00115567,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.02714025,\n",
       "        -0.        , -0.        ,  0.36152363, -0.        ,  0.        ,\n",
       "         0.33434909, -0.        , -0.07526656, -0.00545518, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.03089208, -0.        , -0.        ,\n",
       "        -0.02548011, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017875, -0.        ,  0.00229402, -0.        ,\n",
       "        -0.00303488, -0.00107689, -0.        , -0.00015112, -0.        ,\n",
       "        -0.        , -2.81250168, -0.        , -0.36072077,  0.00112122,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.04422405,\n",
       "        -0.        , -0.        ,  0.34952495, -0.        ,  0.        ,\n",
       "         0.31838288, -0.        , -0.07343828, -0.00476731, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.03212857, -0.        , -0.        ,\n",
       "        -0.02644443, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017486, -0.        ,  0.00152728, -0.        ,\n",
       "        -0.00260583, -0.00106676, -0.        , -0.00010334, -0.        ,\n",
       "        -0.        , -2.83640957, -0.        , -0.36116389,  0.00108632,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.06431601,\n",
       "        -0.        , -0.        ,  0.33681286, -0.        ,  0.        ,\n",
       "         0.30074192, -0.        , -0.07141734, -0.00398803, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.03342773, -0.        , -0.        ,\n",
       "        -0.02746398, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017067, -0.        ,  0.00071379, -0.        ,\n",
       "        -0.00214412, -0.00105667, -0.        , -0.00005349, -0.        ,\n",
       "        -0.        , -2.87089927, -0.        , -0.3516442 ,  0.00104826,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.08520785,\n",
       "        -0.        , -0.        ,  0.32322441, -0.        ,  0.        ,\n",
       "         0.28197859, -0.        , -0.06926837, -0.00316585, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.0346825 , -0.        , -0.        ,\n",
       "        -0.02836681,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00016726, -0.        ,  0.        , -0.        ,\n",
       "        -0.00172723, -0.00105057, -0.        , -0.00001689, -0.        ,\n",
       "        -0.        , -2.90075583, -0.        , -0.32881004,  0.00100576,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.10273522,\n",
       "        -0.        , -0.        ,  0.30854284, -0.        ,  0.        ,\n",
       "         0.26085387, -0.        , -0.0666552 , -0.00240572, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.03515257, -0.        , -0.        ,\n",
       "        -0.02847916,  0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00016834, -0.        ,  0.        , -0.        ,\n",
       "        -0.00165122, -0.00106635, -0.        , -0.00005876, -0.        ,\n",
       "        -0.        , -2.8526783 , -0.        , -0.28923722,  0.00095002,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.09750426,\n",
       "        -0.        , -0.        ,  0.2926832 , -0.        ,  0.        ,\n",
       "         0.23405448, -0.        , -0.06246978, -0.00218821, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.03602609, -0.        , -0.        ,\n",
       "        -0.02848614, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00016917, -0.        ,  0.        , -0.        ,\n",
       "        -0.00157406, -0.00108337, -0.        , -0.00011621, -0.        ,\n",
       "        -0.        , -2.7545713 , -0.        , -0.28919373,  0.00089172,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.09256186,\n",
       "        -0.        , -0.        ,  0.27475472, -0.        ,  0.        ,\n",
       "         0.20487777, -0.        , -0.05794735, -0.00192251, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.03693323, -0.        , -0.        ,\n",
       "        -0.02850562, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017033, -0.        ,  0.        , -0.        ,\n",
       "        -0.00148949, -0.00110121, -0.        , -0.00017471, -0.        ,\n",
       "        -0.        , -2.66295843, -0.        , -0.27662464,  0.00082853,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.08687538,\n",
       "        -0.        , -0.        ,  0.25572771, -0.        ,  0.        ,\n",
       "         0.17386406, -0.        , -0.05312252, -0.00165744, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.03790943, -0.        , -0.        ,\n",
       "        -0.02852535, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017157, -0.        ,  0.        , -0.        ,\n",
       "        -0.00139876, -0.0011203 , -0.        , -0.00023748, -0.        ,\n",
       "        -0.        , -2.56515354, -0.        , -0.26276211,  0.00076078,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.08076247,\n",
       "        -0.        , -0.        ,  0.23533397, -0.        ,  0.        ,\n",
       "         0.1406423 , -0.        , -0.0479537 , -0.00137415, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.038961  , -0.        , -0.        ,\n",
       "        -0.02854443, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017287, -0.        ,  0.        , -0.        ,\n",
       "        -0.00130177, -0.00114081, -0.        , -0.00030518, -0.        ,\n",
       "        -0.        , -2.45702416, -0.        , -0.2510741 ,  0.00068828,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.07426526,\n",
       "        -0.        , -0.        ,  0.21345018, -0.        ,  0.        ,\n",
       "         0.10500343, -0.        , -0.04241129, -0.00106785, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.0400772 , -0.        , -0.        ,\n",
       "        -0.02856783, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017426, -0.        ,  0.        , -0.        ,\n",
       "        -0.00119742, -0.00116279, -0.        , -0.00037752, -0.        ,\n",
       "        -0.        , -2.34190171, -0.        , -0.2378746 ,  0.00061049,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.06726047,\n",
       "        -0.        , -0.        ,  0.19002616, -0.        ,  0.        ,\n",
       "         0.06684552, -0.        , -0.03647634, -0.00074095, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.04128188, -0.        , -0.        ,\n",
       "        -0.02858998, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017571, -0.        ,  0.        , -0.        ,\n",
       "        -0.00108568, -0.00118636, -0.        , -0.00045553, -0.        ,\n",
       "        -0.        , -2.21782124, -0.        , -0.22427934,  0.00052717,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.05977988,\n",
       "        -0.        , -0.        ,  0.1648956 , -0.        ,  0.        ,\n",
       "         0.02593937, -0.        , -0.03011573, -0.00038899, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.04242529, -0.        , -0.        ,\n",
       "        -0.02839343, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00017403, -0.        ,  0.        , -0.        ,\n",
       "        -0.00089732, -0.0012081 , -0.        , -0.00055725, -0.        ,\n",
       "        -0.        , -2.06324578, -0.        , -0.21240259,  0.00041934,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.04020011,\n",
       "        -0.        , -0.        ,  0.14095213, -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.02583197, -0.00015405, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.04340635, -0.        , -0.        ,\n",
       "        -0.0278486 , -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00016729, -0.        ,  0.        , -0.        ,\n",
       "        -0.00058728, -0.00122577, -0.        , -0.00069332, -0.        ,\n",
       "        -0.        , -1.86666175, -0.        , -0.20180785,  0.00027463,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -1.00109814,\n",
       "        -0.        , -0.        ,  0.11999601, -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.02517017, -0.00013329, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.04445736, -0.        , -0.        ,\n",
       "        -0.02726492, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00016007, -0.        ,  0.        , -0.        ,\n",
       "        -0.00025504, -0.00124471, -0.        , -0.00083912, -0.        ,\n",
       "        -0.        , -1.66588013, -0.        , -0.1805738 ,  0.00011955,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.95919309,\n",
       "        -0.        , -0.        ,  0.09753922, -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.02446069, -0.00011116, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.0455745 , -0.        , -0.        ,\n",
       "        -0.02663729, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00015068, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00125746, -0.        , -0.00099947, -0.        ,\n",
       "        -0.        , -1.49034543, -0.        , -0.11488273,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.91776812,\n",
       "        -0.        , -0.        ,  0.07391133, -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.02368166, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.0467916 , -0.        , -0.        ,\n",
       "        -0.02597882, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.0001366 , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00124873, -0.        , -0.00116358, -0.        ,\n",
       "        -0.        , -1.29381931, -0.        , -0.05059894,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.87966207,\n",
       "        -0.        , -0.        ,  0.05090947, -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.02248   , -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.0480825 , -0.        , -0.        ,\n",
       "        -0.02527657, -0.        , -0.        ,  0.        , -0.        ,\n",
       "         0.        , -0.00012153, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00123941, -0.        , -0.00133848, -0.        ,\n",
       "        -0.        , -1.06629024, -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.83879009,\n",
       "        -0.        , -0.        ,  0.0262919 , -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.02119611, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.04945974, -0.        , -0.        ,\n",
       "        -0.02452598, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00010537, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00122942, -0.        , -0.00152627, -0.        ,\n",
       "        -0.        , -0.76739999, -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.79500972,\n",
       "        -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.01981962, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        , -0.04588041, -0.        , -0.        ,\n",
       "        -0.02510613, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00009302, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00122842, -0.        , -0.00157536, -0.        ,\n",
       "        -0.        , -0.5299467 , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.74540949,\n",
       "        -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.01879568, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.04203834, -0.        , -0.        ,\n",
       "        -0.02572905, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00007979, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00122741, -0.        , -0.00162767, -0.        ,\n",
       "        -0.        , -0.2755925 , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.6922521 ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.0176982 , -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.03792515, -0.        , -0.        ,\n",
       "        -0.0263953 , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00006561, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00122631, -0.        , -0.0016838 , -0.        ,\n",
       "        -0.        , -0.00300461, -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.63528233,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01652245, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.03342255, -0.        , -0.        ,\n",
       "        -0.02703322, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00005492, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00122844, -0.        , -0.00159864, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.5661127 ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01615171, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.0285854 , -0.        , -0.        ,\n",
       "        -0.0277195 , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00004351, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00123077, -0.        , -0.00150555, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.49190325,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01576324, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.02341041, -0.        , -0.        ,\n",
       "        -0.02845242, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00003129, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00123324, -0.        , -0.00140597, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.41237034,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01534775, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.0179481 , -0.        , -0.        ,\n",
       "        -0.02922886, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00001811, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00123488, -0.        , -0.00129551, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.32596336,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01476784, -0.0001422 , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.01227809, -0.        , -0.        ,\n",
       "        -0.03003602, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00000407, -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00123486, -0.        , -0.00116975, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.23127957,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01390143, -0.00054645, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.00655172, -0.        , -0.        ,\n",
       "        -0.03082442, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00123247, -0.        , -0.00097988, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.12846736,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01290671, -0.00097802, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.00076122, -0.        , -0.        ,\n",
       "        -0.03159486, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.0012276 , -0.        , -0.00075924, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.01734515,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01178271, -0.00148422, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.03123711, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00123338, -0.        , -0.00063413, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.01090674, -0.00099349, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.03070284, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00124137, -0.        , -0.00051627, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.00998871, -0.00028364, -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.0302137 , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.00124733, -0.        , -0.00036557, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.00845575, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02974665, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00125157, -0.        , -0.00018966, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.0064783 , -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02925613, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00125574, -0.        , -0.00000174, -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        , -0.00435146, -0.        , -0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02882254, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00123514, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.00182603, -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02827697, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00120905, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02748955, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.0011736 , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02664582, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00113561, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02574164, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00109489, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02477251, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00105125, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02373548, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00100444, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02262213, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00095434, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02142937, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00090064, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.02015247, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00084304, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.01878266, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00078136, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.01731602, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00071521, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.01574287, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00064438, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.01405834, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00056841, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.01225172, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00048706, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.01031615, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00039985, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.00824229, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00030638, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "         0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.00601933, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00020623, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "         0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.0036374 , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.00009888, -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.00078775, -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         0.        , -0.        ]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
       " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "coefs_lasso=[array([-0.0011572 , -0.01268751, -0.19517721, -0.00232506, -0.00095865,\n",
    "        -0.02189267,  0.00001518, -0.00007647,  0.00003597, -0.00016014,\n",
    "        -0.00528631, -0.00001163,  0.00215662,  0.00709615,  0.00119132,\n",
    "        -0.00887771, -0.00106456,  0.00170027, -0.00280541, -0.00283531,\n",
    "         0.00325714, -1.78757765, -4.77413968,  4.29230745,  0.01517614,\n",
    "        -0.00000036, -0.00154459, -0.027606  ,  0.04584322,  0.05022409,\n",
    "         0.02432787, -0.05613505,  0.02529971,  0.00026255, -0.04323616,\n",
    "        -2.49930068, -4.35172699, -4.16099799,  2.21055382, -1.12430398,\n",
    "         3.99104722,  1.71065248,  0.35534885, -0.02126251,  0.0525133 ,\n",
    "         0.30755428,  0.06824135, -0.09719541, -0.01102796, -0.00094249,\n",
    "         0.15615256,  0.44456835]),\n",
    " array([-0.00115711, -0.01268753, -0.19517696, -0.00232504, -0.00095865,\n",
    "        -0.02189272,  0.00001518, -0.00007646,  0.00003597, -0.00016014,\n",
    "        -0.0052863 , -0.00001163,  0.00215662,  0.00709614,  0.00119132,\n",
    "        -0.00887771, -0.00106456,  0.00170027, -0.00280528, -0.00283532,\n",
    "         0.00325701, -1.7872718 , -4.77414312,  4.29200497,  0.0151759 ,\n",
    "        -0.00000036, -0.00154455, -0.02760603,  0.0458428 ,  0.05022399,\n",
    "         0.02432787, -0.05613412,  0.02529919,  0.00026218, -0.04323574,\n",
    "        -2.49923121, -4.35172481, -4.16099894,  2.21051466, -1.1242822 ,\n",
    "         3.99101044,  1.71063302,  0.355349  , -0.02126221,  0.05251299,\n",
    "         0.30755412,  0.06824095, -0.0971957 , -0.01102797, -0.00094248,\n",
    "         0.15615241,  0.44456755]),\n",
    " array([-0.00115702, -0.01268756, -0.19517669, -0.00232503, -0.00095865,\n",
    "        -0.02189277,  0.00001518, -0.00007645,  0.00003597, -0.00016014,\n",
    "        -0.00528628, -0.00001163,  0.00215661,  0.00709612,  0.00119132,\n",
    "        -0.00887772, -0.00106456,  0.00170027, -0.00280513, -0.00283534,\n",
    "         0.00325688, -1.78694403, -4.7741468 ,  4.2916808 ,  0.01517565,\n",
    "        -0.00000036, -0.00154451, -0.02760605,  0.04584235,  0.05022389,\n",
    "         0.02432786, -0.05613312,  0.02529864,  0.00026178, -0.04323528,\n",
    "        -2.49915675, -4.35172247, -4.16099995,  2.21047269, -1.12425886,\n",
    "         3.99097103,  1.71061215,  0.35534915, -0.02126188,  0.05251266,\n",
    "         0.30755395,  0.06824052, -0.09719602, -0.01102797, -0.00094248,\n",
    "         0.15615225,  0.4445667 ]),\n",
    " array([-0.00115691, -0.01268758, -0.1951764 , -0.00232502, -0.00095864,\n",
    "        -0.02189283,  0.00001518, -0.00007643,  0.00003597, -0.00016013,\n",
    "        -0.00528626, -0.00001163,  0.00215661,  0.00709611,  0.00119131,\n",
    "        -0.00887772, -0.00106456,  0.00170027, -0.00280498, -0.00283536,\n",
    "         0.00325674, -1.78659277, -4.77415074,  4.2913334 ,  0.01517539,\n",
    "        -0.00000036, -0.00154446, -0.02760607,  0.04584187,  0.05022378,\n",
    "         0.02432785, -0.05613206,  0.02529806,  0.00026135, -0.0432348 ,\n",
    "        -2.49907696, -4.35171997, -4.16100104,  2.21042771, -1.12423385,\n",
    "         3.99092879,  1.7105898 ,  0.35534932, -0.02126154,  0.0525123 ,\n",
    "         0.30755376,  0.06824006, -0.09719635, -0.01102798, -0.00094247,\n",
    "         0.15615208,  0.44456578]),\n",
    " array([-0.00115681, -0.01268761, -0.19517609, -0.002325  , -0.00095864,\n",
    "        -0.0218929 ,  0.00001518, -0.00007642,  0.00003597, -0.00016013,\n",
    "        -0.00528624, -0.00001163,  0.0021566 ,  0.00709609,  0.00119131,\n",
    "        -0.00887772, -0.00106456,  0.00170027, -0.00280481, -0.00283538,\n",
    "         0.00325659, -1.78621634, -4.77415497,  4.29096111,  0.0151751 ,\n",
    "        -0.00000036, -0.00154441, -0.0276061 ,  0.04584135,  0.05022366,\n",
    "         0.02432784, -0.05613091,  0.02529743,  0.00026089, -0.04323427,\n",
    "        -2.49899145, -4.35171728, -4.16100221,  2.21037951, -1.12420704,\n",
    "         3.99088352,  1.71056584,  0.35534949, -0.02126116,  0.05251192,\n",
    "         0.30755357,  0.06823957, -0.09719671, -0.01102799, -0.00094246,\n",
    "         0.15615189,  0.4445648 ]),\n",
    " array([-0.00115669, -0.01268764, -0.19517576, -0.00232499, -0.00095864,\n",
    "        -0.02189297,  0.00001518, -0.00007641,  0.00003597, -0.00016013,\n",
    "        -0.00528622, -0.00001163,  0.0021566 ,  0.00709607,  0.00119131,\n",
    "        -0.00887772, -0.00106456,  0.00170026, -0.00280463, -0.0028354 ,\n",
    "         0.00325643, -1.78581293, -4.7741595 ,  4.29056214,  0.01517479,\n",
    "        -0.00000036, -0.00154436, -0.02760613,  0.0458408 ,  0.05022354,\n",
    "         0.02432783, -0.05612968,  0.02529675,  0.0002604 , -0.04323372,\n",
    "        -2.49889981, -4.3517144 , -4.16100346,  2.21032786, -1.12417831,\n",
    "         3.99083501,  1.71054016,  0.35534968, -0.02126076,  0.05251151,\n",
    "         0.30755335,  0.06823904, -0.0971971 , -0.011028  , -0.00094245,\n",
    "         0.15615169,  0.44456375]),\n",
    " array([-0.00115656, -0.01268767, -0.1951754 , -0.00232497, -0.00095864,\n",
    "        -0.02189304,  0.00001518, -0.00007639,  0.00003597, -0.00016013,\n",
    "        -0.0052862 , -0.00001163,  0.00215659,  0.00709605,  0.00119131,\n",
    "        -0.00887772, -0.00106456,  0.00170026, -0.00280444, -0.00283542,\n",
    "         0.00325625, -1.78538061, -4.77416436,  4.29013457,  0.01517446,\n",
    "        -0.00000036, -0.00154431, -0.02760616,  0.04584021,  0.0502234 ,\n",
    "         0.02432782, -0.05612837,  0.02529602,  0.00025987, -0.04323312,\n",
    "        -2.4988016 , -4.35171132, -4.16100479,  2.2102725 , -1.12414753,\n",
    "         3.99078302,  1.71051264,  0.35534988, -0.02126033,  0.05251107,\n",
    "         0.30755313,  0.06823848, -0.09719751, -0.01102801, -0.00094245,\n",
    "         0.15615147,  0.44456262]),\n",
    " array([-0.00115643, -0.0126877 , -0.19517502, -0.00232495, -0.00095863,\n",
    "        -0.02189312,  0.00001518, -0.00007638,  0.00003597, -0.00016013,\n",
    "        -0.00528618, -0.00001163,  0.00215659,  0.00709603,  0.00119131,\n",
    "        -0.00887773, -0.00106456,  0.00170026, -0.00280423, -0.00283544,\n",
    "         0.00325607, -1.78491731, -4.77416956,  4.28967636,  0.01517411,\n",
    "        -0.00000036, -0.00154425, -0.02760619,  0.04583958,  0.05022325,\n",
    "         0.0243278 , -0.05612696,  0.02529525,  0.00025931, -0.04323247,\n",
    "        -2.49869635, -4.35170801, -4.16100623,  2.21021318, -1.12411454,\n",
    "         3.99072731,  1.71048315,  0.3553501 , -0.02125988,  0.0525106 ,\n",
    "         0.30755288,  0.06823788, -0.09719796, -0.01102802, -0.00094244,\n",
    "         0.15615124,  0.44456141]),\n",
    " array([-0.00115629, -0.01268773, -0.19517462, -0.00232493, -0.00095863,\n",
    "        -0.02189321,  0.00001518, -0.00007636,  0.00003597, -0.00016013,\n",
    "        -0.00528616, -0.00001163,  0.00215658,  0.00709601,  0.0011913 ,\n",
    "        -0.00887773, -0.00106456,  0.00170026, -0.00280401, -0.00283547,\n",
    "         0.00325587, -1.7844208 , -4.77417514,  4.28918531,  0.01517373,\n",
    "        -0.00000036, -0.00154418, -0.02760622,  0.0458389 ,  0.0502231 ,\n",
    "         0.02432779, -0.05612545,  0.02529442,  0.0002587 , -0.04323178,\n",
    "        -2.49858357, -4.35170447, -4.16100777,  2.21014961, -1.12407918,\n",
    "         3.99066761,  1.71045155,  0.35535033, -0.02125938,  0.0525101 ,\n",
    "         0.30755262,  0.06823723, -0.09719843, -0.01102803, -0.00094243,\n",
    "         0.156151  ,  0.44456011]),\n",
    " array([-0.00115613, -0.01268777, -0.19517418, -0.00232491, -0.00095862,\n",
    "        -0.0218933 ,  0.00001517, -0.00007634,  0.00003597, -0.00016013,\n",
    "        -0.00528613, -0.00001163,  0.00215658,  0.00709599,  0.0011913 ,\n",
    "        -0.00887773, -0.00106456,  0.00170026, -0.00280378, -0.00283549,\n",
    "         0.00325565, -1.78388871, -4.77418111,  4.28865906,  0.01517332,\n",
    "        -0.00000036, -0.00154411, -0.02760626,  0.04583817,  0.05022293,\n",
    "         0.02432778, -0.05612383,  0.02529352,  0.00025805, -0.04323105,\n",
    "        -2.49846269, -4.35170068, -4.16100942,  2.21008147, -1.12404129,\n",
    "         3.99060362,  1.71041768,  0.35535058, -0.02125886,  0.05250955,\n",
    "         0.30755234,  0.06823654, -0.09719894, -0.01102804, -0.00094242,\n",
    "         0.15615073,  0.44455873]),\n",
    " array([-0.00115597, -0.01268781, -0.19517371, -0.00232489, -0.00095862,\n",
    "        -0.0218934 ,  0.00001517, -0.00007632,  0.00003597, -0.00016013,\n",
    "        -0.0052861 , -0.00001163,  0.00215657,  0.00709596,  0.0011913 ,\n",
    "        -0.00887773, -0.00106456,  0.00170026, -0.00280353, -0.00283552,\n",
    "         0.00325542, -1.78331849, -4.77418752,  4.28809511,  0.01517289,\n",
    "        -0.00000036, -0.00154404, -0.0276063 ,  0.04583739,  0.05022275,\n",
    "         0.02432776, -0.05612209,  0.02529257,  0.00025736, -0.04323025,\n",
    "        -2.49833316, -4.35169661, -4.16101118,  2.21000846, -1.12400069,\n",
    "         3.99053505,  1.71038138,  0.35535085, -0.02125829,  0.05250898,\n",
    "         0.30755204,  0.06823579, -0.09719949, -0.01102805, -0.0009424 ,\n",
    "         0.15615045,  0.44455724]),\n",
    " array([-0.00115579, -0.01268786, -0.19517321, -0.00232486, -0.00095861,\n",
    "        -0.0218935 ,  0.00001517, -0.0000763 ,  0.00003597, -0.00016013,\n",
    "        -0.00528607, -0.00001163,  0.00215657,  0.00709594,  0.0011913 ,\n",
    "        -0.00887774, -0.00106456,  0.00170026, -0.00280325, -0.00283555,\n",
    "         0.00325518, -1.7827074 , -4.77419438,  4.28749073,  0.01517242,\n",
    "        -0.00000036, -0.00154396, -0.02760634,  0.04583656,  0.05022256,\n",
    "         0.02432774, -0.05612023,  0.02529155,  0.00025661, -0.04322941,\n",
    "        -2.49819434, -4.35169225, -4.16101307,  2.20993021, -1.12395717,\n",
    "         3.99046157,  1.71034249,  0.35535114, -0.02125769,  0.05250836,\n",
    "         0.30755172,  0.06823499, -0.09720007, -0.01102806, -0.00094239,\n",
    "         0.15615015,  0.44455565]),\n",
    " array([-0.00115561, -0.0126879 , -0.19517267, -0.00232484, -0.00095861,\n",
    "        -0.02189361,  0.00001517, -0.00007628,  0.00003597, -0.00016013,\n",
    "        -0.00528604, -0.00001163,  0.00215656,  0.00709591,  0.00119129,\n",
    "        -0.00887774, -0.00106456,  0.00170025, -0.00280296, -0.00283558,\n",
    "         0.00325491, -1.78205251, -4.77420173,  4.28684304,  0.01517192,\n",
    "        -0.00000036, -0.00154388, -0.02760638,  0.04583566,  0.05022235,\n",
    "         0.02432773, -0.05611824,  0.02529045,  0.00025581, -0.0432285 ,\n",
    "        -2.49804558, -4.35168758, -4.1610151 ,  2.20984636, -1.12391054,\n",
    "         3.99038282,  1.7103008 ,  0.35535145, -0.02125704,  0.05250769,\n",
    "         0.30755137,  0.06823414, -0.0972007 , -0.01102808, -0.00094238,\n",
    "         0.15614982,  0.44455394]),\n",
    " array([-0.0011554 , -0.01268795, -0.1951721 , -0.00232481, -0.0009586 ,\n",
    "        -0.02189373,  0.00001517, -0.00007626,  0.00003597, -0.00016013,\n",
    "        -0.00528601, -0.00001163,  0.00215655,  0.00709588,  0.00119129,\n",
    "        -0.00887775, -0.00106456,  0.00170025, -0.00280265, -0.00283562,\n",
    "         0.00325463, -1.78135069, -4.77420961,  4.28614893,  0.01517139,\n",
    "        -0.00000036, -0.00154379, -0.02760643,  0.0458347 ,  0.05022213,\n",
    "         0.02432771, -0.0561161 ,  0.02528927,  0.00025496, -0.04322753,\n",
    "        -2.49788615, -4.35168257, -4.16101727,  2.2097565 , -1.12386057,\n",
    "         3.99029842,  1.71025613,  0.35535178, -0.02125634,  0.05250698,\n",
    "         0.307551  ,  0.06823322, -0.09720137, -0.01102809, -0.00094237,\n",
    "         0.15614948,  0.44455211]),\n",
    " array([-0.00115519, -0.01268801, -0.19517148, -0.00232478, -0.0009586 ,\n",
    "        -0.02189386,  0.00001517, -0.00007623,  0.00003597, -0.00016013,\n",
    "        -0.00528597, -0.00001163,  0.00215654,  0.00709585,  0.00119128,\n",
    "        -0.00887775, -0.00106456,  0.00170025, -0.00280232, -0.00283566,\n",
    "         0.00325433, -1.78059857, -4.77421806,  4.28540508,  0.01517082,\n",
    "        -0.00000036, -0.00154369, -0.02760648,  0.04583367,  0.0502219 ,\n",
    "         0.02432769, -0.05611381,  0.02528801,  0.00025404, -0.04322648,\n",
    "        -2.4977153 , -4.3516772 , -4.1610196 ,  2.20966019, -1.12380701,\n",
    "         3.99020798,  1.71020826,  0.35535213, -0.0212556 ,  0.05250621,\n",
    "         0.30755061,  0.06823224, -0.09720209, -0.01102811, -0.00094235,\n",
    "         0.1561491 ,  0.44455015]),\n",
    " array([-0.00115495, -0.01268806, -0.19517082, -0.00232474, -0.00095859,\n",
    "        -0.021894  ,  0.00001517, -0.0000762 ,  0.00003597, -0.00016013,\n",
    "        -0.00528593, -0.00001163,  0.00215653,  0.00709581,  0.00119128,\n",
    "        -0.00887775, -0.00106456,  0.00170025, -0.00280196, -0.0028357 ,\n",
    "         0.00325401, -1.77979255, -4.77422711,  4.28460792,  0.0151702 ,\n",
    "        -0.00000036, -0.00154359, -0.02760654,  0.04583257,  0.05022164,\n",
    "         0.02432767, -0.05611136,  0.02528666,  0.00025306, -0.04322536,\n",
    "        -2.4975322 , -4.35167145, -4.1610221 ,  2.20955699, -1.12374961,\n",
    "         3.99011105,  1.71015695,  0.35535251, -0.0212548 ,  0.0525054 ,\n",
    "         0.30755018,  0.06823119, -0.09720286, -0.01102812, -0.00094233,\n",
    "         0.1561487 ,  0.44454804]),\n",
    " array([-0.0011547 , -0.01268813, -0.19517011, -0.00232471, -0.00095858,\n",
    "        -0.02189415,  0.00001517, -0.00007617,  0.00003597, -0.00016013,\n",
    "        -0.00528589, -0.00001163,  0.00215652,  0.00709577,  0.00119128,\n",
    "        -0.00887776, -0.00106456,  0.00170025, -0.00280158, -0.00283574,\n",
    "         0.00325366, -1.77892876, -4.77423681,  4.28375363,  0.01516954,\n",
    "        -0.00000036, -0.00154347, -0.0276066 ,  0.04583139,  0.05022137,\n",
    "         0.02432764, -0.05610873,  0.02528521,  0.00025201, -0.04322417,\n",
    "        -2.49733598, -4.35166529, -4.16102478,  2.20944638, -1.1236881 ,\n",
    "         3.99000718,  1.71010197,  0.35535291, -0.02125394,  0.05250452,\n",
    "         0.30754973,  0.06823007, -0.09720369, -0.01102814, -0.00094232,\n",
    "         0.15614828,  0.44454579]),\n",
    " array([-0.00115444, -0.01268819, -0.19516935, -0.00232467, -0.00095858,\n",
    "        -0.02189431,  0.00001517, -0.00007614,  0.00003597, -0.00016012,\n",
    "        -0.00528584, -0.00001163,  0.00215651,  0.00709573,  0.00119127,\n",
    "        -0.00887776, -0.00106456,  0.00170024, -0.00280117, -0.00283579,\n",
    "         0.00325329, -1.77800307, -4.77424721,  4.28283811,  0.01516884,\n",
    "        -0.00000036, -0.00154335, -0.02760666,  0.04583013,  0.05022108,\n",
    "         0.02432762, -0.05610592,  0.02528366,  0.00025088, -0.04322288,\n",
    "        -2.4971257 , -4.35165869, -4.16102764,  2.20932785, -1.12362219,\n",
    "         3.98989587,  1.71004305,  0.35535335, -0.02125302,  0.05250358,\n",
    "         0.30754924,  0.06822886, -0.09720458, -0.01102816, -0.0009423 ,\n",
    "         0.15614782,  0.44454338]),\n",
    " array([-0.00115415, -0.01268826, -0.19516853, -0.00232463, -0.00095857,\n",
    "        -0.02189448,  0.00001516, -0.00007611,  0.00003597, -0.00016012,\n",
    "        -0.00528579, -0.00001163,  0.0021565 ,  0.00709569,  0.00119127,\n",
    "        -0.00887777, -0.00106456,  0.00170024, -0.00280073, -0.00283584,\n",
    "         0.00325289, -1.77701104, -4.77425835,  4.28185698,  0.01516808,\n",
    "        -0.00000036, -0.00154323, -0.02760673,  0.04582877,  0.05022077,\n",
    "         0.02432759, -0.0561029 ,  0.025282  ,  0.00024967, -0.0432215 ,\n",
    "        -2.49690034, -4.35165161, -4.16103071,  2.20920083, -1.12355155,\n",
    "         3.98977657,  1.70997991,  0.35535381, -0.02125204,  0.05250257,\n",
    "         0.30754872,  0.06822756, -0.09720553, -0.01102818, -0.00094228,\n",
    "         0.15614732,  0.44454079]),\n",
    " array([-0.00115385, -0.01268834, -0.19516766, -0.00232459, -0.00095856,\n",
    "        -0.02189466,  0.00001516, -0.00007607,  0.00003596, -0.00016012,\n",
    "        -0.00528574, -0.00001163,  0.00215649,  0.00709565,  0.00119126,\n",
    "        -0.00887777, -0.00106456,  0.00170024, -0.00280026, -0.00283589,\n",
    "         0.00325246, -1.7759479 , -4.77427029,  4.28080553,  0.01516727,\n",
    "        -0.00000036, -0.00154309, -0.0276068 ,  0.04582732,  0.05022043,\n",
    "         0.02432756, -0.05609966,  0.02528022,  0.00024837, -0.04322003,\n",
    "        -2.49665884, -4.35164403, -4.16103401,  2.2090647 , -1.12347584,\n",
    "         3.98964873,  1.70991224,  0.35535431, -0.02125099,  0.05250149,\n",
    "         0.30754816,  0.06822618, -0.09720654, -0.0110282 , -0.00094226,\n",
    "         0.1561468 ,  0.44453802]),\n",
    " array([-0.00115352, -0.01268842, -0.19516673, -0.00232455, -0.00095855,\n",
    "        -0.02189486,  0.00001516, -0.00007603,  0.00003596, -0.00016012,\n",
    "        -0.00528569, -0.00001164,  0.00215648,  0.0070956 ,  0.00119126,\n",
    "        -0.00887778, -0.00106456,  0.00170023, -0.00279975, -0.00283595,\n",
    "         0.003252  , -1.77480858, -4.77428308,  4.27967873,  0.0151664 ,\n",
    "        -0.00000036, -0.00154294, -0.02760688,  0.04582576,  0.05022008,\n",
    "         0.02432753, -0.05609619,  0.02527831,  0.00024698, -0.04321845,\n",
    "        -2.49640003, -4.3516359 , -4.16103753,  2.20891882, -1.12339471,\n",
    "         3.98951172,  1.70983972,  0.35535485, -0.02124986,  0.05250033,\n",
    "         0.30754756,  0.06822469, -0.09720763, -0.01102823, -0.00094223,\n",
    "         0.15614623,  0.44453505]),\n",
    " array([-0.00115316, -0.0126885 , -0.19516572, -0.0023245 , -0.00095854,\n",
    "        -0.02189507,  0.00001516, -0.00007599,  0.00003596, -0.00016012,\n",
    "        -0.00528563, -0.00001164,  0.00215647,  0.00709554,  0.00119125,\n",
    "        -0.00887779, -0.00106457,  0.00170023, -0.00279921, -0.00283601,\n",
    "         0.00325151, -1.7735876 , -4.7742968 ,  4.27847117,  0.01516547,\n",
    "        -0.00000036, -0.00154279, -0.02760697,  0.04582409,  0.05021969,\n",
    "         0.0243275 , -0.05609248,  0.02527626,  0.00024549, -0.04321676,\n",
    "        -2.49612267, -4.35162719, -4.16104132,  2.20876248, -1.12330777,\n",
    "         3.9893649 ,  1.709762  ,  0.35535542, -0.02124865,  0.0524991 ,\n",
    "         0.30754691,  0.0682231 , -0.0972088 , -0.01102825, -0.00094221,\n",
    "         0.15614563,  0.44453186]),\n",
    " array([-0.00115279, -0.0126886 , -0.19516465, -0.00232444, -0.00095853,\n",
    "        -0.02189529,  0.00001516, -0.00007595,  0.00003596, -0.00016012,\n",
    "        -0.00528556, -0.00001164,  0.00215645,  0.00709549,  0.00119124,\n",
    "        -0.00887779, -0.00106457,  0.00170023, -0.00279863, -0.00283607,\n",
    "         0.00325098, -1.77227912, -4.77431149,  4.27717707,  0.01516447,\n",
    "        -0.00000036, -0.00154262, -0.02760706,  0.0458223 ,  0.05021928,\n",
    "         0.02432747, -0.0560885 ,  0.02527407,  0.0002439 , -0.04321494,\n",
    "        -2.49582543, -4.35161785, -4.16104537,  2.20859494, -1.12321459,\n",
    "         3.98920755,  1.70967871,  0.35535603, -0.02124735,  0.05249777,\n",
    "         0.30754622,  0.06822139, -0.09721006, -0.01102828, -0.00094218,\n",
    "         0.15614498,  0.44452845]),\n",
    " array([-0.00115238, -0.0126887 , -0.1951635 , -0.00232439, -0.00095852,\n",
    "        -0.02189553,  0.00001515, -0.0000759 ,  0.00003596, -0.00016012,\n",
    "        -0.00528549, -0.00001164,  0.00215644,  0.00709543,  0.00119124,\n",
    "        -0.0088778 , -0.00106457,  0.00170022, -0.00279801, -0.00283614,\n",
    "         0.00325042, -1.77087687, -4.77432724,  4.27579023,  0.0151634 ,\n",
    "        -0.00000036, -0.00154244, -0.02760715,  0.04582038,  0.05021884,\n",
    "         0.02432743, -0.05608423,  0.02527172,  0.00024219, -0.043213  ,\n",
    "        -2.49550689, -4.35160785, -4.16104971,  2.20841539, -1.12311474,\n",
    "         3.98903893,  1.70958946,  0.35535669, -0.02124596,  0.05249634,\n",
    "         0.30754548,  0.06821956, -0.0972114 , -0.01102831, -0.00094216,\n",
    "         0.15614428,  0.44452479]),\n",
    " array([-0.00115195, -0.0126888 , -0.19516226, -0.00232433, -0.00095851,\n",
    "        -0.02189579,  0.00001515, -0.00007585,  0.00003596, -0.00016011,\n",
    "        -0.00528542, -0.00001164,  0.00215642,  0.00709536,  0.00119123,\n",
    "        -0.00887781, -0.00106457,  0.00170022, -0.00279734, -0.00283622,\n",
    "         0.00324982, -1.76937411, -4.77434411,  4.27430399,  0.01516226,\n",
    "        -0.00000036, -0.00154224, -0.02760726,  0.04581833,  0.05021837,\n",
    "         0.02432739, -0.05607966,  0.0252692 ,  0.00024035, -0.04321091,\n",
    "        -2.49516552, -4.35159713, -4.16105436,  2.20822297, -1.12300773,\n",
    "         3.98885822,  1.7094938 ,  0.3553574 , -0.02124447,  0.05249482,\n",
    "         0.30754469,  0.0682176 , -0.09721284, -0.01102834, -0.00094213,\n",
    "         0.15614354,  0.44452088]),\n",
    " array([-0.00115149, -0.01268892, -0.19516094, -0.00232426, -0.00095849,\n",
    "        -0.02189607,  0.00001515, -0.00007579,  0.00003596, -0.00016011,\n",
    "        -0.00528534, -0.00001164,  0.0021564 ,  0.00709529,  0.00119122,\n",
    "        -0.00887782, -0.00106457,  0.00170021, -0.00279663, -0.0028363 ,\n",
    "         0.00324917, -1.76776366, -4.7743622 ,  4.27271124,  0.01516103,\n",
    "        -0.00000036, -0.00154204, -0.02760737,  0.04581613,  0.05021786,\n",
    "         0.02432735, -0.05607476,  0.0252665 ,  0.00023839, -0.04320868,\n",
    "        -2.49479968, -4.35158564, -4.16105935,  2.20801676, -1.12289305,\n",
    "         3.98866457,  1.7093913 ,  0.35535815, -0.02124288,  0.05249318,\n",
    "         0.30754384,  0.0682155 , -0.09721438, -0.01102838, -0.00094209,\n",
    "         0.15614274,  0.44451668]),\n",
    " array([-0.00115099, -0.01268904, -0.19515953, -0.00232419, -0.00095848,\n",
    "        -0.02189636,  0.00001515, -0.00007574,  0.00003596, -0.00016011,\n",
    "        -0.00528525, -0.00001164,  0.00215638,  0.00709522,  0.00119121,\n",
    "        -0.00887783, -0.00106457,  0.00170021, -0.00279586, -0.00283639,\n",
    "         0.00324847, -1.76603779, -4.77438158,  4.27100434,  0.01515972,\n",
    "        -0.00000036, -0.00154181, -0.02760748,  0.04581377,  0.05021732,\n",
    "         0.0243273 , -0.0560695 ,  0.02526361,  0.00023629, -0.04320628,\n",
    "        -2.49440763, -4.35157333, -4.1610647 ,  2.20779577, -1.12277016,\n",
    "         3.98845703,  1.70928144,  0.35535897, -0.02124117,  0.05249143,\n",
    "         0.30754293,  0.06821325, -0.09721603, -0.01102841, -0.00094206,\n",
    "         0.15614188,  0.44451217]),\n",
    " array([-0.00115045, -0.01268917, -0.19515801, -0.00232412, -0.00095846,\n",
    "        -0.02189668,  0.00001514, -0.00007567,  0.00003596, -0.00016011,\n",
    "        -0.00528516, -0.00001164,  0.00215636,  0.00709514,  0.0011912 ,\n",
    "        -0.00887784, -0.00106457,  0.0017002 , -0.00279504, -0.00283648,\n",
    "         0.00324773, -1.76418824, -4.77440235,  4.26917511,  0.01515831,\n",
    "        -0.00000036, -0.00154158, -0.02760761,  0.04581124,  0.05021674,\n",
    "         0.02432725, -0.05606387,  0.02526051,  0.00023403, -0.04320372,\n",
    "        -2.49398748, -4.35156013, -4.16107043,  2.20755895, -1.12263845,\n",
    "         3.98823462,  1.70916372,  0.35535983, -0.02123933,  0.05248955,\n",
    "         0.30754196,  0.06821084, -0.0972178 , -0.01102845, -0.00094202,\n",
    "         0.15614097,  0.44450735]),\n",
    " array([-0.00114988, -0.01268931, -0.19515638, -0.00232404, -0.00095845,\n",
    "        -0.02189702,  0.00001514, -0.00007561,  0.00003596, -0.00016011,\n",
    "        -0.00528507, -0.00001164,  0.00215634,  0.00709505,  0.00119119,\n",
    "        -0.00887785, -0.00106457,  0.0017002 , -0.00279416, -0.00283658,\n",
    "         0.00324693, -1.76220612, -4.77442461,  4.26721478,  0.01515679,\n",
    "        -0.00000036, -0.00154132, -0.02760775,  0.04580853,  0.05021611,\n",
    "         0.0243272 , -0.05605784,  0.02525719,  0.00023161, -0.04320097,\n",
    "        -2.49353722, -4.35154599, -4.16107656,  2.20730515, -1.12249731,\n",
    "         3.98799627,  1.70903755,  0.35536076, -0.02123737,  0.05248754,\n",
    "         0.30754092,  0.06820825, -0.0972197 , -0.01102849, -0.00094198,\n",
    "         0.15613998,  0.44450218]),\n",
    " array([-0.00114927, -0.01268946, -0.19515464, -0.00232395, -0.00095843,\n",
    "        -0.02189739,  0.00001514, -0.00007553,  0.00003596, -0.0001601 ,\n",
    "        -0.00528496, -0.00001165,  0.00215632,  0.00709496,  0.00119118,\n",
    "        -0.00887786, -0.00106457,  0.00170019, -0.00279322, -0.00283669,\n",
    "         0.00324608, -1.76008196, -4.77444847,  4.26511396,  0.01515518,\n",
    "        -0.00000036, -0.00154105, -0.02760789,  0.04580563,  0.05021544,\n",
    "         0.02432714, -0.05605138,  0.02525363,  0.00022902, -0.04319802,\n",
    "        -2.49305469, -4.35153084, -4.16108314,  2.20703317, -1.12234605,\n",
    "         3.98774083,  1.70890235,  0.35536176, -0.02123526,  0.05248538,\n",
    "         0.3075398 ,  0.06820548, -0.09722173, -0.01102854, -0.00094194,\n",
    "         0.15613893,  0.44449664]),\n",
    " array([-0.00114861, -0.01268963, -0.19515277, -0.00232386, -0.00095841,\n",
    "        -0.02189778,  0.00001513, -0.00007546,  0.00003596, -0.0001601 ,\n",
    "        -0.00528485, -0.00001165,  0.00215629,  0.00709486,  0.00119117,\n",
    "        -0.00887787, -0.00106457,  0.00170018, -0.00279221, -0.0028368 ,\n",
    "         0.00324516, -1.75780556, -4.77447403,  4.26286258,  0.01515344,\n",
    "        -0.00000036, -0.00154075, -0.02760805,  0.04580252,  0.05021473,\n",
    "         0.02432708, -0.05604445,  0.02524982,  0.00022625, -0.04319486,\n",
    "        -2.49253757, -4.3515146 , -4.16109019,  2.20674169, -1.12218395,\n",
    "         3.98746709,  1.70875745,  0.35536283, -0.02123301,  0.05248307,\n",
    "         0.3075386 ,  0.06820251, -0.09722391, -0.01102858, -0.00094189,\n",
    "         0.1561378 ,  0.44449071]),\n",
    " array([-0.00114791, -0.0126898 , -0.19515077, -0.00232377, -0.00095839,\n",
    "        -0.0218982 ,  0.00001513, -0.00007537,  0.00003596, -0.0001601 ,\n",
    "        -0.00528473, -0.00001165,  0.00215627,  0.00709476,  0.00119116,\n",
    "        -0.00887788, -0.00106457,  0.00170018, -0.00279113, -0.00283692,\n",
    "         0.00324418, -1.75536602, -4.77450143,  4.26044985,  0.01515158,\n",
    "        -0.00000036, -0.00154044, -0.02760822,  0.04579918,  0.05021396,\n",
    "         0.02432702, -0.05603703,  0.02524573,  0.00022327, -0.04319148,\n",
    "        -2.4919834 , -4.3514972 , -4.16109775,  2.20642932, -1.12201024,\n",
    "         3.98717374,  1.70860217,  0.35536398, -0.02123059,  0.05248059,\n",
    "         0.30753731,  0.06819933, -0.09722625, -0.01102864, -0.00094184,\n",
    "         0.15613659,  0.44448434]),\n",
    " array([-0.00114716, -0.01268999, -0.19514862, -0.00232366, -0.00095837,\n",
    "        -0.02189865,  0.00001512, -0.00007528,  0.00003595, -0.0001601 ,\n",
    "        -0.0052846 , -0.00001165,  0.00215624,  0.00709464,  0.00119115,\n",
    "        -0.0088779 , -0.00106457,  0.00170017, -0.00278997, -0.00283705,\n",
    "         0.00324313, -1.75275177, -4.77453072,  4.25786426,  0.01514959,\n",
    "        -0.00000036, -0.0015401 , -0.0276084 ,  0.04579561,  0.05021314,\n",
    "         0.02432694, -0.05602907,  0.02524135,  0.00022008, -0.04318785,\n",
    "        -2.49138945, -4.35147861, -4.16110586,  2.20609454, -1.121824  ,\n",
    "         3.98685932,  1.70843571,  0.3553652 , -0.021228  ,  0.05247794,\n",
    "         0.30753594,  0.06819592, -0.09722875, -0.01102869, -0.00094179,\n",
    "         0.15613529,  0.44447752]),\n",
    " array([-0.00114635, -0.01269018, -0.19514632, -0.00232355, -0.00095835,\n",
    "        -0.02189913,  0.00001512, -0.00007519,  0.00003595, -0.00016009,\n",
    "        -0.00528447, -0.00001165,  0.00215621,  0.00709452,  0.00119113,\n",
    "        -0.00887791, -0.00106457,  0.00170016, -0.00278873, -0.00283719,\n",
    "         0.003242  , -1.74995003, -4.77456218,  4.25509331,  0.01514745,\n",
    "        -0.00000036, -0.00153974, -0.02760859,  0.04579178,  0.05021226,\n",
    "         0.02432687, -0.05602054,  0.02523665,  0.00021667, -0.04318396,\n",
    "        -2.49075299, -4.35145863, -4.16111453,  2.20573579, -1.12162449,\n",
    "         3.9865224 ,  1.70825738,  0.35536652, -0.02122522,  0.05247509,\n",
    "         0.30753446,  0.06819227, -0.09723143, -0.01102875, -0.00094174,\n",
    "         0.15613391,  0.44447021]),\n",
    " array([-0.00114548, -0.0126904 , -0.19514386, -0.00232343, -0.00095833,\n",
    "        -0.02189964,  0.00001512, -0.00007509,  0.00003595, -0.00016009,\n",
    "        -0.00528432, -0.00001166,  0.00215618,  0.00709439,  0.00119112,\n",
    "        -0.00887793, -0.00106458,  0.00170015, -0.00278739, -0.00283734,\n",
    "         0.0032408 , -1.7469475 , -4.77459589,  4.25212377,  0.01514516,\n",
    "        -0.00000036, -0.00153935, -0.0276088 ,  0.04578767,  0.05021131,\n",
    "         0.02432679, -0.05601141,  0.02523162,  0.00021301, -0.0431798 ,\n",
    "        -2.49007092, -4.35143721, -4.16112383,  2.20535134, -1.12141068,\n",
    "         3.98616134,  1.70806626,  0.35536793, -0.02122225,  0.05247204,\n",
    "         0.30753288,  0.06818835, -0.09723431, -0.01102881, -0.00094168,\n",
    "         0.15613242,  0.44446238]),\n",
    " array([-0.00114456, -0.01269063, -0.19514121, -0.0023233 , -0.0009583 ,\n",
    "        -0.0219002 ,  0.00001511, -0.00007498,  0.00003595, -0.00016009,\n",
    "        -0.00528416, -0.00001166,  0.00215614,  0.00709425,  0.0011911 ,\n",
    "        -0.00887794, -0.00106458,  0.00170014, -0.00278597, -0.0028375 ,\n",
    "         0.0032395 , -1.74372979, -4.77463202,  4.24894141,  0.01514271,\n",
    "        -0.00000036, -0.00153894, -0.02760902,  0.04578327,  0.0502103 ,\n",
    "         0.0243267 , -0.05600162,  0.02522623,  0.00020908, -0.04317534,\n",
    "        -2.48933997, -4.35141427, -4.1611338 ,  2.20493932, -1.12118154,\n",
    "         3.9857744 ,  1.70786144,  0.35536944, -0.02121906,  0.05246878,\n",
    "         0.30753118,  0.06818416, -0.09723739, -0.01102888, -0.00094161,\n",
    "         0.15613082,  0.44445399]),\n",
    " array([-0.00114356, -0.01269087, -0.19513838, -0.00232316, -0.00095827,\n",
    "        -0.02190079,  0.0000151 , -0.00007486,  0.00003595, -0.00016008,\n",
    "        -0.00528399, -0.00001166,  0.0021561 ,  0.0070941 ,  0.00119109,\n",
    "        -0.00887796, -0.00106458,  0.00170013, -0.00278444, -0.00283768,\n",
    "         0.00323811, -1.74028147, -4.77467074,  4.24553099,  0.01514008,\n",
    "        -0.00000036, -0.00153849, -0.02760925,  0.04577856,  0.05020922,\n",
    "         0.02432661, -0.05599112,  0.02522045,  0.00020488, -0.04317055,\n",
    "        -2.48855663, -4.35138967, -4.16114448,  2.20449778, -1.12093599,\n",
    "         3.98535973,  1.70764194,  0.35537106, -0.02121564,  0.05246528,\n",
    "         0.30752937,  0.06817966, -0.09724069, -0.01102895, -0.00094154,\n",
    "         0.15612911,  0.444445  ]),\n",
    " array([-0.0011425 , -0.01269113, -0.19513535, -0.00232301, -0.00095824,\n",
    "        -0.02190142,  0.0000151 , -0.00007473,  0.00003595, -0.00016008,\n",
    "        -0.00528381, -0.00001166,  0.00215606,  0.00709394,  0.00119107,\n",
    "        -0.00887798, -0.00106458,  0.00170012, -0.00278277, -0.00283786,\n",
    "         0.0032366 , -1.73658664, -4.77471232,  4.24187681,  0.01513726,\n",
    "        -0.00000036, -0.00153802, -0.02760951,  0.04577351,  0.05020806,\n",
    "         0.02432651, -0.05597987,  0.02521426,  0.00020037, -0.04316543,\n",
    "        -2.48771708, -4.35136314, -4.16115582,  2.20402457, -1.12067295,\n",
    "         3.98491532,  1.70740671,  0.35537279, -0.02121198,  0.05246152,\n",
    "         0.30752742,  0.06817484, -0.09724423, -0.01102903, -0.00094147,\n",
    "         0.15612728,  0.44443536]),\n",
    " array([-0.00114135, -0.01269141, -0.1951321 , -0.00232285, -0.00095821,\n",
    "        -0.0219021 ,  0.00001509, -0.0000746 ,  0.00003595, -0.00016007,\n",
    "        -0.00528362, -0.00001167,  0.00215602,  0.00709377,  0.00119105,\n",
    "        -0.008878  , -0.00106458,  0.00170011, -0.00278101, -0.00283806,\n",
    "         0.00323501, -1.73262639, -4.7747568 ,  4.23796007,  0.01513424,\n",
    "        -0.00000036, -0.00153751, -0.02760978,  0.04576809,  0.05020681,\n",
    "         0.02432641, -0.05596782,  0.02520762,  0.00019554, -0.04315993,\n",
    "        -2.48681744, -4.35133488, -4.16116808,  2.20351747, -1.12039095,\n",
    "         3.98443908,  1.70715462,  0.35537465, -0.02120805,  0.0524575 ,\n",
    "         0.30752533,  0.06816967, -0.09724802, -0.01102911, -0.00094139,\n",
    "         0.15612531,  0.44442504]),\n",
    " array([-0.00114013, -0.01269172, -0.19512861, -0.00232268, -0.00095818,\n",
    "        -0.02190283,  0.00001509, -0.00007446,  0.00003594, -0.00016007,\n",
    "        -0.00528341, -0.00001167,  0.00215597,  0.00709359,  0.00119103,\n",
    "        -0.00887802, -0.00106458,  0.0017001 , -0.00277913, -0.00283827,\n",
    "         0.0032333 , -1.72838232, -4.77480446,  4.23376264,  0.01513101,\n",
    "        -0.00000036, -0.00153696, -0.02761007,  0.04576229,  0.05020548,\n",
    "         0.0243263 , -0.05595491,  0.02520051,  0.00019037, -0.04315404,\n",
    "        -2.48585332, -4.3513046 , -4.16118121,  2.20297403, -1.12008873,\n",
    "         3.98392871,  1.70688447,  0.35537664, -0.02120385,  0.05245319,\n",
    "         0.3075231 ,  0.06816414, -0.09725208, -0.0110292 , -0.0009413 ,\n",
    "         0.15612321,  0.44441397]),\n",
    " array([-0.00113882, -0.01269204, -0.19512488, -0.0023225 , -0.00095814,\n",
    "        -0.02190361,  0.00001508, -0.0000743 ,  0.00003594, -0.00016006,\n",
    "        -0.00528318, -0.00001167,  0.00215592,  0.00709339,  0.00119101,\n",
    "        -0.00887805, -0.00106459,  0.00170009, -0.00277711, -0.0028385 ,\n",
    "         0.00323147, -1.72383408, -4.77485554,  4.22926438,  0.01512754,\n",
    "        -0.00000036, -0.00153638, -0.02761039,  0.04575607,  0.05020404,\n",
    "         0.02432617, -0.05594107,  0.02519289,  0.00018482, -0.04314773,\n",
    "        -2.4848201 , -4.35127214, -4.16119529,  2.20239164, -1.11976486,\n",
    "         3.98338177,  1.70659496,  0.35537878, -0.02119934,  0.05244857,\n",
    "         0.3075207 ,  0.0681582 , -0.09725643, -0.0110293 , -0.00094121,\n",
    "         0.15612095,  0.4444021 ]),\n",
    " array([-0.00113741, -0.01269239, -0.19512088, -0.00232231, -0.0009581 ,\n",
    "        -0.02190445,  0.00001507, -0.00007414,  0.00003594, -0.00016006,\n",
    "        -0.00528295, -0.00001168,  0.00215587,  0.00709318,  0.00119098,\n",
    "        -0.00887807, -0.00106459,  0.00170007, -0.00277495, -0.00283875,\n",
    "         0.0032295 , -1.71895988, -4.77491028,  4.22444375,  0.01512383,\n",
    "        -0.00000036, -0.00153575, -0.02761072,  0.04574941,  0.05020251,\n",
    "         0.02432604, -0.05592623,  0.02518472,  0.00017888, -0.04314097,\n",
    "        -2.48371283, -4.35123736, -4.16121038,  2.20176752, -1.11941777,\n",
    "         3.98279562,  1.7062847 ,  0.35538107, -0.02119451,  0.05244363,\n",
    "         0.30751813,  0.06815185, -0.0972611 , -0.0110294 , -0.00094112,\n",
    "         0.15611854,  0.44438939]),\n",
    " array([-0.00113591, -0.01269276, -0.19511659, -0.0023221 , -0.00095806,\n",
    "        -0.02190535,  0.00001506, -0.00007396,  0.00003594, -0.00016005,\n",
    "        -0.00528269, -0.00001168,  0.00215581,  0.00709296,  0.00119096,\n",
    "        -0.0088781 , -0.00106459,  0.00170006, -0.00277263, -0.00283901,\n",
    "         0.0032274 , -1.71373636, -4.77496894,  4.21927763,  0.01511984,\n",
    "        -0.00000036, -0.00153507, -0.02761108,  0.04574227,  0.05020087,\n",
    "         0.0243259 , -0.05591034,  0.02517597,  0.00017251, -0.04313373,\n",
    "        -2.48252621, -4.35120009, -4.16122655,  2.20109866, -1.11904581,\n",
    "         3.98216747,  1.7059522 ,  0.35538352, -0.02118933,  0.05243832,\n",
    "         0.30751538,  0.06814503, -0.0972661 , -0.01102951, -0.00094101,\n",
    "         0.15611595,  0.44437577]),\n",
    " array([-0.00113429, -0.01269315, -0.195112  , -0.00232187, -0.00095801,\n",
    "        -0.02190631,  0.00001505, -0.00007377,  0.00003593, -0.00016004,\n",
    "        -0.00528241, -0.00001169,  0.00215575,  0.00709271,  0.00119093,\n",
    "        -0.00887813, -0.00106459,  0.00170004, -0.00277014, -0.00283929,\n",
    "         0.00322515, -1.70813848, -4.7750318 ,  4.21374127,  0.01511558,\n",
    "        -0.00000036, -0.00153435, -0.02761146,  0.04573461,  0.05019911,\n",
    "         0.02432576, -0.0558933 ,  0.02516658,  0.00016568, -0.04312596,\n",
    "        -2.48125454, -4.35116014, -4.16124387,  2.20038187, -1.11864719,\n",
    "         3.9814943 ,  1.70559587,  0.35538615, -0.02118379,  0.05243264,\n",
    "         0.30751243,  0.06813773, -0.09727146, -0.01102962, -0.0009409 ,\n",
    "         0.15611317,  0.44436117]),\n",
    " array([-0.00113256, -0.01269358, -0.19510707, -0.00232163, -0.00095797,\n",
    "        -0.02190734,  0.00001504, -0.00007356,  0.00003593, -0.00016004,\n",
    "        -0.00528212, -0.00001169,  0.00215569,  0.00709246,  0.0011909 ,\n",
    "        -0.00887816, -0.00106459,  0.00170002, -0.00276748, -0.00283959,\n",
    "         0.00322273, -1.70213942, -4.77509917,  4.20780813,  0.015111  ,\n",
    "        -0.00000035, -0.00153358, -0.02761188,  0.04572641,  0.05019722,\n",
    "         0.0243256 , -0.05587504,  0.02515653,  0.00015836, -0.04311764,\n",
    "        -2.47989174, -4.35111734, -4.16126244,  2.19961371, -1.11822   ,\n",
    "         3.98077289,  1.70521401,  0.35538896, -0.02117784,  0.05242655,\n",
    "         0.30750927,  0.06812991, -0.0972772 , -0.01102975, -0.00094078,\n",
    "         0.1561102 ,  0.44434553]),\n",
    " array([-0.00113071, -0.01269404, -0.19510181, -0.00232137, -0.00095791,\n",
    "        -0.02190843,  0.00001503, -0.00007335,  0.00003593, -0.00016003,\n",
    "        -0.00528181, -0.0000117 ,  0.00215562,  0.00709218,  0.00119087,\n",
    "        -0.00887819, -0.0010646 ,  0.00170001, -0.00276462, -0.0028399 ,\n",
    "         0.00322012, -1.69570958, -4.77516968,  4.20144839,  0.0151061 ,\n",
    "        -0.00000035, -0.00153275, -0.02761231,  0.04571763,  0.05019522,\n",
    "         0.02432543, -0.05585548,  0.02514576,  0.00015052, -0.04310872,\n",
    "        -2.47843231, -4.3510728 , -4.16128289,  2.19879083, -1.11776126,\n",
    "         3.98000001,  1.70480502,  0.35539198, -0.02117147,  0.05242001,\n",
    "         0.30750591,  0.06812152, -0.09728336, -0.01102988, -0.00094065,\n",
    "         0.15610701,  0.44432866]),\n",
    " array([-0.00112873, -0.01269453, -0.19509617, -0.0023211 , -0.00095786,\n",
    "        -0.02190961,  0.00001502, -0.00007311,  0.00003593, -0.00016002,\n",
    "        -0.00528147, -0.0000117 ,  0.00215554,  0.00709188,  0.00119083,\n",
    "        -0.00887823, -0.0010646 ,  0.00169999, -0.00276156, -0.00284024,\n",
    "         0.00321735, -1.68881973, -4.77524677,  4.19463408,  0.01510085,\n",
    "        -0.00000035, -0.00153186, -0.02761279,  0.0457082 ,  0.05019306,\n",
    "         0.02432526, -0.05583445,  0.02513418,  0.00014207, -0.04309915,\n",
    "        -2.4768659 , -4.35102398, -4.16130475,  2.19790587, -1.1172696 ,\n",
    "         3.9791731 ,  1.70436593,  0.35539521, -0.02116464,  0.05241302,\n",
    "         0.30750227,  0.06811254, -0.09728995, -0.01103003, -0.00094051,\n",
    "         0.15610358,  0.44431064]),\n",
    " array([-0.0011266 , -0.01269505, -0.19509014, -0.0023208 , -0.0009578 ,\n",
    "        -0.02191087,  0.00001501, -0.00007286,  0.00003592, -0.00016001,\n",
    "        -0.00528111, -0.00001171,  0.00215546,  0.00709156,  0.0011908 ,\n",
    "        -0.00887827, -0.0010646 ,  0.00169996, -0.00275828, -0.00284061,\n",
    "         0.00321437, -1.68143589, -4.77532927,  4.18733112,  0.01509522,\n",
    "        -0.00000035, -0.00153091, -0.02761329,  0.04569809,  0.05019076,\n",
    "         0.02432509, -0.0558119 ,  0.02512177,  0.000133  , -0.0430889 ,\n",
    "        -2.47518703, -4.35097191, -4.16132834,  2.19695704, -1.11674252,\n",
    "         3.97828727,  1.70389535,  0.35539867, -0.02115713,  0.05240532,\n",
    "         0.3074982 ,  0.06810271, -0.09729722, -0.01103018, -0.00094037,\n",
    "         0.15609993,  0.44429135]),\n",
    " array([-0.00112431, -0.01269562, -0.19508366, -0.00232049, -0.00095774,\n",
    "        -0.02191223,  0.000015  , -0.00007259,  0.00003592, -0.00016   ,\n",
    "        -0.00528072, -0.00001171,  0.00215538,  0.00709122,  0.00119076,\n",
    "        -0.00887831, -0.00106461,  0.00169994, -0.00275477, -0.002841  ,\n",
    "         0.00321118, -1.67352376, -4.77541754,  4.1795049 ,  0.01508919,\n",
    "        -0.00000035, -0.00152989, -0.02761383,  0.04568724,  0.05018819,\n",
    "         0.0243247 , -0.0557877 ,  0.02510868,  0.00012325, -0.0430779 ,\n",
    "        -2.47338733, -4.35092047, -4.16135292,  2.19593874, -1.11617788,\n",
    "         3.97733886,  1.70339062,  0.35540238, -0.02114927,  0.05239726,\n",
    "         0.30749402,  0.06809238, -0.0973048 , -0.01103035, -0.00094021,\n",
    "         0.156096  ,  0.44427068]),\n",
    " array([-0.00112187, -0.01269622, -0.19507671, -0.00232015, -0.00095767,\n",
    "        -0.02191368,  0.00001498, -0.0000723 ,  0.00003591, -0.00015999,\n",
    "        -0.0052803 , -0.00001172,  0.00215528,  0.00709085,  0.00119072,\n",
    "        -0.00887835, -0.00106461,  0.00169992, -0.002751  , -0.00284143,\n",
    "         0.00320777, -1.66504394, -4.77551255,  4.17111812,  0.01508272,\n",
    "        -0.00000035, -0.0015288 , -0.02761441,  0.04567565,  0.05018552,\n",
    "         0.02432446, -0.05576187,  0.02509447,  0.00011289, -0.04306614,\n",
    "        -2.4714607 , -4.35086049, -4.1613793 ,  2.19485215, -1.11557372,\n",
    "         3.97631963,  1.70285071,  0.35540636, -0.02114085,  0.05238864,\n",
    "         0.30748954,  0.06808131, -0.09731294, -0.01103053, -0.00094004,\n",
    "         0.15609179,  0.44424855]),\n",
    " array([-0.00111925, -0.01269686, -0.19506926, -0.00231978, -0.0009576 ,\n",
    "        -0.02191524,  0.00001497, -0.00007199,  0.00003591, -0.00015998,\n",
    "        -0.00527986, -0.00001173,  0.00215519,  0.00709046,  0.00119067,\n",
    "        -0.0088784 , -0.00106461,  0.00169989, -0.00274697, -0.00284188,\n",
    "         0.00320411, -1.6559564 , -4.77561428,  4.16213025,  0.0150758 ,\n",
    "        -0.00000035, -0.00152763, -0.02761504,  0.04566322,  0.05018266,\n",
    "         0.02432421, -0.0557342 ,  0.02507925,  0.0001018 , -0.04305353,\n",
    "        -2.46939614, -4.35079649, -4.16140777,  2.19368773, -1.11492609,\n",
    "         3.9752275 ,  1.70227211,  0.35541063, -0.02113183,  0.05237939,\n",
    "         0.30748474,  0.06806944, -0.09732165, -0.01103072, -0.00093986,\n",
    "         0.15608729,  0.44422482]),\n",
    " array([-0.00111644, -0.01269756, -0.19506127, -0.00231939, -0.00095752,\n",
    "        -0.02191691,  0.00001495, -0.00007166,  0.00003591, -0.00015997,\n",
    "        -0.00527938, -0.00001174,  0.00215508,  0.00709004,  0.00119062,\n",
    "        -0.00887845, -0.00106462,  0.00169986, -0.00274265, -0.00284236,\n",
    "         0.00320018, -1.64621756, -4.77572338,  4.15249827,  0.01506837,\n",
    "        -0.00000035, -0.00152638, -0.02761571,  0.0456499 ,  0.0501796 ,\n",
    "         0.02432394, -0.05570454,  0.02506294,  0.0000899 , -0.04304001,\n",
    "        -2.46718349, -4.35072767, -4.16143811,  2.19243982, -1.1142322 ,\n",
    "         3.97405698,  1.70165204,  0.3554152 , -0.02112216,  0.05236949,\n",
    "         0.3074796 ,  0.06805672, -0.09733099, -0.01103092, -0.00093966,\n",
    "         0.15608246,  0.44419941]),\n",
    " array([-0.00111343, -0.0126983 , -0.19505271, -0.00231897, -0.00095743,\n",
    "        -0.0219187 ,  0.00001494, -0.00007131,  0.0000359 , -0.00015996,\n",
    "        -0.00527887, -0.00001174,  0.00215497,  0.00708959,  0.00119057,\n",
    "        -0.00887851, -0.00106462,  0.00169983, -0.00273801, -0.00284289,\n",
    "         0.00319598, -1.63578128, -4.77584016,  4.14217621,  0.01506041,\n",
    "        -0.00000035, -0.00152503, -0.02761642,  0.04563563,  0.05017631,\n",
    "         0.02432364, -0.05567275,  0.02504546,  0.00007716, -0.04302553,\n",
    "        -2.46481182, -4.35065376, -4.1614704 ,  2.19110233, -1.11348855,\n",
    "         3.97280223,  1.70098734,  0.35542009, -0.0211118 ,  0.05235887,\n",
    "         0.30747409,  0.06804309, -0.097341  , -0.01103114, -0.00093945,\n",
    "         0.15607728,  0.44417214]),\n",
    " array([-0.0011102 , -0.01269909, -0.19504354, -0.00231853, -0.00095734,\n",
    "        -0.02192061,  0.00001492, -0.00007093,  0.0000359 , -0.00015995,\n",
    "        -0.00527832, -0.00001175,  0.00215484,  0.0070891 ,  0.00119052,\n",
    "        -0.00887856, -0.00106463,  0.0016998 , -0.00273305, -0.00284344,\n",
    "         0.00319147, -1.62459655, -4.77596546,  4.13111417,  0.01505189,\n",
    "        -0.00000035, -0.00152359, -0.02761719,  0.04562033,  0.05017279,\n",
    "         0.02432333, -0.05563869,  0.02502673,  0.0000635 , -0.04301001,\n",
    "        -2.4622706 , -4.35057466, -4.16150521,  2.18966912, -1.11269165,\n",
    "         3.97145787,  1.70027519,  0.35542534, -0.0211007 ,  0.05234749,\n",
    "         0.30746818,  0.06802848, -0.09735172, -0.01103137, -0.00093923,\n",
    "         0.15607174,  0.44414295]),\n",
    " array([-0.00110674, -0.01269995, -0.19503371, -0.00231804, -0.00095725,\n",
    "        -0.02192267,  0.0000149 , -0.00007052,  0.00003589, -0.00015993,\n",
    "        -0.00527773, -0.00001176,  0.00215471,  0.00708858,  0.00119046,\n",
    "        -0.00887863, -0.00106463,  0.00169977, -0.00272772, -0.00284404,\n",
    "         0.00318664, -1.61261023, -4.77609974,  4.11925934,  0.01504275,\n",
    "        -0.00000035, -0.00152205, -0.02761801,  0.04560394,  0.05016902,\n",
    "         0.024323  , -0.05560219,  0.02500665,  0.00004886, -0.04299338,\n",
    "        -2.45954727, -4.35048989, -4.16154251,  2.1881332 , -1.11183765,\n",
    "         3.97001717,  1.69951201,  0.35543096, -0.0210888 ,  0.0523353 ,\n",
    "         0.30746185,  0.06801283, -0.09736322, -0.01103162, -0.00093899,\n",
    "         0.15606579,  0.44411167]),\n",
    " array([-0.00110304, -0.01270086, -0.19502317, -0.00231753, -0.00095714,\n",
    "        -0.02192487,  0.00001488, -0.00007009,  0.00003588, -0.00015992,\n",
    "        -0.0052771 , -0.00001177,  0.00215457,  0.00708803,  0.0011904 ,\n",
    "        -0.00887869, -0.00106464,  0.00169973, -0.00272202, -0.00284468,\n",
    "         0.00318147, -1.59976487, -4.77624365,  4.1065549 ,  0.01503296,\n",
    "        -0.00000035, -0.00152039, -0.02761889,  0.04558637,  0.05016497,\n",
    "         0.02432264, -0.05556307,  0.02498513,  0.00003317, -0.04297555,\n",
    "        -2.45662876, -4.35039905, -4.16158248,  2.18648721, -1.11092244,\n",
    "         3.96847322,  1.69869413,  0.35543699, -0.02107604,  0.05232223,\n",
    "         0.30745506,  0.06799605, -0.09737554, -0.01103189, -0.00093873,\n",
    "         0.15605942,  0.44407814]),\n",
    " array([-0.00109907, -0.01270184, -0.19501188, -0.00231698, -0.00095703,\n",
    "        -0.02192723,  0.00001486, -0.00006962,  0.00003588, -0.0001599 ,\n",
    "        -0.00527643, -0.00001178,  0.00215442,  0.00708743,  0.00119033,\n",
    "        -0.00887877, -0.00106464,  0.00169969, -0.00271591, -0.00284537,\n",
    "         0.00317592, -1.58599893, -4.77639787,  4.09293996,  0.01502246,\n",
    "        -0.00000035, -0.00151862, -0.02761984,  0.04556755,  0.05016064,\n",
    "         0.02432225, -0.05552114,  0.02496208,  0.00001636, -0.04295645,\n",
    "        -2.45350108, -4.3503017 , -4.16162531,  2.18472325, -1.10994163,\n",
    "         3.96681862,  1.69781763,  0.35544345, -0.02106238,  0.05230822,\n",
    "         0.30744779,  0.06797807, -0.09738874, -0.01103218, -0.00093846,\n",
    "         0.1560526 ,  0.44404221]),\n",
    " array([-0.00109482, -0.01270289, -0.19499978, -0.00231639, -0.00095692,\n",
    "        -0.02192976,  0.00001483, -0.00006912,  0.00003587, -0.00015989,\n",
    "        -0.0052757 , -0.00001179,  0.00215426,  0.0070868 ,  0.00119026,\n",
    "        -0.00887884, -0.00106465,  0.00169965, -0.00270936, -0.00284611,\n",
    "         0.00316998, -1.57124641, -4.77656315,  4.07834928,  0.01501122,\n",
    "        -0.00000035, -0.00151672, -0.02762085,  0.04554738,  0.050156  ,\n",
    "         0.02432184, -0.05547621,  0.02493737, -0.00000098, -0.04293629,\n",
    "        -2.45016653, -4.35017776, -4.16166605,  2.18282748, -1.10888615,\n",
    "         3.96504035,  1.69688329,  0.35545071, -0.02104774,  0.0522932 ,\n",
    "         0.30744003,  0.06795887, -0.09740292, -0.0110325 , -0.00093818,\n",
    "         0.15604506,  0.44400101]),\n",
    " array([-0.00109017, -0.0127041 , -0.1949869 , -0.00231575, -0.00095679,\n",
    "        -0.02193247,  0.00001481, -0.00006858,  0.00003586, -0.00015987,\n",
    "        -0.00527495, -0.00001181,  0.00215409,  0.00708611,  0.00119018,\n",
    "        -0.00887893, -0.00106465,  0.0016996 , -0.00270234, -0.00284689,\n",
    "         0.0031636 , -1.55543948, -4.7767428 ,  4.06271562,  0.01499916,\n",
    "        -0.00000035, -0.00151468, -0.02762194,  0.04552569,  0.05015075,\n",
    "         0.02432146, -0.05542813,  0.02491092, -0.00001955, -0.04291469,\n",
    "        -2.44659502, -4.35003763, -4.16170973,  2.1807958 , -1.10775453,\n",
    "         3.9631345 ,  1.69588244,  0.35545837, -0.02103206,  0.0522771 ,\n",
    "         0.30743179,  0.06793826, -0.09741811, -0.01103284, -0.00093789,\n",
    "         0.156037  ,  0.44395753]),\n",
    " array([-0.00108517, -0.01270541, -0.19497308, -0.00231508, -0.00095665,\n",
    "        -0.02193536,  0.00001478, -0.000068  ,  0.00003586, -0.00015985,\n",
    "        -0.00527414, -0.00001182,  0.00215391,  0.00708537,  0.0011901 ,\n",
    "        -0.00887901, -0.00106466,  0.00169955, -0.00269481, -0.00284774,\n",
    "         0.00315678, -1.53849731, -4.77693672,  4.0459628 ,  0.01498624,\n",
    "        -0.00000035, -0.0015125 , -0.02762311,  0.04550242,  0.050145  ,\n",
    "         0.02432106, -0.0553766 ,  0.02488258, -0.00003941, -0.04289156,\n",
    "        -2.44276844, -4.34988652, -4.16175605,  2.17861859, -1.10654226,\n",
    "         3.96109161,  1.69481024,  0.35546654, -0.02101525,  0.05225986,\n",
    "         0.30742299,  0.06791615, -0.09743438, -0.01103321, -0.00093758,\n",
    "         0.15602835,  0.44391091]),\n",
    " array([-0.00107981, -0.01270682, -0.19495828, -0.00231435, -0.00095651,\n",
    "        -0.02193846,  0.00001475, -0.00006739,  0.00003585, -0.00015983,\n",
    "        -0.00527327, -0.00001183,  0.00215371,  0.00708457,  0.00119001,\n",
    "        -0.00887911, -0.00106467,  0.0016995 , -0.00268675, -0.00284865,\n",
    "         0.00314946, -1.5203403 , -4.77714524,  4.02801035,  0.01497239,\n",
    "        -0.00000035, -0.00151016, -0.02762436,  0.04547746,  0.05013883,\n",
    "         0.02432064, -0.05532139,  0.02485221, -0.00006064, -0.04286679,\n",
    "        -2.43866811, -4.34972474, -4.16180546,  2.17628534, -1.10524316,\n",
    "         3.95890174,  1.69366142,  0.35547535, -0.02099675,  0.05224087,\n",
    "         0.30741314,  0.06789198, -0.0974523 , -0.01103361, -0.00093725,\n",
    "         0.1560191 ,  0.44386081]),\n",
    " array([-0.00107395, -0.01270843, -0.19494245, -0.00231357, -0.00095635,\n",
    "        -0.02194176,  0.00001472, -0.00006672,  0.00003584, -0.0001598 ,\n",
    "        -0.00527236, -0.00001185,  0.0021535 ,  0.00708371,  0.00118991,\n",
    "        -0.00887921, -0.00106468,  0.00169945, -0.00267811, -0.00284962,\n",
    "         0.00314162, -1.50087928, -4.77737712,  4.00877951,  0.01495754,\n",
    "        -0.00000035, -0.00150765, -0.02762571,  0.04545054,  0.05013184,\n",
    "         0.02432031, -0.05526231,  0.0248197 , -0.00008252, -0.04284064,\n",
    "        -2.43429827, -4.34952259, -4.16185285,  2.17378089, -1.1038477 ,\n",
    "         3.95654942,  1.6924375 ,  0.35548512, -0.02097743,  0.05222103,\n",
    "         0.30740321,  0.06786656, -0.09747101, -0.01103404, -0.00093693,\n",
    "         0.15600891,  0.44380456]),\n",
    " array([-0.00106778, -0.01271006, -0.19492542, -0.00231273, -0.00095619,\n",
    "        -0.02194532,  0.00001469, -0.00006602,  0.00003583, -0.00015978,\n",
    "        -0.00527137, -0.00001186,  0.00215327,  0.0070828 ,  0.00118981,\n",
    "        -0.00887932, -0.00106468,  0.00169939, -0.00266885, -0.00285066,\n",
    "         0.00313322, -1.48002434, -4.77762031,  3.98816507,  0.01494164,\n",
    "        -0.00000035, -0.00150496, -0.02762715,  0.04542181,  0.05012475,\n",
    "         0.02431987, -0.05519893,  0.02478483, -0.00010675, -0.04281226,\n",
    "        -2.42959391, -4.34933314, -4.16190939,  2.17110214, -1.10235652,\n",
    "         3.95403436,  1.69111971,  0.35549532, -0.02095672,  0.05219976,\n",
    "         0.30739239,  0.0678393 , -0.09749106, -0.0110345 , -0.00093655,\n",
    "         0.15599823,  0.44374648]),\n",
    " array([-0.00106102, -0.0127119 , -0.19490718, -0.00231184, -0.00095601,\n",
    "        -0.0219491 ,  0.00001465, -0.00006526,  0.00003582, -0.00015976,\n",
    "        -0.00527033, -0.00001188,  0.00215303,  0.00708181,  0.0011897 ,\n",
    "        -0.00887944, -0.00106469,  0.00169932, -0.00265893, -0.00285177,\n",
    "         0.00312421, -1.45767182, -4.77789054,  3.96608315,  0.01492459,\n",
    "        -0.00000035, -0.00150209, -0.02762869,  0.04539079,  0.05011675,\n",
    "         0.02431952, -0.0551311 ,  0.02474751, -0.00013171, -0.04278228,\n",
    "        -2.42457894, -4.34909843, -4.16196487,  2.16822841, -1.10075564,\n",
    "         3.95133341,  1.68971568,  0.35550656, -0.02093454,  0.05217696,\n",
    "         0.30738096,  0.06781011, -0.0975125 , -0.01103498, -0.00093618,\n",
    "         0.15598653,  0.44368138]),\n",
    " array([-0.00105375, -0.01271388, -0.1948876 , -0.00231088, -0.00095582,\n",
    "        -0.02195315,  0.00001461, -0.00006444,  0.00003581, -0.00015973,\n",
    "        -0.00526921, -0.00001189,  0.00215277,  0.00708075,  0.00118959,\n",
    "        -0.00887957, -0.0010647 ,  0.00169926, -0.00264829, -0.00285296,\n",
    "         0.00311456, -1.4337166 , -4.77818255,  3.94242094,  0.01490631,\n",
    "        -0.00000035, -0.001499  , -0.02763035,  0.04535746,  0.05010815,\n",
    "         0.02431918, -0.05505841,  0.02470752, -0.00015834, -0.0427502 ,\n",
    "        -2.41920647, -4.34884315, -4.16202418,  2.16515024, -1.09904192,\n",
    "         3.9484387 ,  1.68821223,  0.3555186 , -0.02091078,  0.05215252,\n",
    "         0.30736867,  0.06777882, -0.09753547, -0.01103549, -0.00093578,\n",
    "         0.15597399,  0.44361134]),\n",
    " array([-0.00104599, -0.01271597, -0.1948666 , -0.00230985, -0.00095562,\n",
    "        -0.02195748,  0.00001457, -0.00006357,  0.0000358 , -0.0001597 ,\n",
    "        -0.00526801, -0.00001191,  0.00215249,  0.00707962,  0.00118946,\n",
    "        -0.00887971, -0.00106472,  0.00169918, -0.0026369 , -0.00285424,\n",
    "         0.00310421, -1.40804651, -4.77849282,  3.91706114,  0.01488673,\n",
    "        -0.00000035, -0.00149569, -0.02763213,  0.04532177,  0.05009905,\n",
    "         0.02431878, -0.05498048,  0.02466465, -0.00018716, -0.04271569,\n",
    "        -2.41343982, -4.34857932, -4.16208956,  2.16185338, -1.09720719,\n",
    "         3.94533804,  1.68659849,  0.35553137, -0.02088532,  0.05212632,\n",
    "         0.30735544,  0.06774528, -0.09756007, -0.01103604, -0.00093534,\n",
    "         0.15596065,  0.44353711]),\n",
    " array([-0.00103764, -0.01271822, -0.19484406, -0.00230875, -0.0009554 ,\n",
    "        -0.02196212,  0.00001453, -0.00006263,  0.00003579, -0.00015967,\n",
    "        -0.00526673, -0.00001193,  0.00215219,  0.0070784 ,  0.00118933,\n",
    "        -0.00887986, -0.00106473,  0.00169911, -0.00262469, -0.00285561,\n",
    "         0.00309313, -1.3805346 , -4.77882879,  3.88988642,  0.01486575,\n",
    "        -0.00000035, -0.00149215, -0.02763403,  0.04528341,  0.05008922,\n",
    "         0.0243184 , -0.054897  ,  0.02461872, -0.00021776, -0.04267882,\n",
    "        -2.40726734, -4.34828915, -4.16215939,  2.15832139, -1.09524189,\n",
    "         3.94201503,  1.6848722 ,  0.35554513, -0.02085803,  0.05209825,\n",
    "         0.30734127,  0.06770934, -0.09758641, -0.01103662, -0.00093487,\n",
    "         0.15594629,  0.44345695]),\n",
    " array([-0.00102851, -0.01272073, -0.19481988, -0.00230757, -0.00095517,\n",
    "        -0.02196705,  0.00001448, -0.00006163,  0.00003577, -0.00015963,\n",
    "        -0.00526538, -0.00001195,  0.00215187,  0.00707709,  0.00118919,\n",
    "        -0.00888002, -0.00106474,  0.00169902, -0.0026116 , -0.00285707,\n",
    "         0.00308124, -1.35104676, -4.77920021,  3.86077533,  0.01484325,\n",
    "        -0.00000035, -0.00148835, -0.02763608,  0.04524188,  0.05007828,\n",
    "         0.02431817, -0.05480765,  0.02456956, -0.00024937, -0.0426398 ,\n",
    "        -2.40068109, -4.34794634, -4.16223119,  2.15453753, -1.09313641,\n",
    "         3.93844974,  1.68303311,  0.35556013, -0.02082804,  0.05206733,\n",
    "         0.30732547,  0.06767004, -0.09761535, -0.01103724, -0.0009344 ,\n",
    "         0.1559307 ,  0.4433682 ]),\n",
    " array([-0.00101881, -0.01272335, -0.19479392, -0.00230631, -0.00095492,\n",
    "        -0.02197235,  0.00001443, -0.00006056,  0.00003576, -0.0001596 ,\n",
    "        -0.00526393, -0.00001198,  0.00215153,  0.00707569,  0.00118904,\n",
    "        -0.0088802 , -0.00106476,  0.00169893, -0.00259758, -0.00285864,\n",
    "         0.00306851, -1.31944832, -4.77959248,  3.82957241,  0.01481915,\n",
    "        -0.00000035, -0.00148428, -0.02763827,  0.04519749,  0.05006682,\n",
    "         0.02431786, -0.05471184,  0.02451683, -0.00028384, -0.04259772,\n",
    "        -2.39360597, -4.34759811, -4.16231072,  2.15048454, -1.09088251,\n",
    "         3.93463203,  1.68105729,  0.35557601, -0.02079667,  0.05203502,\n",
    "         0.30730916,  0.0676287 , -0.09764559, -0.0110379 , -0.00093387,\n",
    "         0.15591411,  0.4432749 ]),\n",
    " array([-0.00100838, -0.01272617, -0.19476608, -0.00230495, -0.00095466,\n",
    "        -0.02197801,  0.00001437, -0.00005941,  0.00003574, -0.00015956,\n",
    "        -0.00526237, -0.000012  ,  0.00215116,  0.00707419,  0.00118888,\n",
    "        -0.00888039, -0.00106477,  0.00169884, -0.00258255, -0.00286032,\n",
    "         0.00305486, -1.28558662, -4.78001352,  3.79613515,  0.01479332,\n",
    "        -0.00000035, -0.00147992, -0.02764061,  0.04514979,  0.0500545 ,\n",
    "         0.02431753, -0.05460915,  0.02446034, -0.0003206 , -0.04255267,\n",
    "        -2.38602521, -4.3472223 , -4.16239559,  2.14614225, -1.08846841,\n",
    "         3.93053974,  1.67894131,  0.35559301, -0.02076306,  0.05200039,\n",
    "         0.3072917 ,  0.06758439, -0.09767798, -0.0110386 , -0.0009333 ,\n",
    "         0.15589632,  0.44317471]),\n",
    " array([-0.00099695, -0.01272931, -0.19473614, -0.0023035 , -0.00095438,\n",
    "        -0.02198401,  0.00001431, -0.00005817,  0.00003573, -0.00015952,\n",
    "        -0.00526074, -0.00001203,  0.00215076,  0.00707257,  0.0011887 ,\n",
    "        -0.00888059, -0.00106479,  0.00169873, -0.00256645, -0.00286212,\n",
    "         0.00304023, -1.24929228, -4.78047969,  3.76031571,  0.01476563,\n",
    "        -0.00000035, -0.00147525, -0.02764313,  0.04509797,  0.05004084,\n",
    "         0.02431746, -0.05449928,  0.02439987, -0.00035832, -0.04250508,\n",
    "        -2.37793954, -4.34678031, -4.16248399,  2.14149358, -1.08588497,\n",
    "         3.92615055,  1.67669047,  0.35561152, -0.02072709,  0.05196326,\n",
    "         0.3072731 ,  0.06753689, -0.09771258, -0.01103935, -0.00093273,\n",
    "         0.15587694,  0.44306401]),\n",
    " array([-0.00098466, -0.01273267, -0.194704  , -0.00230194, -0.00095407,\n",
    "        -0.02199043,  0.00001425, -0.00005685,  0.00003571, -0.00015947,\n",
    "        -0.00525901, -0.00001205,  0.00215034,  0.00707083,  0.00118852,\n",
    "        -0.00888081, -0.00106481,  0.00169862, -0.00254919, -0.00286405,\n",
    "         0.00302455, -1.21039658, -4.78098017,  3.72192975,  0.01473595,\n",
    "        -0.00000035, -0.00147024, -0.02764583,  0.04504217,  0.05002622,\n",
    "         0.02431744, -0.05438154,  0.02433508, -0.00039845, -0.04245417,\n",
    "        -2.36927716, -4.34630445, -4.16257892,  2.13651502, -1.08312033,\n",
    "         3.92144688,  1.67428237,  0.35563134, -0.02068855,  0.05192346,\n",
    "         0.30725316,  0.06748596, -0.09774963, -0.01104015, -0.00093211,\n",
    "         0.15585615,  0.44294556]),\n",
    " array([-0.00097157, -0.01273622, -0.19466961, -0.00230027, -0.00095375,\n",
    "        -0.02199731,  0.00001419, -0.00005543,  0.00003569, -0.00015942,\n",
    "        -0.00525713, -0.00001208,  0.00214989,  0.00706898,  0.00118832,\n",
    "        -0.00888104, -0.00106483,  0.0016985 , -0.0025307 , -0.00286612,\n",
    "         0.00300775, -1.16871627, -4.78150763,  3.68078444,  0.01470416,\n",
    "        -0.00000035, -0.00146487, -0.02764872,  0.04498251,  0.05001081,\n",
    "         0.0243173 , -0.0542553 ,  0.02426561, -0.00044206, -0.04239933,\n",
    "        -2.35997595, -4.34581632, -4.16268349,  2.13117993, -1.08015773,\n",
    "         3.91640862,  1.67169637,  0.35565239, -0.0206461 ,  0.05187968,\n",
    "         0.3072307 ,  0.06743027, -0.09779047, -0.01104101, -0.00093143,\n",
    "         0.15583409,  0.4428206 ]),\n",
    " array([-0.00095721, -0.01274016, -0.19463257, -0.00229849, -0.00095341,\n",
    "        -0.02200459,  0.00001411, -0.00005391,  0.00003567, -0.00015937,\n",
    "        -0.00525518, -0.00001211,  0.0021494 ,  0.00706699,  0.00118812,\n",
    "        -0.00888129, -0.00106485,  0.00169837, -0.00251088, -0.00286832,\n",
    "         0.00298973, -1.12404316, -4.78208949,  3.6367068 ,  0.01467008,\n",
    "        -0.00000035, -0.00145912, -0.02765183,  0.04491741,  0.04999381,\n",
    "         0.02431754, -0.05412023,  0.02419128, -0.00048647, -0.04234146,\n",
    "        -2.35005591, -4.34524764, -4.16279367,  2.12547199, -1.07699075,\n",
    "         3.911006  ,  1.6689497 ,  0.35567523, -0.02060181,  0.05183389,\n",
    "         0.30720778,  0.06737165, -0.09783297, -0.01104192, -0.00093074,\n",
    "         0.15580999,  0.44268326]),\n",
    " array([-0.00094184, -0.01274432, -0.19459303, -0.00229657, -0.00095304,\n",
    "        -0.02201236,  0.00001404, -0.00005228,  0.00003565, -0.00015932,\n",
    "        -0.00525309, -0.00001215,  0.00214888,  0.00706485,  0.00118789,\n",
    "        -0.00888156, -0.00106487,  0.00169823, -0.00248964, -0.0028707 ,\n",
    "         0.00297044, -1.0761754 , -4.78270221,  3.58945813,  0.01463354,\n",
    "        -0.00000035, -0.00145295, -0.02765514,  0.04484766,  0.04997535,\n",
    "         0.02431655, -0.05397474,  0.02411257, -0.00053521, -0.04227908,\n",
    "        -2.33939442, -4.34468834, -4.16291351,  2.11932656, -1.07359157,\n",
    "         3.9052359 ,  1.66599546,  0.35569944, -0.02055431,  0.05178482,\n",
    "         0.30718317,  0.06730882, -0.09787855, -0.0110429 , -0.00092998,\n",
    "         0.15578428,  0.4425383 ]),\n",
    " array([-0.00092528, -0.0127488 , -0.19455051, -0.00229452, -0.00095265,\n",
    "        -0.02202065,  0.00001395, -0.00005054,  0.00003562, -0.00015926,\n",
    "        -0.00525086, -0.00001219,  0.00214833,  0.00706256,  0.00118765,\n",
    "        -0.00888185, -0.00106489,  0.00169808, -0.00246688, -0.00287324,\n",
    "         0.00294976, -1.02487039, -4.78336255,  3.53882618,  0.0145944 ,\n",
    "        -0.00000034, -0.00144635, -0.0276587 ,  0.04477246,  0.04995602,\n",
    "         0.02431675, -0.05381958,  0.02402728, -0.00058601, -0.04221264,\n",
    "        -2.32799603, -4.34405248, -4.1630447 ,  2.11277484, -1.06995856,\n",
    "         3.89903648,  1.6628467 ,  0.35572548, -0.02050215,  0.05173094,\n",
    "         0.30715562,  0.06724018, -0.0979286 , -0.01104396, -0.00092917,\n",
    "         0.1557567 ,  0.44238283]),\n",
    " array([-0.00090734, -0.01275367, -0.19450477, -0.00229232, -0.00095223,\n",
    "        -0.02202949,  0.00001387, -0.00004867,  0.0000356 , -0.0001592 ,\n",
    "        -0.00524852, -0.00001222,  0.00214773,  0.00706011,  0.0011874 ,\n",
    "        -0.00888215, -0.00106492,  0.00169792, -0.0024425 , -0.00287596,\n",
    "         0.00292759, -0.96988711, -4.78407696,  3.4845737 ,  0.01455244,\n",
    "        -0.00000034, -0.00143927, -0.02766254,  0.04469079,  0.04993512,\n",
    "         0.02431726, -0.05365345,  0.02393594, -0.00063879, -0.04214202,\n",
    "        -2.31580651, -4.34334575, -4.1631839 ,  2.10576208, -1.06607519,\n",
    "         3.89239075,  1.65949107,  0.35575349, -0.02044746,  0.0516744 ,\n",
    "         0.30712722,  0.06716771, -0.097981  , -0.01104509, -0.00092832,\n",
    "         0.15572685,  0.44221514]),\n",
    " array([-0.00088796, -0.01275892, -0.19445571, -0.00228996, -0.00095178,\n",
    "        -0.02203888,  0.00001377, -0.00004666,  0.00003557, -0.00015913,\n",
    "        -0.00524606, -0.00001227,  0.00214709,  0.00705748,  0.00118712,\n",
    "        -0.00888248, -0.00106495,  0.00169775, -0.00241636, -0.00287886,\n",
    "         0.00290382, -0.91096555, -4.78484123,  3.42643439,  0.01450748,\n",
    "        -0.00000034, -0.00143168, -0.02766665,  0.04460241,  0.04991268,\n",
    "         0.02431795, -0.0534755 ,  0.0238381 , -0.00069419, -0.04206671,\n",
    "        -2.30275706, -4.34258096, -4.16333517,  2.09825341, -1.06191883,\n",
    "         3.88526895,  1.65590844,  0.35578348, -0.02038899,  0.05161393,\n",
    "         0.30709692,  0.06709011, -0.09803696, -0.0110463 , -0.00092741,\n",
    "         0.15569472,  0.44203556]),\n",
    " array([-0.00086704, -0.01276455, -0.1944031 , -0.00228744, -0.0009513 ,\n",
    "        -0.02204887,  0.00001367, -0.00004452,  0.00003554, -0.00015906,\n",
    "        -0.00524344, -0.00001231,  0.0021464 ,  0.00705466,  0.00118683,\n",
    "        -0.00888283, -0.00106498,  0.00169756, -0.00238835, -0.00288197,\n",
    "         0.00287835, -0.84781855, -4.78565861,  3.36412464,  0.0144593 ,\n",
    "        -0.00000034, -0.00142354, -0.02767106,  0.04450683,  0.04988866,\n",
    "         0.02431885, -0.05328491,  0.02373331, -0.00075243, -0.04198636,\n",
    "        -2.28878558, -4.34175603, -4.16349995,  2.09021394, -1.05747212,\n",
    "         3.87763887,  1.65208428,  0.35581562, -0.02032464,  0.0515474 ,\n",
    "         0.30706287,  0.0670052 , -0.09809856, -0.01104761, -0.00092644,\n",
    "         0.15566026,  0.44184401]),\n",
    " array([-0.00084435, -0.01277067, -0.1943467 , -0.00228473, -0.00095079,\n",
    "        -0.02205933,  0.00001356, -0.00004221,  0.00003551, -0.00015898,\n",
    "        -0.00524073, -0.00001236,  0.00214566,  0.00705164,  0.00118652,\n",
    "        -0.00888319, -0.00106501,  0.00169736, -0.00235782, -0.00288515,\n",
    "         0.00285036, -0.7801468 , -4.78652253,  3.29735105,  0.01440766,\n",
    "        -0.00000034, -0.00141483, -0.02767577,  0.04440263,  0.04986288,\n",
    "         0.02432029, -0.05308098,  0.02362114, -0.00081225, -0.04190115,\n",
    "        -2.27386058, -4.34085588, -4.16368327,  2.08161587, -1.05271307,\n",
    "         3.86946577,  1.64802016,  0.35585009, -0.02025742,  0.05147784,\n",
    "         0.30702822,  0.06691579, -0.09816278, -0.01104901, -0.00092544,\n",
    "         0.15562285,  0.4416377 ]),\n",
    " array([-0.0008198 , -0.0127772 , -0.19428607, -0.00228183, -0.00095024,\n",
    "        -0.02207055,  0.00001344, -0.00003974,  0.00003548, -0.0001589 ,\n",
    "        -0.00523786, -0.00001242,  0.00214487,  0.00704841,  0.00118619,\n",
    "        -0.00888358, -0.00106504,  0.00169715, -0.00232562, -0.00288869,\n",
    "         0.00282104, -0.7076252 , -4.78745901,  3.22579098,  0.01435232,\n",
    "        -0.00000034, -0.00140549, -0.02768087,  0.04428974,  0.04983516,\n",
    "         0.02432195, -0.0528625 ,  0.023501  , -0.00087487, -0.04181026,\n",
    "        -2.25786971, -4.33987649, -4.16388005,  2.07240723, -1.04762845,\n",
    "         3.86070758,  1.64368094,  0.35588698, -0.02018541,  0.05140335,\n",
    "         0.3069908 ,  0.06681987, -0.09823153, -0.01105052, -0.00092435,\n",
    "         0.15558264,  0.44141885]),\n",
    " array([-0.00079341, -0.01278416, -0.19422114, -0.00227871, -0.00094966,\n",
    "        -0.02208254,  0.00001332, -0.0000371 ,  0.00003545, -0.00015881,\n",
    "        -0.00523482, -0.00001247,  0.00214402,  0.00704495,  0.00118583,\n",
    "        -0.00888399, -0.00106507,  0.00169692, -0.00229111, -0.00289251,\n",
    "         0.00278963, -0.62989868, -4.78845773,  3.14908973,  0.01429301,\n",
    "        -0.00000034, -0.00139548, -0.02768635,  0.04416784,  0.04980559,\n",
    "         0.02432395, -0.05262854,  0.0233723 , -0.00094091, -0.04171318,\n",
    "        -2.24074526, -4.33882804, -4.16409685,  2.06254754, -1.04218898,\n",
    "         3.85132947,  1.6390486 ,  0.35592649, -0.02010614,  0.0513214 ,\n",
    "         0.3069487 ,  0.06671492, -0.09830728, -0.01105214, -0.00092317,\n",
    "         0.15553955,  0.44118656]),\n",
    " array([-0.000765  , -0.0127916 , -0.19415151, -0.00227538, -0.00094903,\n",
    "        -0.02209528,  0.00001318, -0.00003427,  0.00003541, -0.00015872,\n",
    "        -0.00523161, -0.00001254,  0.00214312,  0.00704125,  0.00118545,\n",
    "        -0.00888442, -0.00106511,  0.00169667, -0.00225413, -0.00289658,\n",
    "         0.00275595, -0.5466112 , -4.78952066,  3.06689181,  0.01422945,\n",
    "        -0.00000034, -0.00138475, -0.02769224,  0.04403614,  0.04977395,\n",
    "         0.02432618, -0.05237789,  0.02323446, -0.00101051, -0.04160943,\n",
    "        -2.22239892, -4.33770499, -4.16433277,  2.05198665, -1.0363644 ,\n",
    "         3.84128038,  1.63409731,  0.35596868, -0.0200233 ,  0.05123575,\n",
    "         0.30690546,  0.06660448, -0.09838635, -0.01105389, -0.0009219 ,\n",
    "         0.15549316,  0.44093901]),\n",
    " array([-0.00073391, -0.01279963, -0.19407699, -0.00227181, -0.00094837,\n",
    "        -0.0221086 ,  0.00001304, -0.00003123,  0.00003537, -0.00015862,\n",
    "        -0.00522832, -0.00001261,  0.00214214,  0.00703728,  0.00118505,\n",
    "        -0.00888488, -0.00106515,  0.0016964 , -0.00221449, -0.00290092,\n",
    "         0.00271984, -0.45735744, -4.79065294,  2.97879505,  0.01416133,\n",
    "        -0.00000034, -0.00137325, -0.02769856,  0.04389118,  0.04973894,\n",
    "         0.02432715, -0.05210853,  0.02308885, -0.00108116, -0.04149962,\n",
    "        -2.20277809, -4.33652001, -4.16459559,  2.04064268, -1.03013765,\n",
    "         3.83055104,  1.62884149,  0.35601388, -0.01993219,  0.05114153,\n",
    "         0.30685693,  0.06648355, -0.09847323, -0.01105577, -0.00092057,\n",
    "         0.15544294,  0.44067536]),\n",
    " array([-0.0007005 , -0.01280817, -0.19399691, -0.00226798, -0.00094766,\n",
    "        -0.02212284,  0.00001288, -0.00002797,  0.00003533, -0.00015851,\n",
    "        -0.00522484, -0.00001269,  0.0021411 ,  0.00703303,  0.00118461,\n",
    "        -0.00888536, -0.00106519,  0.00169612, -0.00217202, -0.00290558,\n",
    "         0.00268114, -0.36170381, -4.79186361,  2.88438259,  0.01408833,\n",
    "        -0.00000033, -0.00136093, -0.02770539,  0.04373468,  0.04970242,\n",
    "         0.02433052, -0.05182131,  0.02293104, -0.00115428, -0.04138258,\n",
    "        -2.18178691, -4.33519415, -4.16488439,  2.02854785, -1.02348517,\n",
    "         3.81902746,  1.62324133,  0.35606223, -0.01983695,  0.05104307,\n",
    "         0.30680695,  0.06635623, -0.09856398, -0.01105779, -0.00091914,\n",
    "         0.15538891,  0.44039542]),\n",
    " array([-0.00066428, -0.0128173 , -0.19391111, -0.00226387, -0.00094691,\n",
    "        -0.02213787,  0.00001272, -0.00002448,  0.00003528, -0.00015839,\n",
    "        -0.00522122, -0.00001277,  0.00213997,  0.00702847,  0.00118414,\n",
    "        -0.00888586, -0.00106523,  0.00169582, -0.0021265 , -0.00291055,\n",
    "         0.00263965, -0.25919227, -4.79315281,  2.78319317,  0.01401011,\n",
    "        -0.00000033, -0.00134772, -0.02771275,  0.043564  ,  0.04966328,\n",
    "         0.02433464, -0.05151393,  0.02276213, -0.00122879, -0.0412583 ,\n",
    "        -2.15933463, -4.3337571 , -4.16520664,  2.01560734, -1.01637708,\n",
    "         3.80668933,  1.61728944,  0.35611398, -0.01973215,  0.05093479,\n",
    "         0.30675074,  0.06621691, -0.09866385, -0.01105996, -0.0009176 ,\n",
    "         0.15533075,  0.44009867]),\n",
    " array([-0.00062528, -0.01282702, -0.19381911, -0.00225946, -0.0009461 ,\n",
    "        -0.02215386,  0.00001254, -0.00002074,  0.00003523, -0.00015827,\n",
    "        -0.00521739, -0.00001286,  0.00213877,  0.00702358,  0.00118364,\n",
    "        -0.00888639, -0.00106527,  0.00169549, -0.00207773, -0.0029159 ,\n",
    "         0.0025952 , -0.1493327 , -4.79452742,  2.67474284,  0.01392628,\n",
    "        -0.00000033, -0.00133357, -0.02772067,  0.04337913,  0.04962169,\n",
    "         0.02433984, -0.05118508,  0.02258052, -0.0013061 , -0.04112548,\n",
    "        -2.13528923, -4.33220102, -4.16555796,  2.00176289, -1.00876626,\n",
    "         3.79346121,  1.61092941,  0.35616934, -0.01961966,  0.05081859,\n",
    "         0.30669017,  0.06606732, -0.09877107, -0.0110623 , -0.00091595,\n",
    "         0.15526831,  0.4397832 ]),\n",
    " array([-0.00058288, -0.01283741, -0.19372022, -0.00225474, -0.00094523,\n",
    "        -0.02217072,  0.00001235, -0.00001673,  0.00003518, -0.00015813,\n",
    "        -0.00521342, -0.00001296,  0.00213747,  0.00701834,  0.0011831 ,\n",
    "        -0.00888695, -0.00106532,  0.00169515, -0.00202546, -0.00292163,\n",
    "         0.00254757, -0.03159843, -4.79599663,  2.55851534,  0.01383643,\n",
    "        -0.00000033, -0.0013184 , -0.02772924,  0.0431763 ,  0.04957742,\n",
    "         0.02434704, -0.05083373,  0.02238506, -0.00138246, -0.04098453,\n",
    "        -2.10958236, -4.33047662, -4.16594449,  1.98697533, -1.00063305,\n",
    "         3.77927567,  1.60417069,  0.35622858, -0.01950208,  0.05069716,\n",
    "         0.30662775,  0.06590968, -0.09888294, -0.01106481, -0.00091419,\n",
    "         0.15520071,  0.43944825]),\n",
    " array([-0.00054317, -0.01284075, -0.193598  , -0.00224969, -0.00094442,\n",
    "        -0.02218371,  0.00001215, -0.00001243,  0.00003512, -0.000158  ,\n",
    "        -0.00520803, -0.00001318,  0.00213608,  0.0070155 ,  0.00118245,\n",
    "        -0.0088875 , -0.00106538,  0.00169446, -0.00196425, -0.00292749,\n",
    "         0.00249585,  0.01582939, -4.77771609,  2.49065561,  0.01373858,\n",
    "        -0.00000033, -0.00130216, -0.02772698,  0.04294594,  0.0495264 ,\n",
    "         0.02436138, -0.05046258,  0.02218033, -0.00146544, -0.0408361 ,\n",
    "        -2.08165946, -4.32749754, -4.1663783 ,  1.97126484, -0.99188599,\n",
    "         3.76401479,  1.59677202,  0.35632009, -0.01936463,  0.05056305,\n",
    "         0.30651888,  0.0657421 , -0.09900752, -0.01106658, -0.00091183,\n",
    "         0.15514538,  0.43963666]),\n",
    " array([-0.00049773, -0.01284381, -0.1934669 , -0.00224423, -0.00094351,\n",
    "        -0.02219615,  0.00001193, -0.00000784,  0.00003506, -0.00015785,\n",
    "        -0.00520413, -0.0000133 ,  0.00213453,  0.00701423,  0.00118172,\n",
    "        -0.00888875, -0.00106541,  0.00169357, -0.00190111, -0.00293252,\n",
    "         0.0024387 ,  0.03964787, -4.77480354,  2.4632989 ,  0.01363397,\n",
    "        -0.00000033, -0.00128473, -0.02772817,  0.04269795,  0.04947416,\n",
    "         0.02437112, -0.05005847,  0.02196207, -0.00155115, -0.04067756,\n",
    "        -2.05187635, -4.32508084, -4.16670206,  1.9543068 , -0.98240914,\n",
    "         3.74751553,  1.58907685,  0.35641606, -0.01922374,  0.05041708,\n",
    "         0.30645876,  0.06556517, -0.09914385, -0.01107043, -0.00090975,\n",
    "         0.15507948,  0.43926653]),\n",
    " array([-0.00044673, -0.01285335, -0.19333519, -0.00223836, -0.0009425 ,\n",
    "        -0.0222107 ,  0.00001169, -0.00000294,  0.000035  , -0.00015767,\n",
    "        -0.00520056, -0.00001343,  0.00213285,  0.00701133,  0.001181  ,\n",
    "        -0.0088897 , -0.00106547,  0.00169286, -0.00183562, -0.00293804,\n",
    "         0.0023782 ,  0.06429766, -4.77396709,  2.43774602,  0.01352291,\n",
    "        -0.00000032, -0.00126605, -0.02773608,  0.04243004,  0.04942392,\n",
    "         0.02438367, -0.04962698,  0.02172535, -0.00163051, -0.04051051,\n",
    "        -2.02034598, -4.3227692 , -4.16710382,  1.93621575, -0.97232201,\n",
    "         3.7298551 ,  1.58100768,  0.35649678, -0.019074  ,  0.0502642 ,\n",
    "         0.3063751 ,  0.06536666, -0.09928731, -0.01107445, -0.00090779,\n",
    "         0.15499792,  0.4388732 ]),\n",
    " array([-0.00039212, -0.01286424, -0.19319764, -0.00223206, -0.00094142,\n",
    "        -0.02222652,  0.00001144,  0.00000121,  0.00003498, -0.00015745,\n",
    "        -0.00519922, -0.00001351,  0.00213111,  0.00700616,  0.00118031,\n",
    "        -0.0088903 , -0.00106554,  0.0016922 , -0.00176583, -0.00294397,\n",
    "         0.00231335,  0.08841129, -4.77367488,  2.413448  ,  0.01340407,\n",
    "        -0.00000032, -0.00124604, -0.02774604,  0.04214171,  0.04937033,\n",
    "         0.02439709, -0.04916559,  0.02147228, -0.00171088, -0.04033232,\n",
    "        -1.98668211, -4.32057481, -4.16760509,  1.91683504, -0.96150659,\n",
    "         3.71099919,  1.57242096,  0.35657338, -0.01891276,  0.05010028,\n",
    "         0.30627158,  0.06515384, -0.09944068, -0.01107833, -0.00090609,\n",
    "         0.15491477,  0.43846743]),\n",
    " array([-0.0003366 , -0.01287585, -0.19304917, -0.00222522, -0.00094032,\n",
    "        -0.0222426 ,  0.00001117,  0.00000401,  0.00003502, -0.00015717,\n",
    "        -0.00519919, -0.00001353,  0.00212929,  0.00699975,  0.00117966,\n",
    "        -0.00889057, -0.00106565,  0.00169164, -0.0016914 , -0.00295068,\n",
    "         0.00224423,  0.11254248, -4.77375366,  2.38967511,  0.01327644,\n",
    "        -0.00000032, -0.00122459, -0.02775567,  0.04182336,  0.04931737,\n",
    "         0.02441499, -0.0486747 ,  0.02120285, -0.00178705, -0.04014405,\n",
    "        -1.95073101, -4.31832946, -4.16818599,  1.89612608, -0.94990607,\n",
    "         3.69080106,  1.56338904,  0.35664354, -0.01873978,  0.04992343,\n",
    "         0.30614382,  0.06492961, -0.09960306, -0.01108084, -0.00090461,\n",
    "         0.15483448,  0.43802989]),\n",
    " array([-0.00027987, -0.01288926, -0.19289039, -0.00221779, -0.00093919,\n",
    "        -0.02225874,  0.00001088,  0.00000681,  0.00003506, -0.00015688,\n",
    "        -0.00519661, -0.00001355,  0.00212737,  0.00699389,  0.00117901,\n",
    "        -0.00889098, -0.00106578,  0.00169108, -0.00161157, -0.00295816,\n",
    "         0.00217045,  0.13605172, -4.77368553,  2.36634324,  0.0131395 ,\n",
    "        -0.00000031, -0.0012016 , -0.02776499,  0.0414762 ,  0.04925943,\n",
    "         0.02443251, -0.04814817,  0.02091896, -0.00186706, -0.03994264,\n",
    "        -1.91223441, -4.31593809, -4.1688305 ,  1.87389571, -0.93743601,\n",
    "         3.66919658,  1.55377913,  0.35671791, -0.01855353,  0.04973266,\n",
    "         0.30600674,  0.06468787, -0.09977748, -0.0110839 , -0.00090297,\n",
    "         0.15474811,  0.43755724]),\n",
    " array([-0.00024342, -0.01290292, -0.19272608, -0.00220981, -0.00093777,\n",
    "        -0.02228434,  0.00001057,  0.0000096 ,  0.00003511, -0.0001566 ,\n",
    "        -0.0051892 , -0.00001357,  0.00212545,  0.00699118,  0.00117844,\n",
    "        -0.00889187, -0.00106589,  0.00169036, -0.00152463, -0.00296587,\n",
    "         0.00208994,  0.15837877, -4.7733419 ,  2.34405271,  0.01299329,\n",
    "        -0.00000031, -0.001177  , -0.02777642,  0.04108243,  0.04920702,\n",
    "         0.0244728 , -0.04759881,  0.02061726, -0.00194102, -0.03972615,\n",
    "        -1.87127245, -4.3129899 , -4.16972801,  1.85075452, -0.92451836,\n",
    "         3.64606603,  1.54368391,  0.35679792, -0.01835356,  0.04952578,\n",
    "         0.30587057,  0.06443172, -0.09996508, -0.01108761, -0.00090125,\n",
    "         0.15466564,  0.43698258]),\n",
    " array([-0.00022204, -0.01291707, -0.19255627, -0.00220118, -0.00093613,\n",
    "        -0.02231645,  0.00001024,  0.00001228,  0.00003518, -0.00015632,\n",
    "        -0.00517803, -0.00001359,  0.0021235 ,  0.00699098,  0.00117792,\n",
    "        -0.00889308, -0.001066  ,  0.00168949, -0.00143187, -0.00297336,\n",
    "         0.00200348,  0.17928758, -4.77263638,  2.32293851,  0.01283703,\n",
    "        -0.00000031, -0.00115068, -0.02778965,  0.04064151,  0.04915755,\n",
    "         0.02452858, -0.04701452,  0.0202965 , -0.00201511, -0.03949145,\n",
    "        -1.82745757, -4.30972357, -4.1708646 ,  1.82618157, -0.91079926,\n",
    "         3.62138459,  1.53290767,  0.35688108, -0.01813986,  0.04930295,\n",
    "         0.3057346 ,  0.06415999, -0.10016542, -0.01109206, -0.00089959,\n",
    "         0.15458458,  0.43630851]),\n",
    " array([-0.00020898, -0.01293162, -0.19237798, -0.00219175, -0.00093432,\n",
    "        -0.0223534 ,  0.00000989,  0.00001489,  0.00003524, -0.00015603,\n",
    "        -0.00516416, -0.00001362,  0.00212147,  0.00699277,  0.00117744,\n",
    "        -0.00889452, -0.0010661 ,  0.0016885 , -0.00133268, -0.00298185,\n",
    "         0.00191168,  0.19739109, -4.77166974,  2.30435531,  0.01266988,\n",
    "        -0.00000031, -0.00112247, -0.02780532,  0.04015408,  0.04910438,\n",
    "         0.02459187, -0.04639281,  0.01996121, -0.00208512, -0.03924191,\n",
    "        -1.78069387, -4.30608502, -4.17216654,  1.80006922, -0.89628758,\n",
    "         3.59499791,  1.52152885,  0.35696643, -0.01791088,  0.04906324,\n",
    "         0.30559459,  0.06386872, -0.10037963, -0.01109739, -0.00089782,\n",
    "         0.15450123,  0.43556157]),\n",
    " array([-0.00019479, -0.01294699, -0.19218884, -0.00218137, -0.00093244,\n",
    "        -0.02239112,  0.00000951,  0.00001741,  0.00003532, -0.00015573,\n",
    "        -0.00514958, -0.00001369,  0.00211931,  0.00699551,  0.00117701,\n",
    "        -0.00889597, -0.00106619,  0.00168744, -0.00122623, -0.00299099,\n",
    "         0.00181332,  0.21212827, -4.77038833,  2.28878095,  0.01249093,\n",
    "        -0.0000003 , -0.00109225, -0.02782327,  0.03962521,  0.04904887,\n",
    "         0.02466666, -0.04573115,  0.01959834, -0.00214878, -0.0389782 ,\n",
    "        -1.73079707, -4.30191699, -4.17358596,  1.77229683, -0.88071424,\n",
    "         3.56658403,  1.5094974 ,  0.35705159, -0.0176655 ,  0.04880603,\n",
    "         0.30544536,  0.06355429, -0.10060772, -0.01110374, -0.000896  ,\n",
    "         0.15441131,  0.43476414]),\n",
    " array([-0.00017471, -0.01296333, -0.19198843, -0.0021699 , -0.00093054,\n",
    "        -0.02242774,  0.00000911,  0.0000198 ,  0.0000354 , -0.00015539,\n",
    "        -0.00513512, -0.0000138 ,  0.00211701,  0.00699875,  0.00117662,\n",
    "        -0.00889733, -0.00106627,  0.00168633, -0.00111201, -0.00300084,\n",
    "         0.00170793,  0.22233843, -4.76874993,  2.2772704 ,  0.0122993 ,\n",
    "        -0.0000003 , -0.00105987, -0.02784351,  0.03904506,  0.04898471,\n",
    "         0.02474184, -0.04502191,  0.0192148 , -0.00219881, -0.03870098,\n",
    "        -1.67753687, -4.29743385, -4.17512518,  1.74254076, -0.86391814,\n",
    "         3.53610256,  1.49679131,  0.35713524, -0.01739583,  0.04852334,\n",
    "         0.3052783 ,  0.06320756, -0.10085683, -0.0111112 , -0.00089418,\n",
    "         0.15431242,  0.43392639]),\n",
    " array([-0.0001498 , -0.01298086, -0.1917768 , -0.00215721, -0.00092861,\n",
    "        -0.02246356,  0.00000868,  0.000022  ,  0.00003549, -0.00015504,\n",
    "        -0.00512029, -0.00001397,  0.00211457,  0.00700252,  0.00117626,\n",
    "        -0.00889855, -0.00106634,  0.00168517, -0.00098739, -0.00301086,\n",
    "         0.00159232,  0.22746368, -4.76669722,  2.27039046,  0.01209405,\n",
    "        -0.0000003 , -0.00102516, -0.02786613,  0.03840973,  0.04891529,\n",
    "         0.02482612, -0.04426445,  0.01880219, -0.00223249, -0.03840964,\n",
    "        -1.62075833, -4.29246611, -4.17684183,  1.71078102, -0.84578103,\n",
    "         3.50336128,  1.4833842 ,  0.35721718, -0.01711332,  0.04822678,\n",
    "         0.30510593,  0.06284001, -0.10111591, -0.01111978, -0.00089246,\n",
    "         0.15420394,  0.43303187]),\n",
    " array([-0.00012065, -0.01299952, -0.19155314, -0.00214314, -0.00092662,\n",
    "        -0.02250047,  0.00000822,  0.00002398,  0.00003558, -0.00015465,\n",
    "        -0.0051045 , -0.00001417,  0.00211198,  0.00700688,  0.00117593,\n",
    "        -0.00889973, -0.00106642,  0.00168396, -0.00085579, -0.00302227,\n",
    "         0.00147116,  0.23060851, -4.76433725,  2.26506807,  0.01187417,\n",
    "        -0.00000029, -0.00098796, -0.02789147,  0.03771261,  0.04883916,\n",
    "         0.02491976, -0.04345535,  0.01835981, -0.00224712, -0.03810305,\n",
    "        -1.56014791, -4.2869458 , -4.17876134,  1.67687156, -0.82628098,\n",
    "         3.46822504,  1.46921858,  0.35729834, -0.01680251,  0.04790051,\n",
    "         0.30491414,  0.06243642, -0.10140049, -0.01112949, -0.00089066,\n",
    "         0.1540871 ,  0.43207816]),\n",
    " array([-0.00008873, -0.01301967, -0.19131766, -0.00212749, -0.00092455,\n",
    "        -0.02253885,  0.00000773,  0.00002562,  0.00003568, -0.00015424,\n",
    "        -0.00508711, -0.00001441,  0.00210923,  0.00701197,  0.00117563,\n",
    "        -0.00890087, -0.00106651,  0.00168268, -0.00071461, -0.00303449,\n",
    "         0.00134128,  0.23182029, -4.7615695 ,  2.26119495,  0.01163861,\n",
    "        -0.00000029, -0.00094809, -0.0279193 ,  0.03694593,  0.04875391,\n",
    "         0.02501735, -0.04258732,  0.01789147, -0.00224326, -0.0377783 ,\n",
    "        -1.4953718 , -4.28105771, -4.18090572,  1.64050975, -0.80522781,\n",
    "         3.43060581,  1.45416617,  0.3573788 , -0.01647651,  0.04755771,\n",
    "         0.30471799,  0.06201006, -0.10169788, -0.01114038, -0.00088891,\n",
    "         0.15396118,  0.43104504]),\n",
    " array([-0.00005626, -0.01304157, -0.19107041, -0.0021101 , -0.0009224 ,\n",
    "        -0.02257975,  0.00000721,  0.00002693,  0.00003578, -0.00015381,\n",
    "        -0.00506718, -0.00001468,  0.00210633,  0.00701788,  0.00117535,\n",
    "        -0.00890197, -0.00106661,  0.0016813 , -0.00056065, -0.00304688,\n",
    "         0.00119879,  0.23202536, -4.75833407,  2.25789646,  0.01138632,\n",
    "        -0.00000028, -0.00090538, -0.02794956,  0.03610451,  0.04866663,\n",
    "         0.02513449, -0.04166588,  0.01738268, -0.00221193, -0.03743439,\n",
    "        -1.42632644, -4.27450992, -4.18335108,  1.60193379, -0.78253044,\n",
    "         3.3901279 ,  1.43821093,  0.35746011, -0.01611752,  0.04717998,\n",
    "         0.30450444,  0.06154368, -0.10202637, -0.01115251, -0.00088729,\n",
    "         0.15382789,  0.4299152 ]),\n",
    " array([-0.00002672, -0.0130648 , -0.19081287, -0.00209095, -0.00092004,\n",
    "        -0.02262758,  0.00000665,  0.00002812,  0.00003588, -0.00015335,\n",
    "        -0.00504374, -0.00001496,  0.00210329,  0.00702475,  0.00117503,\n",
    "        -0.00890323, -0.00106675,  0.00167982, -0.00039804, -0.00306061,\n",
    "         0.00104904,  0.23196702, -4.75483495,  2.25454687,  0.01111596,\n",
    "        -0.00000028, -0.00085958, -0.02798228,  0.0351736 ,  0.04856808,\n",
    "         0.02525536, -0.04067829,  0.01685326, -0.00215435, -0.03707143,\n",
    "        -1.35256894, -4.26771214, -4.18622978,  1.56060003, -0.75833599,\n",
    "         3.34709909,  1.42138432,  0.35754547, -0.015731  ,  0.04677289,\n",
    "         0.30427927,  0.06104443, -0.10238151, -0.01116548, -0.00088551,\n",
    "         0.15368735,  0.42868974]),\n",
    " array([-0.00000543, -0.01308864, -0.19054534, -0.00206992, -0.00091741,\n",
    "        -0.02268505,  0.00000606,  0.00002924,  0.00003597, -0.00015287,\n",
    "        -0.00501559, -0.00001524,  0.00210009,  0.00703277,  0.00117464,\n",
    "        -0.0089047 , -0.00106693,  0.00167822, -0.00022367, -0.00307505,\n",
    "         0.00088821,  0.23182259, -4.75106267,  2.25109652,  0.01082629,\n",
    "        -0.00000027, -0.0008105 , -0.02801756,  0.03415031,  0.04846494,\n",
    "         0.02539832, -0.03963118,  0.01628789, -0.00206525, -0.03668721,\n",
    "        -1.273958  , -4.26021153, -4.18962783,  1.51678498, -0.73262003,\n",
    "         3.30112794,  1.40363945,  0.35763699, -0.01531457,  0.04633376,\n",
    "         0.30404435,  0.06051122, -0.10276658, -0.01117924, -0.00088363,\n",
    "         0.15354185,  0.42734318]),\n",
    " array([-0.        , -0.01310852, -0.19027243, -0.00204689, -0.00091441,\n",
    "        -0.02275397,  0.00000543,  0.00003028,  0.00003605, -0.00015234,\n",
    "        -0.00498229, -0.00001558,  0.00209675,  0.00704244,  0.00117413,\n",
    "        -0.0089063 , -0.00106715,  0.00167651, -0.00003341, -0.00308925,\n",
    "         0.00071135,  0.23164266, -4.74680844,  2.24736844,  0.01051588,\n",
    "        -0.00000027, -0.00075789, -0.02805521,  0.03302329,  0.0483525 ,\n",
    "         0.02556608, -0.03852279,  0.0156891 , -0.00194042, -0.03628011,\n",
    "        -1.1902871 , -4.25163783, -4.19365048,  1.47031986, -0.70528799,\n",
    "         3.2520979 ,  1.38495984,  0.35773922, -0.01486565,  0.04585972,\n",
    "         0.30380421,  0.05994298, -0.10318565, -0.01119415, -0.0008818 ,\n",
    "         0.15339388,  0.42585647]),\n",
    " array([-0.00003683, -0.01319104, -0.1899797 , -0.0020215 , -0.00091184,\n",
    "        -0.02280005,  0.00000475,  0.00003151,  0.00003612, -0.000152  ,\n",
    "        -0.00494919, -0.00001502,  0.00209309,  0.00704038,  0.0011738 ,\n",
    "        -0.00890055, -0.00106663,  0.00167384,  0.00000125, -0.00296154,\n",
    "         0.00053888,  0.24667512, -4.73686743,  2.23773957,  0.01017468,\n",
    "        -0.00000026, -0.00070168, -0.02805251,  0.03176446,  0.04839579,\n",
    "         0.0257659 , -0.03734142,  0.0150578 , -0.00178464, -0.03584404,\n",
    "        -1.10170893, -4.25132005, -4.19779622,  1.42037321, -0.67445574,\n",
    "         3.19882414,  1.36678668,  0.35770258, -0.01437153,  0.04533775,\n",
    "         0.30363513,  0.05937535, -0.10367575, -0.01122185, -0.00089895,\n",
    "         0.15325476,  0.42341413]),\n",
    " array([-0.00003505, -0.01327056, -0.18966536, -0.00199343, -0.00091003,\n",
    "        -0.02279772,  0.00000401,  0.00003305,  0.00003617, -0.00015185,\n",
    "        -0.00492442, -0.00001453,  0.00208954,  0.00703321,  0.0011722 ,\n",
    "        -0.0088914 , -0.00106507,  0.00167035,  0.00000544, -0.00280125,\n",
    "         0.00036812,  0.26049778, -4.71772919,  2.21882438,  0.00980897,\n",
    "        -0.00000025, -0.0006416 , -0.02804229,  0.030415  ,  0.04843904,\n",
    "         0.02596993, -0.03606294,  0.01436301, -0.00157825, -0.03539166,\n",
    "        -1.00901992, -4.25036045, -4.2016661 ,  1.36695697, -0.64030504,\n",
    "         3.14055197,  1.34844229,  0.35784728, -0.01385063,  0.04477616,\n",
    "         0.3035357 ,  0.05874766, -0.10418114, -0.01124171, -0.00092066,\n",
    "         0.1530364 ,  0.42112432]),\n",
    " array([-0.00001495, -0.01334668, -0.18934251, -0.00196265, -0.00090814,\n",
    "        -0.02278398,  0.00000321,  0.00003467,  0.00003621, -0.00015169,\n",
    "        -0.00490173, -0.00001399,  0.00208584,  0.00702779,  0.00117013,\n",
    "        -0.00888409, -0.00106364,  0.00166636,  0.00000858, -0.00263898,\n",
    "         0.00019683,  0.27398973, -4.69694733,  2.19854724,  0.00941789,\n",
    "        -0.00000024, -0.00057725, -0.02803401,  0.02896045,  0.0484636 ,\n",
    "         0.02617477, -0.03468471,  0.01362426, -0.00133378, -0.03491688,\n",
    "        -0.91012189, -4.2491038 , -4.20579095,  1.3095171 , -0.60335741,\n",
    "         3.07807623,  1.32883733,  0.35800867, -0.01328998,  0.04416559,\n",
    "         0.30342047,  0.05807358, -0.10471404, -0.01126059, -0.00094269,\n",
    "         0.15280387,  0.41867031]),\n",
    " array([-0.0000036 , -0.01342391, -0.18901176, -0.00192916, -0.00090595,\n",
    "        -0.02277212,  0.00000237,  0.00003634,  0.00003624, -0.00015149,\n",
    "        -0.00487421, -0.00001339,  0.00208197,  0.00702536,  0.00116778,\n",
    "        -0.00887701, -0.00106235,  0.00166184,  0.0000105 , -0.00247409,\n",
    "         0.00002376,  0.28817558, -4.67526443,  2.17726933,  0.00900006,\n",
    "        -0.00000023, -0.00050839, -0.02802804,  0.02739959,  0.04848823,\n",
    "         0.02640234, -0.03321465,  0.01283184, -0.00107236, -0.03440496,\n",
    "        -0.80432755, -4.24759575, -4.21047309,  1.24828123, -0.56347991,\n",
    "         3.01113938,  1.30757039,  0.3581798 , -0.01267847,  0.04350091,\n",
    "         0.30329256,  0.05733388, -0.10529747, -0.01128145, -0.00096545,\n",
    "         0.15255865,  0.41601074]),\n",
    " array([-0.        , -0.01335365, -0.18862964, -0.00189342, -0.00090464,\n",
    "        -0.02278112,  0.00000148,  0.0000379 ,  0.00003624, -0.00015101,\n",
    "        -0.00485782, -0.00001357,  0.00207901,  0.00704921,  0.00116379,\n",
    "        -0.00887203, -0.00106173,  0.0016529 , -0.        , -0.00239522,\n",
    "        -0.        ,  0.26754329, -4.6240149 ,  2.125292  ,  0.008543  ,\n",
    "        -0.00000021, -0.00043478, -0.02796205,  0.02572519,  0.04826112,\n",
    "         0.02664744, -0.03165015,  0.0120019 , -0.00080978, -0.03386169,\n",
    "        -0.68937491, -4.23405052, -4.21614246,  1.18274227, -0.52060467,\n",
    "         2.9398349 ,  1.28329407,  0.35889104, -0.01203295,  0.04270446,\n",
    "         0.3039396 ,  0.05669703, -0.1059217 , -0.0113213 , -0.00097652,\n",
    "         0.15246023,  0.41345608]),\n",
    " array([-0.        , -0.01335973, -0.18829442, -0.00185487, -0.00090127,\n",
    "        -0.02285338,  0.00000054,  0.00003908,  0.00003624, -0.00015041,\n",
    "        -0.00483104, -0.00001333,  0.00207549,  0.00708434,  0.00116188,\n",
    "        -0.00887565, -0.00106211,  0.00164727, -0.        , -0.00238529,\n",
    "        -0.        ,  0.26521445, -4.60389023,  2.10429234,  0.00806668,\n",
    "        -0.0000002 , -0.0003554 , -0.02797488,  0.02394856,  0.04812327,\n",
    "         0.02690314, -0.02997511,  0.01111822, -0.00055712, -0.0332657 ,\n",
    "        -0.56410683, -4.22560601, -4.22367601,  1.11289333, -0.47553108,\n",
    "         2.86505993,  1.25500263,  0.35913495, -0.01133013,  0.04192304,\n",
    "         0.30382578,  0.05586917, -0.10657674, -0.01134469, -0.00097871,\n",
    "         0.15230833,  0.41064981]),\n",
    " array([-0.        , -0.01337816, -0.18796564, -0.00181279, -0.00089758,\n",
    "        -0.02294133, -0.0000001 ,  0.00002881,  0.00003622, -0.0001502 ,\n",
    "        -0.00483755, -0.0000137 ,  0.00207192,  0.00710474,  0.00116114,\n",
    "        -0.00887602, -0.00106206,  0.00164245, -0.        , -0.00237885,\n",
    "        -0.        ,  0.26421021, -4.5877627 ,  2.08723155,  0.00755697,\n",
    "        -0.00000019, -0.00027035, -0.02800329,  0.02207054,  0.04798406,\n",
    "         0.02715684, -0.02816966,  0.01016858, -0.00032198, -0.03260498,\n",
    "        -0.42926285, -4.21897406, -4.23246706,  1.03741916, -0.4263439 ,\n",
    "         2.78564116,  1.22419437,  0.35941455, -0.01056179,  0.04108448,\n",
    "         0.30350169,  0.05496005, -0.10727833, -0.01136593, -0.0009835 ,\n",
    "         0.15213985,  0.40803086]),\n",
    " array([-0.        , -0.01340076, -0.18758199, -0.00176574, -0.00089416,\n",
    "        -0.02302663, -0.00000032,  0.00000356,  0.00003613, -0.00015037,\n",
    "        -0.00487749, -0.0000148 ,  0.00206764,  0.0071234 ,  0.0011612 ,\n",
    "        -0.00887909, -0.00106197,  0.00163804, -0.        , -0.00237288,\n",
    "        -0.        ,  0.26368379, -4.57046405,  2.0683263 ,  0.00700892,\n",
    "        -0.00000018, -0.00017918, -0.02802776,  0.0200786 ,  0.04785895,\n",
    "         0.02741759, -0.02624143,  0.00914335, -0.00009685, -0.0318789 ,\n",
    "        -0.2845938 , -4.21337391, -4.24192336,  0.95655026, -0.37210567,\n",
    "         2.69988022,  1.19106198,  0.35971232, -0.00973127,  0.04016517,\n",
    "         0.30296654,  0.05400523, -0.10799014, -0.01137716, -0.00098987,\n",
    "         0.15197647,  0.40540703]),\n",
    " array([-0.        , -0.01339889, -0.18721059, -0.00171511, -0.00089004,\n",
    "        -0.02313444, -0.00000052, -0.        ,  0.00003503, -0.00015091,\n",
    "        -0.00486309, -0.00001768,  0.00206194,  0.00715252,  0.00115959,\n",
    "        -0.00888316, -0.00106178,  0.00163267, -0.        , -0.00236127,\n",
    "        -0.        ,  0.2588009 , -4.54572199,  2.0431997 ,  0.00642603,\n",
    "        -0.00000017, -0.00008147, -0.02806991,  0.01821011,  0.04755453,\n",
    "         0.02749392, -0.02430362,  0.00811361, -0.        , -0.03096145,\n",
    "        -0.16539007, -4.19308286, -4.239702  ,  0.87540879, -0.32181961,\n",
    "         2.63902471,  1.14912522,  0.36019459, -0.00884422,  0.03919674,\n",
    "         0.30265213,  0.05293509, -0.10879679, -0.01141209, -0.00099075,\n",
    "         0.15165103,  0.4022767 ]),\n",
    " array([-0.        , -0.0133831 , -0.18682655, -0.00166318, -0.00088468,\n",
    "        -0.02330393, -0.00000071, -0.        ,  0.00003374, -0.00015128,\n",
    "        -0.00484435, -0.00002016,  0.00205538,  0.00715925,  0.00115622,\n",
    "        -0.00888534, -0.00106234,  0.0016278 , -0.        , -0.00235013,\n",
    "        -0.        ,  0.25378702, -4.52288357,  2.02048911,  0.00592517,\n",
    "        -0.00000015, -0.        , -0.02801859,  0.01634964,  0.04719678,\n",
    "         0.0274646 , -0.02227722,  0.0070447 , -0.        , -0.02989402,\n",
    "        -0.05198823, -4.17178135, -4.23291114,  0.79060816, -0.27273369,\n",
    "         2.59015713,  1.10035013,  0.36076593, -0.00785566,  0.03813211,\n",
    "         0.30231223,  0.05181102, -0.10974116, -0.01144272, -0.00098849,\n",
    "         0.15134575,  0.39921872]),\n",
    " array([-0.        , -0.01340324, -0.18617778, -0.00160958, -0.00087883,\n",
    "        -0.02345502, -0.0000009 , -0.        ,  0.00003242, -0.00015167,\n",
    "        -0.00478554, -0.00002273,  0.00204661,  0.00717197,  0.00115267,\n",
    "        -0.00889146, -0.00106362,  0.00162173, -0.        , -0.0023378 ,\n",
    "        -0.        ,  0.25397359, -4.49481934,  1.99280724,  0.00589751,\n",
    "        -0.00000014, -0.        , -0.02790045,  0.01454561,  0.04655537,\n",
    "         0.02717273, -0.02020734,  0.00594813, -0.        , -0.02861023,\n",
    "        -0.        , -4.10784836, -4.18531336,  0.71330484, -0.23364465,\n",
    "         2.57918498,  1.03323594,  0.36176634, -0.00681214,  0.03697384,\n",
    "         0.30211935,  0.05058133, -0.11072731, -0.01149189, -0.00100606,\n",
    "         0.15094369,  0.3924119 ]),\n",
    " array([-0.        , -0.01340487, -0.18548792, -0.00155319, -0.00087231,\n",
    "        -0.02357823, -0.00000109, -0.        ,  0.00003102, -0.00015171,\n",
    "        -0.00471419, -0.0000251 ,  0.00203624,  0.00718689,  0.00114766,\n",
    "        -0.0088955 , -0.00106577,  0.00161429, -0.        , -0.00232906,\n",
    "        -0.        ,  0.25762166, -4.46437692,  1.96046726,  0.00587215,\n",
    "        -0.00000013, -0.        , -0.02778118,  0.01281624,  0.04584761,\n",
    "         0.02649992, -0.01828256,  0.00491621, -0.        , -0.02697733,\n",
    "        -0.        , -4.04317936, -4.12011141,  0.63734546, -0.20268788,\n",
    "         2.62382238,  0.95382637,  0.36276424, -0.00568287,  0.03575409,\n",
    "         0.30186647,  0.04922596, -0.11182521, -0.01154682, -0.00101704,\n",
    "         0.15044663,  0.38599115]),\n",
    " array([-0.        , -0.01341801, -0.18482953, -0.00149494, -0.00086557,\n",
    "        -0.02373095, -0.00000125, -0.        ,  0.00002953, -0.00015187,\n",
    "        -0.00464362, -0.00002707,  0.0020262 ,  0.00719192,  0.00114221,\n",
    "        -0.00889855, -0.00106815,  0.001607  , -0.        , -0.0023208 ,\n",
    "        -0.        ,  0.27516085, -4.43645952,  1.91631029,  0.00584232,\n",
    "        -0.00000011, -0.        , -0.02763601,  0.01100725,  0.04513575,\n",
    "         0.02577459, -0.01625936,  0.00382908, -0.        , -0.02523502,\n",
    "         0.        , -3.97945156, -4.05238266,  0.5562396 , -0.17202835,\n",
    "         2.67395735,  0.87041268,  0.36376616, -0.00445762,  0.03444146,\n",
    "         0.30149255,  0.04778937, -0.11302541, -0.01159617, -0.00102473,\n",
    "         0.14995662,  0.37977431]),\n",
    " array([-0.        , -0.01343337, -0.184122  , -0.00143415, -0.00085844,\n",
    "        -0.02389013, -0.00000139, -0.        ,  0.00002794, -0.00015205,\n",
    "        -0.00457026, -0.00002911,  0.0020155 ,  0.00719696,  0.0011364 ,\n",
    "        -0.00890201, -0.00107068,  0.00159917, -0.        , -0.00231213,\n",
    "        -0.        ,  0.28740059, -4.40698342,  1.87603   ,  0.0058099 ,\n",
    "        -0.0000001 , -0.        , -0.02747796,  0.00904998,  0.04437962,\n",
    "         0.02499639, -0.01407282,  0.00266952, -0.        , -0.02337191,\n",
    "         0.        , -3.91126749, -3.97942091,  0.4684889 , -0.14010439,\n",
    "         2.72877129,  0.7813465 ,  0.36484209, -0.00312993,  0.03302023,\n",
    "         0.30106409,  0.04623825, -0.11432532, -0.01164767, -0.00103273,\n",
    "         0.14943523,  0.37319156]),\n",
    " array([-0.        , -0.01345023, -0.18334661, -0.00137045, -0.00085084,\n",
    "        -0.0240537 , -0.0000015 , -0.        ,  0.00002624, -0.00015225,\n",
    "        -0.00449224, -0.00003129,  0.00200383,  0.00720245,  0.00113009,\n",
    "        -0.00890601, -0.00107342,  0.00159069, -0.        , -0.00230323,\n",
    "        -0.        ,  0.29720769, -4.37569302,  1.83660745,  0.00577521,\n",
    "        -0.00000008, -0.        , -0.02730884,  0.00695286,  0.04356541,\n",
    "         0.02414821, -0.01172898,  0.00144217, -0.        , -0.02137667,\n",
    "         0.        , -3.83783926, -3.9003857 ,  0.3744104 , -0.10686046,\n",
    "         2.78736937,  0.68611209,  0.36600384, -0.00169472,  0.03148581,\n",
    "         0.30058935,  0.04456209, -0.11573189, -0.01170262, -0.00104118,\n",
    "         0.14887638,  0.36620214]),\n",
    " array([-0.        , -0.01346413, -0.18251282, -0.00130357, -0.00084258,\n",
    "        -0.02423381, -0.00000158, -0.        ,  0.00002442, -0.00015248,\n",
    "        -0.00441424, -0.00003364,  0.0019914 ,  0.0072122 ,  0.00112313,\n",
    "        -0.00891025, -0.0010763 ,  0.00158158, -0.        , -0.00229363,\n",
    "        -0.        ,  0.30610665, -4.34277536,  1.79640813,  0.00573968,\n",
    "        -0.00000007, -0.        , -0.02713498,  0.00481205,  0.04274314,\n",
    "         0.02347372, -0.00931192,  0.00017042, -0.        , -0.01951383,\n",
    "         0.        , -3.7539523 , -3.81970437,  0.27785085, -0.08021056,\n",
    "         2.84589692,  0.59712849,  0.36721427, -0.00037227,  0.03006214,\n",
    "         0.30026882,  0.04298406, -0.11702409, -0.01175736, -0.00104903,\n",
    "         0.14826716,  0.35938559]),\n",
    " array([-0.        , -0.01344227, -0.18167344, -0.00122897, -0.00083407,\n",
    "        -0.024386  , -0.00000163, -0.00001017,  0.00002272, -0.00015247,\n",
    "        -0.00431719, -0.00003606,  0.00197871,  0.00725314,  0.00111515,\n",
    "        -0.00891791, -0.00107969,  0.00157046, -0.        , -0.00228084,\n",
    "        -0.        ,  0.31810985, -4.29454971,  1.74240825,  0.00570369,\n",
    "        -0.00000005, -0.        , -0.02694747,  0.00225821,  0.041427  ,\n",
    "         0.02252802, -0.00656032,  0.        , -0.        , -0.01847489,\n",
    "         0.        , -3.67559093, -3.75748665,  0.17138907, -0.0536315 ,\n",
    "         2.91545775,  0.5340609 ,  0.36870709, -0.        ,  0.02957214,\n",
    "         0.30110349,  0.04241935, -0.1174361 , -0.01180426, -0.00108671,\n",
    "         0.1474968 ,  0.34987795]),\n",
    " array([-0.        , -0.01342848, -0.18101211, -0.00114966, -0.00082624,\n",
    "        -0.02453964, -0.00000163, -0.00002368,  0.00002094, -0.00015253,\n",
    "        -0.00424336, -0.00003749,  0.00196734,  0.00729726,  0.0011071 ,\n",
    "        -0.00891874, -0.00108374,  0.00155925, -0.        , -0.00226759,\n",
    "        -0.        ,  0.32108441, -4.25392424,  1.6956938 ,  0.0056619 ,\n",
    "        -0.00000003, -0.        , -0.02673849,  0.        ,  0.04005258,\n",
    "         0.02124023, -0.00384324,  0.        , -0.        , -0.01727979,\n",
    "         0.        , -3.62322771, -3.70455072,  0.05152987, -0.02844806,\n",
    "         2.98729924,  0.48125035,  0.36972297, -0.        ,  0.02953115,\n",
    "         0.30197658,  0.04218979, -0.11746712, -0.01185159, -0.00108636,\n",
    "         0.14659619,  0.34631025]),\n",
    " array([ 0.        , -0.01354941, -0.18028286, -0.00106884, -0.00081975,\n",
    "        -0.02486972, -0.00000149, -0.00003769,  0.00001898, -0.00015304,\n",
    "        -0.00426382, -0.00003819,  0.00195702,  0.00726159,  0.00110277,\n",
    "        -0.0089017 , -0.00108692,  0.00155314, -0.        , -0.00225706,\n",
    "        -0.        ,  0.3131255 , -4.25544915,  1.68815939,  0.00560982,\n",
    "        -0.00000002, -0.        , -0.02651267, -0.        ,  0.03856602,\n",
    "         0.01958001, -0.00294122,  0.        , -0.        , -0.01572143,\n",
    "         0.        , -3.53941816, -3.60692783,  0.        , -0.0613534 ,\n",
    "         2.97415291,  0.4266198 ,  0.36971245, -0.        ,  0.02954832,\n",
    "         0.30245003,  0.04186235, -0.11742587, -0.01191098, -0.00104633,\n",
    "         0.14626057,  0.34847358]),\n",
    " array([ 0.        , -0.01364222, -0.17871079, -0.00098302, -0.00081099,\n",
    "        -0.025248  , -0.00000134, -0.0000517 ,  0.0000169 , -0.00015403,\n",
    "        -0.00422726, -0.00004157,  0.00193961,  0.00722032,  0.00109801,\n",
    "        -0.00890354, -0.00108708,  0.0015466 , -0.        , -0.00223568,\n",
    "        -0.        ,  0.31540724, -4.22398979,  1.64401652,  0.00556061,\n",
    "        -0.        ,  0.        , -0.02628555, -0.        ,  0.03709378,\n",
    "         0.01785061, -0.00264295,  0.        , -0.        , -0.0141045 ,\n",
    "         0.        , -3.40309673, -3.45729641,  0.        , -0.12303174,\n",
    "         2.92836455,  0.35444797,  0.37045399, -0.        ,  0.02951352,\n",
    "         0.30339   ,  0.04166736, -0.11742899, -0.01199158, -0.00103443,\n",
    "         0.14633855,  0.34516981]),\n",
    " array([ 0.        , -0.01373052, -0.17663642, -0.00088957, -0.00079821,\n",
    "        -0.02564621, -0.00000121, -0.0000659 ,  0.00001468, -0.00015472,\n",
    "        -0.00415647, -0.00004537,  0.00191582,  0.00719216,  0.00109055,\n",
    "        -0.00891535, -0.0010883 ,  0.00153664, -0.        , -0.00221764,\n",
    "        -0.        ,  0.31936355, -4.18498634,  1.59620154,  0.00551299,\n",
    "        -0.        ,  0.        , -0.02602876, -0.        ,  0.03545998,\n",
    "         0.01590448, -0.00235712,  0.        , -0.        , -0.01227941,\n",
    "         0.        , -3.26172071, -3.30055469,  0.        , -0.1766647 ,\n",
    "         2.87723441,  0.27268487,  0.37164912, -0.        ,  0.02947928,\n",
    "         0.30450164,  0.04144535, -0.11746629, -0.01207875, -0.00102502,\n",
    "         0.14623082,  0.34057604]),\n",
    " array([ 0.        , -0.01382333, -0.17427614, -0.00078891, -0.00078312,\n",
    "        -0.02605283, -0.00000111, -0.00008063,  0.00001231, -0.00015518,\n",
    "        -0.00407082, -0.00004922,  0.00188818,  0.00717021,  0.00108102,\n",
    "        -0.00893012, -0.00109044,  0.00152441, -0.        , -0.00220319,\n",
    "        -0.        ,  0.32890109, -4.1439371 ,  1.54377087,  0.00546428,\n",
    "        -0.        ,  0.        , -0.02576095, -0.        ,  0.03370036,\n",
    "         0.01372421, -0.00204708,  0.        , -0.        , -0.01022981,\n",
    "         0.        , -3.1164828 , -3.13483946,  0.        , -0.22626634,\n",
    "         2.82019834,  0.18173942,  0.37305593, -0.        ,  0.02946108,\n",
    "         0.30568633,  0.04116044, -0.11752994, -0.01217388, -0.00101478,\n",
    "         0.14600576,  0.33558144]),\n",
    " array([-0.        , -0.01391716, -0.17176138, -0.00068127, -0.00076671,\n",
    "        -0.0264699 , -0.00000102, -0.00009566,  0.00000978, -0.00015551,\n",
    "        -0.00397349, -0.00005315,  0.00185839,  0.00715346,  0.00107012,\n",
    "        -0.00894577, -0.00109315,  0.00151066, -0.        , -0.00218965,\n",
    "        -0.        ,  0.34418192, -4.10011578,  1.48385757,  0.00541355,\n",
    "        -0.        ,  0.        , -0.02547901, -0.        ,  0.03183866,\n",
    "         0.01145111, -0.00170872,  0.        , -0.        , -0.00809461,\n",
    "         0.        , -2.96531402, -2.96352125, -0.        , -0.27530336,\n",
    "         2.75843963,  0.08743114,  0.37457746, -0.        ,  0.02944918,\n",
    "         0.3069566 ,  0.04083183, -0.11760889, -0.01227538, -0.0010039 ,\n",
    "         0.14570217,  0.33040529]),\n",
    " array([-0.        , -0.01400141, -0.16920854, -0.00056707, -0.0007494 ,\n",
    "        -0.02690548, -0.00000097, -0.00011086,  0.00000711, -0.00015578,\n",
    "        -0.00387171, -0.0000571 ,  0.00182789,  0.00714777,  0.00105796,\n",
    "        -0.00896057, -0.00109628,  0.00149576, -0.        , -0.00217558,\n",
    "        -0.        ,  0.35755503, -4.05334894,  1.42291898,  0.00536208,\n",
    "        -0.        ,  0.        , -0.0251888 , -0.        ,  0.02997934,\n",
    "         0.00950011, -0.00132917,  0.        , -0.        , -0.00628086,\n",
    "         0.        , -2.80215918, -2.79550294, -0.        , -0.33459094,\n",
    "         2.69328444,  0.00880813,  0.37611373, -0.        ,  0.02943859,\n",
    "         0.30830009,  0.0404747 , -0.11769472, -0.01237674, -0.00099073,\n",
    "         0.14532023,  0.32598561]),\n",
    " array([-0.        , -0.01404153, -0.16666064, -0.00044439, -0.00072986,\n",
    "        -0.02733967, -0.00000087, -0.00012976,  0.00000427, -0.00015625,\n",
    "        -0.00374697, -0.00006115,  0.00179691,  0.00718632,  0.00104263,\n",
    "        -0.00897257, -0.00110041,  0.0014785 , -0.        , -0.00216106,\n",
    "        -0.        ,  0.37202332, -4.00383752,  1.35196106,  0.00531889,\n",
    "        -0.        ,  0.        , -0.02493346, -0.        ,  0.02866736,\n",
    "         0.00859368, -0.00100741,  0.        , -0.        , -0.0055218 ,\n",
    "         0.        , -2.65212133, -2.6645978 , -0.        , -0.41641321,\n",
    "         2.62920201,  0.        ,  0.37755986, -0.        ,  0.0294911 ,\n",
    "         0.30947046,  0.04005852, -0.11775793, -0.01248713, -0.00093397,\n",
    "         0.14457225,  0.33001052]),\n",
    " array([ 0.        , -0.01407731, -0.1639496 , -0.00031268, -0.00070905,\n",
    "        -0.02777845, -0.00000075, -0.00015082,  0.00000123, -0.00015681,\n",
    "        -0.00360103, -0.00006545,  0.00176391,  0.00723844,  0.0010258 ,\n",
    "        -0.0089848 , -0.00110505,  0.00145945, -0.        , -0.00214474,\n",
    "        -0.        ,  0.38429081, -3.94974973,  1.27577281,  0.00527426,\n",
    "        -0.        ,  0.        , -0.02466905,  0.        ,  0.02738859,\n",
    "         0.00769262, -0.00069768,  0.        ,  0.        , -0.00478886,\n",
    "         0.        , -2.5016083 , -2.53165845, -0.        , -0.50273435,\n",
    "         2.5609507 ,  0.        ,  0.37908086, -0.        ,  0.02956422,\n",
    "         0.31069382,  0.03959639, -0.11781918, -0.01261432, -0.00086197,\n",
    "         0.14369645,  0.33625476]),\n",
    " array([ 0.        , -0.01402041, -0.16066515, -0.00023511, -0.00068353,\n",
    "        -0.02808773, -0.0000007 , -0.0001713 ,  0.        , -0.00015679,\n",
    "        -0.00357594, -0.00006956,  0.00172591,  0.00723498,  0.00100447,\n",
    "        -0.00899814, -0.00110691,  0.00144155, -0.00003166, -0.00210364,\n",
    "        -0.        ,  0.39996888, -3.89816872,  1.20297185,  0.00522325,\n",
    "        -0.        ,  0.        , -0.02436807,  0.        ,  0.02596149,\n",
    "         0.00669651, -0.00034885,  0.        ,  0.        , -0.00397472,\n",
    "         0.00242222, -2.34231952, -2.39253258, -0.        , -0.59674153,\n",
    "         2.48981941,  0.        ,  0.38146542, -0.        ,  0.02957323,\n",
    "         0.31159563,  0.03917226, -0.11774848, -0.01266328, -0.00079551,\n",
    "         0.14264507,  0.34363547]),\n",
    " array([ 0.        , -0.01402793, -0.15658305, -0.00020642, -0.00066108,\n",
    "        -0.02808784, -0.00000067, -0.0001845 ,  0.        , -0.00015516,\n",
    "        -0.0035275 , -0.00007108,  0.00168116,  0.00727219,  0.00098391,\n",
    "        -0.0090128 , -0.00110972,  0.00141869, -0.00015041, -0.00198685,\n",
    "        -0.        ,  0.42808195, -3.83919915,  1.122458  ,  0.0051606 ,\n",
    "        -0.        ,  0.        , -0.02400371,  0.        ,  0.02459662,\n",
    "         0.00557764, -0.        ,  0.        ,  0.        , -0.00303994,\n",
    "         0.01348229, -2.18095803, -2.23835506, -0.        , -0.69740784,\n",
    "         2.40118357, -0.        ,  0.38371666, -0.        ,  0.0295852 ,\n",
    "         0.31259554,  0.03866254, -0.11766391, -0.01273698, -0.00073518,\n",
    "         0.1415261 ,  0.35150645]),\n",
    " array([-0.        , -0.01401374, -0.15249054, -0.00018535, -0.00064204,\n",
    "        -0.02792712, -0.00000062, -0.00019327,  0.        , -0.00015305,\n",
    "        -0.0033778 , -0.00007352,  0.00163548,  0.00732049,  0.00096326,\n",
    "        -0.00902119, -0.00111258,  0.00139653, -0.00020778, -0.00189097,\n",
    "        -0.00003207,  0.4458802 , -3.76719125,  1.02902605,  0.00510412,\n",
    "        -0.        ,  0.        , -0.02365254,  0.        ,  0.02333759,\n",
    "         0.00443011, -0.        ,  0.        ,  0.        , -0.00203796,\n",
    "         0.03486257, -2.01954961, -2.07574494, -0.        , -0.80563843,\n",
    "         2.31639862, -0.        ,  0.385741  , -0.        ,  0.02965238,\n",
    "         0.31390583,  0.03808246, -0.11769827, -0.01285675, -0.00067072,\n",
    "         0.14041969,  0.36091755]),\n",
    " array([-0.        , -0.01400794, -0.1481074 , -0.00017052, -0.00062296,\n",
    "        -0.02772088, -0.00000052, -0.00019824,  0.        , -0.00015072,\n",
    "        -0.00318157, -0.0000759 ,  0.00158663,  0.0073714 ,  0.00094174,\n",
    "        -0.00903261, -0.00111541,  0.00137415, -0.00027158, -0.00179888,\n",
    "        -0.00006225,  0.46850274, -3.69567084,  0.9355691 ,  0.00504689,\n",
    "        -0.        ,  0.        , -0.0232961 ,  0.        ,  0.02196582,\n",
    "         0.00321264, -0.        ,  0.        ,  0.        , -0.00097934,\n",
    "         0.09207391, -1.85567964, -1.91292587, -0.        , -0.91068866,\n",
    "         2.1909167 , -0.        ,  0.38774576, -0.        ,  0.02973188,\n",
    "         0.31522503,  0.03745394, -0.11772817, -0.01298086, -0.00059753,\n",
    "         0.1392834 ,  0.37103344]),\n",
    " array([-0.        , -0.01400115, -0.14348128, -0.00016061, -0.00060329,\n",
    "        -0.02752185, -0.00000041, -0.00019988,  0.        , -0.00014828,\n",
    "        -0.0029745 , -0.00007869,  0.00153544,  0.00741887,  0.00091978,\n",
    "        -0.00904583, -0.00111789,  0.0013514 , -0.00032038, -0.0017082 ,\n",
    "        -0.00010245,  0.48918646, -3.61869496,  0.83759553,  0.00498371,\n",
    "        -0.        ,  0.        , -0.02290635,  0.        ,  0.02042467,\n",
    "         0.00205022, -0.        ,  0.        ,  0.        , -0.        ,\n",
    "         0.17793972, -1.6779961 , -1.74694454, -0.        , -1.01645267,\n",
    "         2.03374019, -0.        ,  0.38992164, -0.        ,  0.02978416,\n",
    "         0.3167417 ,  0.03683378, -0.11776097, -0.0131087 , -0.00053081,\n",
    "         0.13815599,  0.38016007]),\n",
    " array([-0.        , -0.0139552 , -0.13912244, -0.00015506, -0.00058697,\n",
    "        -0.02729612, -0.00000024, -0.00020043, -0.        , -0.00014536,\n",
    "        -0.00275158, -0.00008173,  0.00148732,  0.00746734,  0.00089884,\n",
    "        -0.00906241, -0.00112015,  0.00132855, -0.00037303, -0.00162312,\n",
    "        -0.00012186,  0.50922815, -3.51725491,  0.72646925,  0.00490572,\n",
    "        -0.        ,  0.        , -0.02242664,  0.        ,  0.01833394,\n",
    "         0.00183764, -0.        ,  0.        ,  0.        , -0.        ,\n",
    "         0.26390848, -1.42614302, -1.57742321, -0.        , -1.12333979,\n",
    "         1.87532285, -0.        ,  0.39252104, -0.        ,  0.0296607 ,\n",
    "         0.31921264,  0.03637932, -0.11789527, -0.01321201, -0.0005582 ,\n",
    "         0.13725724,  0.37673482]),\n",
    " array([-0.        , -0.01390297, -0.13452589, -0.00015097, -0.00056997,\n",
    "        -0.02704897, -0.00000009, -0.00019891, -0.        , -0.0001422 ,\n",
    "        -0.00250737, -0.00008487,  0.00143654,  0.00752032,  0.00087657,\n",
    "        -0.00907939, -0.00112259,  0.00130436, -0.0004232 , -0.00153632,\n",
    "        -0.00014368,  0.52997255, -3.40905832,  0.60725331,  0.00482215,\n",
    "         0.        ,  0.        , -0.02191176,  0.        ,  0.01605766,\n",
    "         0.00154158,  0.        ,  0.0000743 ,  0.        , -0.        ,\n",
    "         0.35512816, -1.16212514, -1.39999936, -0.        , -1.23492388,\n",
    "         1.70774653, -0.        ,  0.39526175,  0.        ,  0.02952819,\n",
    "         0.32186283,  0.03589679, -0.11804055, -0.0133221 , -0.00058601,\n",
    "         0.1362889 ,  0.37318711]),\n",
    " array([-0.        , -0.01384509, -0.12967251, -0.00014602, -0.00055203,\n",
    "        -0.0267823 , -0.        , -0.00019567, -0.        , -0.00013879,\n",
    "        -0.00224002, -0.00008811,  0.00138284,  0.0075791 ,  0.00085277,\n",
    "        -0.00909663, -0.00112523,  0.00127852, -0.00048193, -0.00144206,\n",
    "        -0.00016265,  0.55305032, -3.29260071,  0.47740315,  0.00473236,\n",
    "         0.        ,  0.        , -0.02135796,  0.        ,  0.01355977,\n",
    "         0.00109785,  0.        ,  0.00028385,  0.        , -0.        ,\n",
    "         0.44840081, -0.88552559, -1.2129903 , -0.        , -1.351839  ,\n",
    "         1.53337369, -0.        ,  0.39816332,  0.        ,  0.02938481,\n",
    "         0.32471557,  0.03538003, -0.11819663, -0.01344037, -0.00061537,\n",
    "         0.13523256,  0.3694166 ]),\n",
    " array([-0.        , -0.01379956, -0.12454205, -0.00014015, -0.000533  ,\n",
    "        -0.02650583, -0.        , -0.0001898 , -0.        , -0.0001351 ,\n",
    "        -0.00195344, -0.00009137,  0.00132607,  0.0076419 ,  0.0008277 ,\n",
    "        -0.00911378, -0.00112823,  0.00125109, -0.00054783, -0.00133667,\n",
    "        -0.00018471,  0.57400128, -3.17256145,  0.34651135,  0.00463405,\n",
    "         0.        ,  0.        , -0.02076493,  0.00018361,  0.01076583,\n",
    "         0.00059057,  0.        ,  0.00051166,  0.        , -0.        ,\n",
    "         0.53195011, -0.58236474, -1.01041551, -0.        , -1.47698025,\n",
    "         1.35285968, -0.        ,  0.40125969,  0.        ,  0.0292239 ,\n",
    "         0.32777342,  0.0348049 , -0.11834814, -0.01356903, -0.00064736,\n",
    "         0.13402936,  0.36521303]),\n",
    " array([-0.        , -0.01378479, -0.11892741, -0.00013338, -0.00051179,\n",
    "        -0.02622732, -0.        , -0.00018371, -0.        , -0.00013111,\n",
    "        -0.00166304, -0.00009511,  0.00126392,  0.00770883,  0.00080143,\n",
    "        -0.00913175, -0.00113167,  0.00122157, -0.00061457, -0.00121342,\n",
    "        -0.00022555,  0.59864328, -3.05266193,  0.21460298,  0.00452705,\n",
    "         0.        ,  0.        , -0.02014308,  0.000701  ,  0.00755552,\n",
    "         0.        ,  0.        ,  0.00074899,  0.        , -0.        ,\n",
    "         0.60715125, -0.2400934 , -0.78602058,  0.        , -1.61234083,\n",
    "         1.15354352, -0.        ,  0.40468202,  0.        ,  0.02903841,\n",
    "         0.3310263 ,  0.03412487, -0.1184729 , -0.01371132, -0.00068557,\n",
    "         0.13259856,  0.3602317 ]),\n",
    " array([ 0.        , -0.01377929, -0.11288972, -0.00012577, -0.00048881,\n",
    "        -0.02592805, -0.        , -0.00017729, -0.        , -0.0001268 ,\n",
    "        -0.00135377, -0.00009871,  0.00119696,  0.00778054,  0.00077277,\n",
    "        -0.00915256, -0.00113532,  0.00118955, -0.00069061, -0.00107336,\n",
    "        -0.00027515,  0.62933648, -2.92458686,  0.0722559 ,  0.00441275,\n",
    "         0.        ,  0.        , -0.01947598,  0.00103573,  0.00753125,\n",
    "         0.        ,  0.        ,  0.00036834,  0.00022172,  0.        ,\n",
    "         0.68839398, -0.        , -0.54599495,  0.        , -1.75633607,\n",
    "         0.93876372, -0.        ,  0.40837391,  0.        ,  0.02884394,\n",
    "         0.33450316,  0.03339275, -0.11861437, -0.0138608 , -0.00072912,\n",
    "         0.13104865,  0.3546205 ]),\n",
    " array([ 0.        , -0.01379329, -0.10649645, -0.00011765, -0.00046408,\n",
    "        -0.02560232, -0.        , -0.00017056, -0.        , -0.00012229,\n",
    "        -0.00103438, -0.00010118,  0.00112613,  0.00787032,  0.00074127,\n",
    "        -0.00917949, -0.00113925,  0.00115378, -0.00076931, -0.00091658,\n",
    "        -0.00034061,  0.5854541 , -2.78723567,  0.        ,  0.00428976,\n",
    "         0.        ,  0.        , -0.01874578,  0.00143036,  0.01294316,\n",
    "         0.        ,  0.        ,  0.        ,  0.0003558 ,  0.        ,\n",
    "         0.77642535, -0.        , -0.30218925,  0.        , -1.90060263,\n",
    "         0.71383692, -0.        ,  0.41233831,  0.        ,  0.02864532,\n",
    "         0.33811219,  0.03262193, -0.11877716, -0.01400502, -0.0007769 ,\n",
    "         0.12934849,  0.34849429]),\n",
    " array([-0.        , -0.01368188, -0.10002595, -0.00011031, -0.00043931,\n",
    "        -0.02519468, -0.        , -0.00016285, -0.        , -0.00011723,\n",
    "        -0.00072717, -0.00010296,  0.00105351,  0.00798686,  0.00070574,\n",
    "        -0.00920458, -0.00114302,  0.00111394, -0.00085431, -0.00076063,\n",
    "        -0.0003944 ,  0.43886976, -2.62195909,  0.        ,  0.00416792,\n",
    "         0.        ,  0.        , -0.01794346,  0.00187417,  0.01749091,\n",
    "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
    "         0.86776984,  0.        , -0.09229345,  0.        , -2.04639754,\n",
    "         0.52638119, -0.        ,  0.41623626,  0.        ,  0.02844476,\n",
    "         0.3418503 ,  0.03188881, -0.1190109 , -0.01409252, -0.00082176,\n",
    "         0.12763719,  0.3426099 ]),\n",
    " array([-0.        , -0.01346325, -0.09382173, -0.00010565, -0.00041118,\n",
    "        -0.02483931, -0.        , -0.00015335, -0.        , -0.00011184,\n",
    "        -0.0004234 , -0.00010486,  0.00098145,  0.00812833,  0.00066668,\n",
    "        -0.00922634, -0.00114828,  0.00107293, -0.0009463 , -0.00061287,\n",
    "        -0.00042906,  0.27704477, -2.44326517,  0.        ,  0.00403167,\n",
    "         0.        ,  0.        , -0.01704101,  0.00201632,  0.0194358 ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
    "         0.96370906,  0.        , -0.        ,  0.        , -2.07085705,\n",
    "         0.33777064,  0.        ,  0.42101689,  0.        ,  0.02818668,\n",
    "         0.34543261,  0.03123279, -0.11918793, -0.01411729, -0.00086671,\n",
    "         0.12501178,  0.33603215]),\n",
    " array([-0.        , -0.0132085 , -0.08816092, -0.00010368, -0.00038297,\n",
    "        -0.0243989 , -0.        , -0.00014261, -0.        , -0.00010608,\n",
    "        -0.00014013, -0.00010514,  0.000913  ,  0.00830143,  0.0006246 ,\n",
    "        -0.00923684, -0.00115625,  0.00102855, -0.00104313, -0.00046016,\n",
    "        -0.00046422,  0.10431452, -2.25554556,  0.        ,  0.00387453,\n",
    "         0.        ,  0.        , -0.01601071,  0.00223113,  0.01915222,\n",
    "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
    "         1.09112792,  0.        , -0.        ,  0.        , -1.99941876,\n",
    "         0.11769689,  0.        ,  0.42588083,  0.        ,  0.02789339,\n",
    "         0.34908118,  0.03052778, -0.11933393, -0.01412286, -0.00091542,\n",
    "         0.12182289,  0.3297766 ]),\n",
    " array([-0.        , -0.01286121, -0.08274264, -0.00010362, -0.00035095,\n",
    "        -0.02405902, -0.        , -0.00013044, -0.        , -0.00010038,\n",
    "        -0.        , -0.00010474,  0.00084657,  0.00850044,  0.00057822,\n",
    "        -0.00923565, -0.00116367,  0.00098323, -0.00115681, -0.00028353,\n",
    "        -0.00052399,  0.        , -2.1089563 ,  0.        ,  0.00372608,\n",
    "         0.        ,  0.        , -0.01499553,  0.00169624,  0.01866938,\n",
    "         0.        ,  0.        ,  0.        ,  0.        , -0.        ,\n",
    "         1.16480401,  0.        , -0.        ,  0.        , -1.93037318,\n",
    "         0.        ,  0.        ,  0.43114684,  0.        ,  0.02769011,\n",
    "         0.35231643,  0.02994572, -0.11958867, -0.01414191, -0.00096003,\n",
    "         0.11856132,  0.32179384]),\n",
    " array([-0.        , -0.0125132 , -0.07782179, -0.00010424, -0.00031603,\n",
    "        -0.023844  , -0.        , -0.00011825, -0.        , -0.00009441,\n",
    "        -0.        , -0.00010368,  0.00078319,  0.00871106,  0.00053215,\n",
    "        -0.0092264 , -0.00117249,  0.00094052, -0.00131295, -0.00015969,\n",
    "        -0.00051952,  0.        , -2.0425045 ,  0.        ,  0.00356859,\n",
    "         0.        ,  0.        , -0.01395391,  0.00182607,  0.01825189,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         1.09495088,  0.        , -0.        ,  0.        , -1.86748872,\n",
    "         0.        ,  0.        ,  0.43626034,  0.        ,  0.02751565,\n",
    "         0.3543658 ,  0.02914796, -0.11971544, -0.01407096, -0.000989  ,\n",
    "         0.11464202,  0.3124493 ]),\n",
    " array([-0.        , -0.01215054, -0.07275501, -0.0001053 , -0.00028042,\n",
    "        -0.0236767 , -0.        , -0.00010561, -0.        , -0.00008833,\n",
    "        -0.        , -0.00010338,  0.00071827,  0.00892364,  0.00048655,\n",
    "        -0.00921279, -0.00118101,  0.00089851, -0.00146504, -0.00006359,\n",
    "        -0.00048717,  0.        , -1.97959558,  0.        ,  0.00339904,\n",
    "         0.        ,  0.        , -0.01284788,  0.00201529,  0.01784071,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         1.02162957,  0.        , -0.        ,  0.        , -1.80502047,\n",
    "         0.        ,  0.        ,  0.44147417,  0.        ,  0.02731758,\n",
    "         0.35644598,  0.02831794, -0.11980035, -0.01399996, -0.00101002,\n",
    "         0.11055211,  0.30304432]),\n",
    " array([-0.        , -0.01176552, -0.06737082, -0.00010425, -0.00024192,\n",
    "        -0.02359514, -0.        , -0.00009388, -0.        , -0.00008213,\n",
    "        -0.        , -0.00010377,  0.0006498 ,  0.0091374 ,  0.00043998,\n",
    "        -0.00920025, -0.00118949,  0.0008562 , -0.00161683, -0.        ,\n",
    "        -0.00041961,  0.        , -1.9180159 ,  0.        ,  0.00321727,\n",
    "         0.        ,  0.        , -0.0116679 ,  0.0022032 ,  0.01742542,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.94759692,  0.        , -0.        ,  0.        , -1.74244418,\n",
    "         0.        ,  0.        ,  0.44699511,  0.        ,  0.02709463,\n",
    "         0.35855111,  0.02748929, -0.11985501, -0.01391983, -0.00102295,\n",
    "         0.10625704,  0.29333924]),\n",
    " array([-0.        , -0.0113284 , -0.06160168, -0.00010023, -0.00019985,\n",
    "        -0.02354492, -0.        , -0.00008355, -0.        , -0.00007535,\n",
    "        -0.        , -0.00010459,  0.00057632,  0.00937606,  0.00039029,\n",
    "        -0.00919036, -0.00119925,  0.00081126, -0.00177498, -0.        ,\n",
    "        -0.00028124,  0.        , -1.85763952,  0.        ,  0.00302677,\n",
    "         0.        ,  0.        , -0.01042664,  0.00240751,  0.01700654,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.86888671,  0.        , -0.        ,  0.        , -1.67661094,\n",
    "         0.        ,  0.        ,  0.45291578,  0.        ,  0.02686356,\n",
    "         0.36072509,  0.02657587, -0.11990335, -0.01383355, -0.0010262 ,\n",
    "         0.10165815,  0.28344025]),\n",
    " array([-0.        , -0.01086116, -0.05539719, -0.00009296, -0.00015439,\n",
    "        -0.02349896, -0.        , -0.00007461, -0.        , -0.0000681 ,\n",
    "        -0.        , -0.0001055 ,  0.00049728,  0.0096318 ,  0.00033696,\n",
    "        -0.00918022, -0.0012098 ,  0.00076291, -0.00192563, -0.        ,\n",
    "        -0.00015222,  0.        , -1.79237121,  0.        ,  0.00282244,\n",
    "         0.        ,  0.        , -0.00909507,  0.00262583,  0.01655819,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.78407276,  0.        , -0.        ,  0.        , -1.60546256,\n",
    "         0.        ,  0.        ,  0.45928406,  0.        ,  0.02661571,\n",
    "         0.36305198,  0.02559544, -0.11995492, -0.01374064, -0.00103037,\n",
    "         0.09671922,  0.27276072]),\n",
    " array([-0.        , -0.01036233, -0.0487137 , -0.00008191, -0.00010511,\n",
    "        -0.02346276, -0.        , -0.00006732, -0.        , -0.00006036,\n",
    "        -0.        , -0.00010652,  0.00041214,  0.00990495,  0.00027968,\n",
    "        -0.00917015, -0.00122121,  0.00071089, -0.00206591, -0.        ,\n",
    "        -0.00003561,  0.        , -1.72174926,  0.        ,  0.00260311,\n",
    "         0.        ,  0.        , -0.0076656 ,  0.00285837,  0.01607892,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.69281231,  0.        , -0.        ,  0.        , -1.52866421,\n",
    "         0.        ,  0.        ,  0.46614636,  0.        ,  0.02634931,\n",
    "         0.36554036,  0.02454612, -0.12000957, -0.01363992, -0.00103556,\n",
    "         0.09141326,  0.26122996]),\n",
    " array([-0.        , -0.00983112, -0.04150564, -0.00006761, -0.00005158,\n",
    "        -0.02343783, -0.        , -0.00006118, -0.        , -0.00005209,\n",
    "        -0.        , -0.00010767,  0.0003203 ,  0.01019607,  0.00021811,\n",
    "        -0.00916039, -0.00123354,  0.00065499, -0.00212831, -0.        ,\n",
    "        -0.        , -0.        , -1.64504061, -0.        ,  0.00236806,\n",
    "         0.        ,  0.        , -0.00613266,  0.00310581,  0.01556717,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.59477372,  0.        , -0.        ,  0.        , -1.44595622,\n",
    "         0.        ,  0.        ,  0.47354309,  0.        ,  0.02606436,\n",
    "         0.36818963,  0.02341971, -0.12006675, -0.01352952, -0.00104205,\n",
    "         0.08570559,  0.24882768]),\n",
    " array([-0.        , -0.00918647, -0.03455571, -0.00007428, -0.00000614,\n",
    "        -0.02294376, -0.        , -0.00004065, -0.        , -0.00004182,\n",
    "        -0.        , -0.00010727,  0.00022909,  0.01059891,  0.0001508 ,\n",
    "        -0.00912787, -0.00124842,  0.00059231, -0.00215698, -0.        ,\n",
    "        -0.        , -0.        , -1.56695273, -0.        ,  0.00213111,\n",
    "         0.        ,  0.        , -0.00457893,  0.00345072,  0.01474908,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.47728917,  0.00938176, -0.        ,  0.        , -1.34908549,\n",
    "         0.        ,  0.        ,  0.48057813,  0.        ,  0.0258194 ,\n",
    "         0.37132481,  0.0219058 , -0.12020264, -0.01346732, -0.00105023,\n",
    "         0.07964361,  0.23642879]),\n",
    " array([-0.        , -0.00839251, -0.02995849, -0.00007708, -0.        ,\n",
    "        -0.02166226, -0.        , -0.00003128, -0.        , -0.00002971,\n",
    "        -0.        , -0.00010513,  0.00016458,  0.01122783,  0.00009315,\n",
    "        -0.00903063, -0.00126074,  0.00053865, -0.00215006, -0.        ,\n",
    "        -0.        , -0.        , -1.51591158, -0.        ,  0.00190626,\n",
    "         0.        ,  0.        , -0.00313058,  0.00372803,  0.01302874,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.34653869,  0.05077188, -0.        ,  0.        , -1.23732083,\n",
    "         0.        ,  0.        ,  0.48446402,  0.        ,  0.02557129,\n",
    "         0.37500068,  0.02014078, -0.12031994, -0.01353797, -0.00102982,\n",
    "         0.07432603,  0.22415734]),\n",
    " array([-0.        , -0.00749675, -0.02532932, -0.00009716, -0.        ,\n",
    "        -0.02005405, -0.        , -0.00000969, -0.        , -0.00001574,\n",
    "        -0.        , -0.00010134,  0.00009821,  0.01196167,  0.00002953,\n",
    "        -0.00891857, -0.00127481,  0.00047979, -0.00214079, -0.        ,\n",
    "        -0.        , -0.        , -1.4635749 , -0.        ,  0.00167362,\n",
    "         0.        ,  0.        , -0.00161764,  0.00401332,  0.01127147,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.20368836,  0.08919127, -0.        ,  0.        , -1.11436367,\n",
    "         0.        ,  0.        ,  0.4882196 ,  0.        ,  0.02533076,\n",
    "         0.37913569,  0.0181589 , -0.12050521, -0.01363506, -0.00100522,\n",
    "         0.06870633,  0.21120159]),\n",
    " array([-0.        , -0.00670442, -0.02023363, -0.00010241, -0.        ,\n",
    "        -0.01854519, -0.        , -0.00000192, -0.        , -0.00000187,\n",
    "        -0.        , -0.00010754,  0.00002518,  0.01271964,  0.        ,\n",
    "        -0.00880763, -0.00128741,  0.00042178, -0.00213235, -0.        ,\n",
    "        -0.        , -0.        , -1.40803663, -0.        ,  0.00141909,\n",
    "         0.        ,  0.        , -0.00009307,  0.00437458,  0.00923392,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.04997988,  0.14407451, -0.        ,  0.        , -0.98480419,\n",
    "         0.        , -0.        ,  0.48990688,  0.        ,  0.02508223,\n",
    "         0.38323813,  0.0158353 , -0.12045257, -0.01389594, -0.00098189,\n",
    "         0.06269497,  0.19837018]),\n",
    " array([-0.        , -0.00622583, -0.01802149, -0.00009696, -0.        ,\n",
    "        -0.01734536, -0.        , -0.00000726, -0.        , -0.        ,\n",
    "        -0.        , -0.000116  ,  0.        ,  0.01315218,  0.        ,\n",
    "        -0.00875608, -0.00128336,  0.00036028, -0.00211071, -0.        ,\n",
    "        -0.        , -0.        , -1.37419325, -0.        ,  0.001407  ,\n",
    "         0.        ,  0.        , -0.        ,  0.00366488,  0.00819407,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        ,  0.11021827, -0.        ,  0.        , -0.90062995,\n",
    "         0.        , -0.        ,  0.49044568,  0.        ,  0.02456806,\n",
    "         0.38566826,  0.01375348, -0.11958148, -0.01406929, -0.00095984,\n",
    "         0.05765355,  0.18337841]),\n",
    " array([-0.        , -0.00568012, -0.01765311, -0.00009588, -0.        ,\n",
    "        -0.01623496, -0.        , -0.00000954, -0.        , -0.        ,\n",
    "        -0.        , -0.00012408,  0.        ,  0.01328757,  0.        ,\n",
    "        -0.00868562, -0.00128419,  0.00029983, -0.00208908, -0.        ,\n",
    "        -0.        , -0.        , -1.35387715, -0.        ,  0.00141195,\n",
    "         0.        ,  0.        , -0.        ,  0.00250634,  0.00743936,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        ,  0.04348768, -0.        ,  0.        , -0.843178  ,\n",
    "         0.        , -0.        ,  0.49054129,  0.        ,  0.02401063,\n",
    "         0.38753352,  0.01146383, -0.1183937 , -0.01423225, -0.00093716,\n",
    "         0.05307069,  0.17185224]),\n",
    " array([-0.        , -0.0050039 , -0.01713806, -0.00008657, -0.        ,\n",
    "        -0.01500155, -0.        , -0.00001784, -0.        , -0.        ,\n",
    "        -0.        , -0.00013478,  0.        ,  0.01342395,  0.        ,\n",
    "        -0.00860968, -0.00128619,  0.00022956, -0.0020131 , -0.        ,\n",
    "        -0.        , -0.        , -1.20576153, -0.17721775,  0.00141812,\n",
    "         0.        ,  0.        , -0.        ,  0.00117382,  0.00598114,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        ,  0.        , -0.        ,  0.        , -0.77703016,\n",
    "         0.        , -0.        ,  0.49101265,  0.        ,  0.02325165,\n",
    "         0.39053445,  0.00902584, -0.11693741, -0.01442426, -0.00092807,\n",
    "         0.04813302,  0.16241428]),\n",
    " array([-0.        , -0.0043188 , -0.01650904, -0.00009296, -0.        ,\n",
    "        -0.01382641, -0.        , -0.00001416, -0.        , -0.        ,\n",
    "        -0.        , -0.00014506,  0.        ,  0.01352399,  0.        ,\n",
    "        -0.00852673, -0.0012873 ,  0.00015614, -0.00191129, -0.        ,\n",
    "        -0.        , -0.        , -1.02204308, -0.41525446,  0.00142476,\n",
    "         0.        ,  0.        , -0.        ,  0.        ,  0.00331566,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        ,  0.        , -0.        ,  0.        , -0.71926475,\n",
    "         0.        , -0.        ,  0.49162273,  0.        ,  0.02236216,\n",
    "         0.39418089,  0.00667267, -0.11536821, -0.01461214, -0.00091552,\n",
    "         0.04298956,  0.15309842]),\n",
    " array([-0.        , -0.00396997, -0.01590118, -0.000111  , -0.        ,\n",
    "        -0.01348484, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00014623,  0.        ,  0.01343622,  0.        ,\n",
    "        -0.0084191 , -0.00128211,  0.00009742, -0.00179895, -0.        ,\n",
    "        -0.        , -0.        , -0.92320626, -0.59026149,  0.00143212,\n",
    "         0.        ,  0.        , -0.        ,  0.        ,  0.00009678,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        ,  0.        , -0.        ,  0.        , -0.71916978,\n",
    "         0.        , -0.        ,  0.49162442,  0.        ,  0.02134468,\n",
    "         0.39854809,  0.0054856 , -0.1140508 , -0.01472556, -0.00087061,\n",
    "         0.03846771,  0.14064127]),\n",
    " array([-0.        , -0.00360955, -0.01527499, -0.00010959, -0.        ,\n",
    "        -0.01319167, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00014728,  0.        ,  0.01333761,  0.        ,\n",
    "        -0.00829935, -0.00127657,  0.00003555, -0.00167683, -0.        ,\n",
    "        -0.        , -0.        , -0.81554217, -0.78018319,  0.00144041,\n",
    "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        ,  0.        , -0.        ,  0.        , -0.71835563,\n",
    "         0.        , -0.        ,  0.4915463 ,  0.        ,  0.02025669,\n",
    "         0.40325139,  0.00428508, -0.11266669, -0.01484783, -0.00082051,\n",
    "         0.03371036,  0.12688734]),\n",
    " array([ 0.        , -0.00318   , -0.01499875, -0.0001073 , -0.        ,\n",
    "        -0.01294168, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00015551,  0.        ,  0.0131665 ,  0.        ,\n",
    "        -0.00815912, -0.00126955,  0.        , -0.00158408, -0.        ,\n",
    "        -0.        , -0.        , -0.72410863, -0.95223639,  0.00144318,\n",
    "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        ,  0.        , -0.        , -0.        , -0.72073752,\n",
    "        -0.        , -0.        ,  0.49121219,  0.        ,  0.01911392,\n",
    "         0.40756074,  0.00286817, -0.11101651, -0.01497408, -0.00076843,\n",
    "         0.02842501,  0.11419747]),\n",
    " array([ 0.        , -0.00265811, -0.01519093, -0.00010385, -0.        ,\n",
    "        -0.01275568, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.0001732 ,  0.        ,  0.01290064,  0.        ,\n",
    "        -0.00799576, -0.0012607 ,  0.        , -0.00153034, -0.        ,\n",
    "        -0.        , -0.        , -0.65532896, -1.09896585,  0.00143864,\n",
    "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
    "        -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.72747806,\n",
    "        -0.        , -0.        ,  0.49055386,  0.        ,  0.01791022,\n",
    "         0.41126402,  0.00120201, -0.10903248, -0.01509886, -0.00071429,\n",
    "         0.02253847,  0.10307112]),\n",
    " array([ 0.        , -0.00194195, -0.01542926, -0.00009989, -0.        ,\n",
    "        -0.01260586, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00019077,  0.        ,  0.01261579,  0.        ,\n",
    "        -0.00784885, -0.00125123,  0.        , -0.00146382, -0.        ,\n",
    "        -0.        , -0.12145293, -0.58874999, -1.12625221,  0.00143026,\n",
    "         0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.73449028,\n",
    "        -0.        , -0.        ,  0.48974881,  0.        ,  0.01662812,\n",
    "         0.41554722,  0.        , -0.10716346, -0.01518504, -0.00064657,\n",
    "         0.01673683,  0.0903231 ]),\n",
    " array([ 0.        , -0.0008661 , -0.01565462, -0.00009513, -0.        ,\n",
    "        -0.01257639, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00020653,  0.        ,  0.01231846,  0.        ,\n",
    "        -0.00775775, -0.00124213,  0.        , -0.00136575, -0.        ,\n",
    "        -0.        , -0.34607965, -0.4960017 , -1.08433549,  0.00141471,\n",
    "         0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.74177176,\n",
    "        -0.        , -0.        ,  0.48873434,  0.        ,  0.01524682,\n",
    "         0.42068783,  0.        , -0.10562279, -0.01516729, -0.00055238,\n",
    "         0.01164902,  0.0757597 ]),\n",
    " array([ 0.        , -0.        , -0.01590655, -0.00009035, -0.        ,\n",
    "        -0.01263603, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00021929,  0.        ,  0.01202222,  0.        ,\n",
    "        -0.00766392, -0.00123439,  0.        , -0.00127015, -0.        ,\n",
    "        -0.        , -0.60412199, -0.40405163, -1.01248063,  0.00140142,\n",
    "         0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.74984221,\n",
    "        -0.        , -0.        ,  0.48715078,  0.        ,  0.01375788,\n",
    "         0.42601279,  0.        , -0.10398738, -0.01508398, -0.00045668,\n",
    "         0.00615783,  0.06003585]),\n",
    " array([ 0.        , -0.        , -0.01626718, -0.00008635, -0.        ,\n",
    "        -0.01298704, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00022042,  0.        ,  0.01177922,  0.        ,\n",
    "        -0.00757603, -0.00123201,  0.        , -0.00119508, -0.        ,\n",
    "        -0.        , -0.84887042, -0.32905766, -0.93566256,  0.00139807,\n",
    "         0.        ,  0.        , -0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.75891988,\n",
    "        -0.        , -0.        ,  0.48391726,  0.        ,  0.01213934,\n",
    "         0.43097985,  0.        , -0.10221705, -0.01482277, -0.00037365,\n",
    "         0.00016962,  0.04270191]),\n",
    " array([ 0.        , -0.        , -0.01665497, -0.00008207, -0.        ,\n",
    "        -0.01336177, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00022162,  0.        ,  0.01151926, -0.        ,\n",
    "        -0.00748178, -0.00122951,  0.        , -0.00111489, -0.        ,\n",
    "        -0.        , -1.09749893, -0.24841089, -0.86691753,  0.00139473,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.7686633 ,\n",
    "        -0.        , -0.        ,  0.4804859 ,  0.        ,  0.01040511,\n",
    "         0.43626187,  0.        , -0.10031381, -0.01453982, -0.00028581,\n",
    "         0.        ,  0.02407635]),\n",
    " array([ 0.        , -0.        , -0.01707298, -0.00007747, -0.        ,\n",
    "        -0.01376268, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00022291,  0.        ,  0.01124052, -0.        ,\n",
    "        -0.00738081, -0.00122682,  0.        , -0.00102893, -0.        ,\n",
    "        -0.        , -1.34007327, -0.16174771, -0.81740636,  0.00139118,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.77911591,\n",
    "        -0.        , -0.        ,  0.47680534,  0.        ,  0.0085463 ,\n",
    "         0.44192516,  0.        , -0.09827449, -0.01423608, -0.00019175,\n",
    "         0.        ,  0.00412988]),\n",
    " array([ 0.        , -0.        , -0.01751046, -0.0000725 , -0.        ,\n",
    "        -0.01419651, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00022422,  0.        ,  0.0109363 , -0.        ,\n",
    "        -0.00726991, -0.00122361,  0.        , -0.00093703, -0.        ,\n",
    "        -0.        , -1.58004441, -0.05915237, -0.79287421,  0.0013878 ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.79046981,\n",
    "        -0.        , -0.        ,  0.47278974,  0.        ,  0.00661057,\n",
    "         0.44785787,  0.        , -0.09615457, -0.01390246, -0.00010211,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.01793878, -0.00006721, -0.        ,\n",
    "        -0.01468342, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00022491,  0.        ,  0.01061327, -0.        ,\n",
    "        -0.00715218, -0.00121972,  0.        , -0.00084877, -0.        ,\n",
    "        -0.        , -0.47206301, -0.        , -2.07466935,  0.00138369,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.80241767,\n",
    "        -0.        , -0.        ,  0.46852212,  0.        ,  0.00451088,\n",
    "         0.45445093,  0.        , -0.09394847, -0.01353497, -0.00001207,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.01859851, -0.00006168, -0.        ,\n",
    "        -0.01514751, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00022501,  0.        ,  0.01026839, -0.        ,\n",
    "        -0.00702921, -0.00121436,  0.        , -0.00076781, -0.        ,\n",
    "        -0.        , -0.56318496, -0.        , -2.10931647,  0.00137957,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.81434861,\n",
    "        -0.        , -0.        ,  0.46349278,  0.        ,  0.00228089,\n",
    "         0.46147845,  0.        , -0.09165541, -0.01314562, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.01930305, -0.00005576, -0.        ,\n",
    "        -0.01564186, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.000225  ,  0.        ,  0.00989813, -0.        ,\n",
    "        -0.00689625, -0.00120843,  0.        , -0.0006832 , -0.        ,\n",
    "        -0.        , -0.60117783, -0.        , -2.20295667,  0.0013752 ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.82703333,\n",
    "        -0.        , -0.        ,  0.4581278 ,  0.        ,  0.        ,\n",
    "         0.46839022,  0.        , -0.08928024, -0.01272976, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.01987225, -0.00004978, -0.        ,\n",
    "        -0.01618195, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00022216,  0.        ,  0.00949794, -0.        ,\n",
    "        -0.00672016, -0.0011993 ,  0.        , -0.00065132, -0.        ,\n",
    "        -0.        , -0.65014302, -0.        , -2.18093307,  0.0013677 ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.83982492,\n",
    "        -0.        , -0.        ,  0.45345477,  0.        ,  0.        ,\n",
    "         0.46185505,  0.        , -0.08867525, -0.01229219, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.02048225, -0.00004337, -0.        ,\n",
    "        -0.01676074, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00021911,  0.        ,  0.00906907, -0.        ,\n",
    "        -0.00653144, -0.00118953, -0.        , -0.00061715, -0.        ,\n",
    "        -0.        , -0.70372532, -0.        , -2.15622396,  0.00135966,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.85353324,\n",
    "        -0.        , -0.        ,  0.44844682,  0.        ,  0.        ,\n",
    "         0.4548515 ,  0.        , -0.08802688, -0.01182327, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.02113596, -0.0000365 , -0.        ,\n",
    "        -0.01738102, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00021585,  0.        ,  0.00860946, -0.        ,\n",
    "        -0.0063292 , -0.00117905, -0.        , -0.00058053, -0.        ,\n",
    "        -0.        , -0.76113389, -0.        , -2.12975785,  0.00135105,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.868224  ,\n",
    "        -0.        , -0.        ,  0.44307996,  0.        ,  0.        ,\n",
    "         0.44734601,  0.        , -0.08733205, -0.01132075, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.02183652, -0.00002914, -0.        ,\n",
    "        -0.01804574, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00021235, -0.        ,  0.00811691, -0.        ,\n",
    "        -0.00611246, -0.00116782, -0.        , -0.0005413 , -0.        ,\n",
    "        -0.        , -0.82277038, -0.        , -2.10128147,  0.00134182,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.88396761,\n",
    "        -0.        , -0.        ,  0.43732848,  0.        ,  0.        ,\n",
    "         0.43930263, -0.        , -0.08658742, -0.01078221, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.02258729, -0.00002125, -0.        ,\n",
    "        -0.01875811, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.0002086 , -0.        ,  0.00758907, -0.        ,\n",
    "        -0.0058802 , -0.00115579, -0.        , -0.00049924, -0.        ,\n",
    "        -0.        , -0.88888781, -0.        , -2.07070069,  0.00133192,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.90083953,\n",
    "        -0.        , -0.        ,  0.4311648 ,  0.        ,  0.        ,\n",
    "         0.43068279, -0.        , -0.08578943, -0.01020507, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.02339187, -0.0000128 , -0.        ,\n",
    "        -0.01952153, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00020458, -0.        ,  0.0070234 , -0.        ,\n",
    "        -0.00563128, -0.00114289, -0.        , -0.00045418, -0.        ,\n",
    "        -0.        , -0.95892496, -0.        , -2.03874645,  0.00132132,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.91892063,\n",
    "        -0.        , -0.        ,  0.42455939, -0.        ,  0.        ,\n",
    "         0.42144521, -0.        , -0.08493425, -0.00958657, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        , -0.        , -0.02425411, -0.00000373, -0.        ,\n",
    "        -0.02033967, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00020028, -0.        ,  0.00641718, -0.        ,\n",
    "        -0.00536453, -0.00112907, -0.        , -0.00040589, -0.        ,\n",
    "        -0.        , -1.03383725, -0.        , -2.0046463 ,  0.00130996,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.93829756,\n",
    "        -0.        , -0.        ,  0.41748058, -0.        ,  0.        ,\n",
    "         0.41154559, -0.        , -0.08401778, -0.00892374, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.02506615, -0.        , -0.        ,\n",
    "        -0.02103634, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00019703, -0.        ,  0.00588024, -0.        ,\n",
    "        -0.00509084, -0.00111989, -0.        , -0.0003684 , -0.        ,\n",
    "        -0.        , -1.09824751, -0.        , -1.96582817,  0.00129174,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.95360312,\n",
    "        -0.        , -0.        ,  0.40976208, -0.        ,  0.        ,\n",
    "         0.40084717, -0.        , -0.08288212, -0.00836131, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.02586138, -0.        , -0.        ,\n",
    "        -0.02166231, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00019446, -0.        ,  0.00538032, -0.        ,\n",
    "        -0.0048057 , -0.00111381, -0.        , -0.00033778, -0.        ,\n",
    "        -0.        , -1.15666686, -0.        , -1.92268038,  0.00126815,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.96634807,\n",
    "        -0.        , -0.        ,  0.40140173, -0.        ,  0.        ,\n",
    "         0.38932218, -0.        , -0.08156225, -0.00785764, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.0267136 , -0.        , -0.        ,\n",
    "        -0.02233315, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.0001917 , -0.        ,  0.00484458, -0.        ,\n",
    "        -0.00450013, -0.0011073 , -0.        , -0.00030497, -0.        ,\n",
    "        -0.        , -1.21924729, -0.        , -1.87646597,  0.00124288,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.98000641,\n",
    "        -0.        , -0.        ,  0.39244221, -0.        ,  0.        ,\n",
    "         0.37697123, -0.        , -0.08014778, -0.00731787, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.0276269 , -0.        , -0.        ,\n",
    "        -0.02305205, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00018875, -0.        ,  0.00427044, -0.        ,\n",
    "        -0.00417266, -0.00110033, -0.        , -0.00026981, -0.        ,\n",
    "        -0.        , -1.2862089 , -0.        , -1.82704323,  0.0012158 ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.99464362,\n",
    "        -0.        , -0.        ,  0.38284058, -0.        ,  0.        ,\n",
    "         0.36373511, -0.        , -0.07863194, -0.00673942, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.02860565, -0.        , -0.        ,\n",
    "        -0.02382249, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.00018559, -0.        ,  0.00365516, -0.        ,\n",
    "        -0.00382171, -0.00109285, -0.        , -0.00023213, -0.        ,\n",
    "        -0.        , -1.35804913, -0.        , -1.77399888,  0.00118678,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.01032984,\n",
    "        -0.        , -0.        ,  0.37255083, -0.        ,  0.        ,\n",
    "         0.3495504 , -0.        , -0.07700746, -0.00611952, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.02965455, -0.        , -0.        ,\n",
    "        -0.02464813, -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.0001822 , -0.        ,  0.00299577, -0.        ,\n",
    "        -0.00344562, -0.00108484, -0.        , -0.00019175, -0.        ,\n",
    "        -0.        , -1.43497437, -0.        , -1.71721655,  0.00115567,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.02714025,\n",
    "        -0.        , -0.        ,  0.36152363, -0.        ,  0.        ,\n",
    "         0.33434909, -0.        , -0.07526656, -0.00545518, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.03089208, -0.        , -0.        ,\n",
    "        -0.02548011, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017875, -0.        ,  0.00229402, -0.        ,\n",
    "        -0.00303488, -0.00107689, -0.        , -0.00015112, -0.        ,\n",
    "        -0.        , -2.81250168, -0.        , -0.36072077,  0.00112122,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.04422405,\n",
    "        -0.        , -0.        ,  0.34952495, -0.        ,  0.        ,\n",
    "         0.31838288, -0.        , -0.07343828, -0.00476731, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.03212857, -0.        , -0.        ,\n",
    "        -0.02644443, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017486, -0.        ,  0.00152728, -0.        ,\n",
    "        -0.00260583, -0.00106676, -0.        , -0.00010334, -0.        ,\n",
    "        -0.        , -2.83640957, -0.        , -0.36116389,  0.00108632,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.06431601,\n",
    "        -0.        , -0.        ,  0.33681286, -0.        ,  0.        ,\n",
    "         0.30074192, -0.        , -0.07141734, -0.00398803, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.03342773, -0.        , -0.        ,\n",
    "        -0.02746398, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017067, -0.        ,  0.00071379, -0.        ,\n",
    "        -0.00214412, -0.00105667, -0.        , -0.00005349, -0.        ,\n",
    "        -0.        , -2.87089927, -0.        , -0.3516442 ,  0.00104826,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.08520785,\n",
    "        -0.        , -0.        ,  0.32322441, -0.        ,  0.        ,\n",
    "         0.28197859, -0.        , -0.06926837, -0.00316585, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.0346825 , -0.        , -0.        ,\n",
    "        -0.02836681,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00016726, -0.        ,  0.        , -0.        ,\n",
    "        -0.00172723, -0.00105057, -0.        , -0.00001689, -0.        ,\n",
    "        -0.        , -2.90075583, -0.        , -0.32881004,  0.00100576,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.10273522,\n",
    "        -0.        , -0.        ,  0.30854284, -0.        ,  0.        ,\n",
    "         0.26085387, -0.        , -0.0666552 , -0.00240572, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.03515257, -0.        , -0.        ,\n",
    "        -0.02847916,  0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00016834, -0.        ,  0.        , -0.        ,\n",
    "        -0.00165122, -0.00106635, -0.        , -0.00005876, -0.        ,\n",
    "        -0.        , -2.8526783 , -0.        , -0.28923722,  0.00095002,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.09750426,\n",
    "        -0.        , -0.        ,  0.2926832 , -0.        ,  0.        ,\n",
    "         0.23405448, -0.        , -0.06246978, -0.00218821, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.03602609, -0.        , -0.        ,\n",
    "        -0.02848614, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00016917, -0.        ,  0.        , -0.        ,\n",
    "        -0.00157406, -0.00108337, -0.        , -0.00011621, -0.        ,\n",
    "        -0.        , -2.7545713 , -0.        , -0.28919373,  0.00089172,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.09256186,\n",
    "        -0.        , -0.        ,  0.27475472, -0.        ,  0.        ,\n",
    "         0.20487777, -0.        , -0.05794735, -0.00192251, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.03693323, -0.        , -0.        ,\n",
    "        -0.02850562, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017033, -0.        ,  0.        , -0.        ,\n",
    "        -0.00148949, -0.00110121, -0.        , -0.00017471, -0.        ,\n",
    "        -0.        , -2.66295843, -0.        , -0.27662464,  0.00082853,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.08687538,\n",
    "        -0.        , -0.        ,  0.25572771, -0.        ,  0.        ,\n",
    "         0.17386406, -0.        , -0.05312252, -0.00165744, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.03790943, -0.        , -0.        ,\n",
    "        -0.02852535, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017157, -0.        ,  0.        , -0.        ,\n",
    "        -0.00139876, -0.0011203 , -0.        , -0.00023748, -0.        ,\n",
    "        -0.        , -2.56515354, -0.        , -0.26276211,  0.00076078,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.08076247,\n",
    "        -0.        , -0.        ,  0.23533397, -0.        ,  0.        ,\n",
    "         0.1406423 , -0.        , -0.0479537 , -0.00137415, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.038961  , -0.        , -0.        ,\n",
    "        -0.02854443, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017287, -0.        ,  0.        , -0.        ,\n",
    "        -0.00130177, -0.00114081, -0.        , -0.00030518, -0.        ,\n",
    "        -0.        , -2.45702416, -0.        , -0.2510741 ,  0.00068828,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.07426526,\n",
    "        -0.        , -0.        ,  0.21345018, -0.        ,  0.        ,\n",
    "         0.10500343, -0.        , -0.04241129, -0.00106785, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.0400772 , -0.        , -0.        ,\n",
    "        -0.02856783, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017426, -0.        ,  0.        , -0.        ,\n",
    "        -0.00119742, -0.00116279, -0.        , -0.00037752, -0.        ,\n",
    "        -0.        , -2.34190171, -0.        , -0.2378746 ,  0.00061049,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.06726047,\n",
    "        -0.        , -0.        ,  0.19002616, -0.        ,  0.        ,\n",
    "         0.06684552, -0.        , -0.03647634, -0.00074095, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.04128188, -0.        , -0.        ,\n",
    "        -0.02858998, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017571, -0.        ,  0.        , -0.        ,\n",
    "        -0.00108568, -0.00118636, -0.        , -0.00045553, -0.        ,\n",
    "        -0.        , -2.21782124, -0.        , -0.22427934,  0.00052717,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.05977988,\n",
    "        -0.        , -0.        ,  0.1648956 , -0.        ,  0.        ,\n",
    "         0.02593937, -0.        , -0.03011573, -0.00038899, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.04242529, -0.        , -0.        ,\n",
    "        -0.02839343, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00017403, -0.        ,  0.        , -0.        ,\n",
    "        -0.00089732, -0.0012081 , -0.        , -0.00055725, -0.        ,\n",
    "        -0.        , -2.06324578, -0.        , -0.21240259,  0.00041934,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.04020011,\n",
    "        -0.        , -0.        ,  0.14095213, -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.02583197, -0.00015405, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.04340635, -0.        , -0.        ,\n",
    "        -0.0278486 , -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00016729, -0.        ,  0.        , -0.        ,\n",
    "        -0.00058728, -0.00122577, -0.        , -0.00069332, -0.        ,\n",
    "        -0.        , -1.86666175, -0.        , -0.20180785,  0.00027463,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -1.00109814,\n",
    "        -0.        , -0.        ,  0.11999601, -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.02517017, -0.00013329, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.04445736, -0.        , -0.        ,\n",
    "        -0.02726492, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00016007, -0.        ,  0.        , -0.        ,\n",
    "        -0.00025504, -0.00124471, -0.        , -0.00083912, -0.        ,\n",
    "        -0.        , -1.66588013, -0.        , -0.1805738 ,  0.00011955,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.95919309,\n",
    "        -0.        , -0.        ,  0.09753922, -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.02446069, -0.00011116, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.0455745 , -0.        , -0.        ,\n",
    "        -0.02663729, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00015068, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00125746, -0.        , -0.00099947, -0.        ,\n",
    "        -0.        , -1.49034543, -0.        , -0.11488273,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.91776812,\n",
    "        -0.        , -0.        ,  0.07391133, -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.02368166, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.0467916 , -0.        , -0.        ,\n",
    "        -0.02597882, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.0001366 , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00124873, -0.        , -0.00116358, -0.        ,\n",
    "        -0.        , -1.29381931, -0.        , -0.05059894,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.87966207,\n",
    "        -0.        , -0.        ,  0.05090947, -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.02248   , -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.0480825 , -0.        , -0.        ,\n",
    "        -0.02527657, -0.        , -0.        ,  0.        , -0.        ,\n",
    "         0.        , -0.00012153, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00123941, -0.        , -0.00133848, -0.        ,\n",
    "        -0.        , -1.06629024, -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.83879009,\n",
    "        -0.        , -0.        ,  0.0262919 , -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.02119611, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.04945974, -0.        , -0.        ,\n",
    "        -0.02452598, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00010537, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00122942, -0.        , -0.00152627, -0.        ,\n",
    "        -0.        , -0.76739999, -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.79500972,\n",
    "        -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.01981962, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([ 0.        ,  0.        , -0.04588041, -0.        , -0.        ,\n",
    "        -0.02510613, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00009302, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00122842, -0.        , -0.00157536, -0.        ,\n",
    "        -0.        , -0.5299467 , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.74540949,\n",
    "        -0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
    "         0.        , -0.        , -0.01879568, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.04203834, -0.        , -0.        ,\n",
    "        -0.02572905, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00007979, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00122741, -0.        , -0.00162767, -0.        ,\n",
    "        -0.        , -0.2755925 , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.6922521 ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.0176982 , -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.03792515, -0.        , -0.        ,\n",
    "        -0.0263953 , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00006561, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00122631, -0.        , -0.0016838 , -0.        ,\n",
    "        -0.        , -0.00300461, -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.63528233,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01652245, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.03342255, -0.        , -0.        ,\n",
    "        -0.02703322, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00005492, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00122844, -0.        , -0.00159864, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.5661127 ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01615171, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.0285854 , -0.        , -0.        ,\n",
    "        -0.0277195 , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00004351, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00123077, -0.        , -0.00150555, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.49190325,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01576324, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.02341041, -0.        , -0.        ,\n",
    "        -0.02845242, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00003129, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00123324, -0.        , -0.00140597, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.41237034,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01534775, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.0179481 , -0.        , -0.        ,\n",
    "        -0.02922886, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00001811, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00123488, -0.        , -0.00129551, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.32596336,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01476784, -0.0001422 , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.01227809, -0.        , -0.        ,\n",
    "        -0.03003602, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00000407, -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00123486, -0.        , -0.00116975, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.23127957,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01390143, -0.00054645, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.00655172, -0.        , -0.        ,\n",
    "        -0.03082442, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00123247, -0.        , -0.00097988, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.12846736,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01290671, -0.00097802, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.00076122, -0.        , -0.        ,\n",
    "        -0.03159486, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.0012276 , -0.        , -0.00075924, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.01734515,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01178271, -0.00148422, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.03123711, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00123338, -0.        , -0.00063413, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.01090674, -0.00099349, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.03070284, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00124137, -0.        , -0.00051627, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.00998871, -0.00028364, -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.0302137 , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.00124733, -0.        , -0.00036557, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.00845575, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02974665, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00125157, -0.        , -0.00018966, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.0064783 , -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02925613, -0.        , -0.        ,  0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00125574, -0.        , -0.00000174, -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        , -0.00435146, -0.        , -0.        ,\n",
    "         0.        ,  0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02882254, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00123514, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.00182603, -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02827697, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00120905, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02748955, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.0011736 , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02664582, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00113561, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02574164, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00109489, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02477251, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00105125, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02373548, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00100444, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02262213, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00095434, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02142937, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00090064, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.02015247, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00084304, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.01878266, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00078136, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.01731602, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00071521, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.01574287, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00064438, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.01405834, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00056841, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.01225172, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00048706, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.01031615, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00039985, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.00824229, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00030638, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "         0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.00601933, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00020623, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
    "         0.        , -0.        ,  0.        , -0.        ,  0.        ,\n",
    "        -0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.0036374 , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.00009888, -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.00078775, -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        , -0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
    "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
    "         0.        , -0.        ]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.]),\n",
    " array([-0.,  0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
    "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
    "         0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "ax = plt.gca()\n",
    "ax.plot(np.log(lambdas), coefs_lasso)\n",
    "# ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Log Lambdas',fontsize=18)\n",
    "# plt.xlim((-13,0))\n",
    "plt.ylabel('coefficients',fontsize=18)\n",
    "plt.title('(a)Lasso coefficients Vs alpha',y=-0.18,fontsize=22)\n",
    "plt.savefig('lasso1.jpg', dpi=600, bbox_inches='tight',pad_inches=0)#bbox_inches='tight',pad_inches=0解决图片空白边缘\n",
    "plt.savefig('lasso1.svg', dpi=600, bbox_inches='tight',pad_inches=0)#bbox_inches='tight',pad_inches=0解决图片空白边缘\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.623237882163124e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArYAAAIwCAYAAABz6igpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABiiklEQVR4nO3dd3xT9f7H8XeSLkZb2rKhAlK2IssBUqGgQlFBLrgAFRH9qbivojgAJxdcV8E9QK+CIIqioiJLqgKysSpLKkMuQmlpgUJXzu8PbmLapjtNcpLX8/HIw5Czvic9Nu9+8znfr8UwDEMAAACAyVl93QAAAADAEwi2AAAACAgEWwAAAAQEgi0AAAACAsEWAAAAAYFgCwAAgIBAsAUAAEBACPF1A3zJbrdr//79ioyMlMVi8XVzAAAAUIxhGDp69KiaNm0qq7XsPtmgDrb79+9XfHy8r5sBAACAcuzdu1fNmzcvc52gDraRkZGSTr1RUVFRPm4N4BtGYaHS1/8mSarfvYMsNpuPWwQAwN+ys7MVHx/vzG1lCepg6yg/iIqKItgiaBXm5Wvn0vWSpJYXnC1bWKiPWwQAQEkVKRvl5jEAAAAEBIItAAAAAgLBFgAAAAHB1MF29+7dslgsJR5jx471ddMAAADgZaa+eWz9+vUKCQnRO++8U6SgOCEhwYetAgAAgC+YOtiuW7dOnTp10rXXXuvrpgAAAMDHTB1s165dq169evm6GYCpWW02JVx/qfM5AABmZdoaW8MwtG7dOq1du1annXaaatWqpfPOO09ffvllqdvk5uYqOzu7yAMIdhabVdHtWym6fStZbKb9lQAAgHmD7fbt23XkyBEdOXJE48aN09SpU5WTk6PBgwfr+++/d7vNlClTFB0d7XwwnS4AAEDgsBiGYfi6EVWRlpamDz74QOPGjVNMTIykU1OudejQQT179tT8+fNLbJObm6vc3Fznvx1TtGVlZTHzGIKWUViow5u2S5LiurRlSl0AgF/Jzs5WdHR0hfKaaWtsW7VqpUceeaTIa1FRURo6dKjmzJnjdpvw8HCFh4d7o3mAadgL7do9f4kkKebMBNkItgAAkzJtKUJp6tSpo4yMDOXn5/u6KQAAAPAi0wbbmTNn6oEHHijx+pYtWxQVFaXQ0FAftAoAAAC+Ytpgm5WVpZdeeklpaWnO11JSUrR48WINGDDAhy0DAACAL5g22I4ePVrR0dFKTEzUHXfcodGjR2vAgAGKjo7Wk08+6evmAQAAwMtMG2zr1aun7777Tp07d9Y777yjRYsW6fLLL9fatWvVtm1bXzcPAAAAXmbaUREkqV27dlq0aJGvmwEAAAA/YOpga0bLly9XUlKSli9fXuZ65a1TlX0kJSVVrrEIClabTaePGOh8DgCAWRFsvSgjI0Pp6enKyMgoctNbcZGRkWWuU95yd+tERka6DcIVCdAIbBabVTFntvF1MwAAqDaCrZeFhYUV+W9V1qnKPnJzc0sE4YoEaEmlBmIAAAB/QrD1ssTERElSz549S13HMUVwaeuUt7y0dVatWlVknYoEaHfB17X3l4BrfkahXUd+/V2SVK9ja1lspr2nFAAQ5Ai2QaR4EK5ogC4efB29v46A61rOQNA1H3thoXbN/lqS1OWxW2Qj2AIATIpgi3K5C76rVq1Sbm5ukZrh4nW8hFwAAOBNBFtUiSPspqamSjrVq+tax0vIBQAA3kawRbUUrxl21PEWD7mSKFcAAAA1imALj3J3s5rrTWiudbkAAACeRLBFjXGEXNeb0Bx1ufTeAgAATyPYwmtcyxWK32xGwAUAANXFuD7wup49exa52cy1BxfeZ7VZ1WL4hWox/EJZGeoLAGBifIrBJ3r27Fkk4Kanp0tyP8sZapbFZlP97h1Uv3sHWWw2XzcHAIAqI9jCp1wDbkZGBr23AACgygi28AuOYcOK31yGmmcU2pW1NU1ZW9NkFNp93RwAAKqMYAu/0rNnT8Ktl9kLC7Xz3S+0890vZC8s9HVzAACoMoIt/A7hFgAAVAXBFn6JcAsAACqLYAu/RbgFAACVQbCFX3MNtxLDgQEAgNIRbOH3GA4MAABUBMEWplB8ODAAAIDiCLYwFUfvLb22nmO1WRU/uI/iB/dhSl0AgKnxKQbTOeOMMyhJ8CCLzaaGPTurYc/OTKkLADA1gi1MidESAABAcQRbmFLx0RJQdYbdrqO79unorn0y7EypCwAwL4ItTIt6W8+wFxRq+5sLtP3NBbIXMKUuAMC8CLYwNUe9LQAAAMEWpkevLQAAkAi2CAD02gIAAIlgiwBBry0AACDYIiDQawsAAAi2CBhhYWGSRM8tAABBKsTXDQA8JTExURkZGfTcVpLFalWz5POdzwEAMCs+xRBwqLetHGuITY0v6KbGF3STNYQpdQEA5kWwRcCh3hYAgOBEsEVAote24gy7Xcf3/qXje/9iSl0AgKkRbBGQ6LWtOHtBoba+Mk9bX5nHlLoAAFMj2CJg0WsLAEBwIdgiYNFrCwBAcCHYIqDRawsAQPAg2CKg0WsLAEDwINgi4DlmJAMAAIGNYIuAl5iYSDkCAABBgCl1EfAyMzMpRyiDxWpVk/7nOJ8DAGBWBFsEBcoRSmcNsanphef6uhkAAFQb3TMICpQjAAAQ+OixRVCgHKF0ht3QyUMZkqSIBrGyWC0+bhEAAFVDjy2CBuUI7tkLCvTrv2fr13/Plr2gwNfNAQCgygi2CBqUIwAAENgItggalCMAABDYAirYvvHGG7JYqA9E6ShHAAAgcAVMsD1w4IAeeOABXzcDfi4xMdHXTQAAADUkYILtHXfcoaysLF83AyZBrS0AAIEnIILt559/ro8//lhjxozxdVNgAhkZGdTaAgAQgEwfbI8eParbbrtNt956q3r37l3murm5ucrOzi7yQHCi1vZvFqtVjRK7qlFiV6bUBQCYmuk/xR566CFZLBb961//KnfdKVOmKDo62vmIj4/3Qgvhjxj662/WEJuaD+qt5oN6yxpi83VzAACoMlMH29WrV+uVV17R66+/rsjIyHLXnzBhgrKyspyPvXv3eqGV8EcM/QUAQOAx7ZS6+fn5uummmzRixAglJydXaJvw8HCFh4fXcMtgFpQjnGLYDeVlHZUkhUVHMqUuAMC0TBtsn3nmGe3du1fz58939rwdO3ZMkpSenq7Q0FBFR0f7sonwcwz9dYq9oECp096VJHV57BbZwkJ93CIAAKrGtMF28eLFysrKUvv27Ussa9Cggfr06aMVK1Z4v2EwleXLlyspKcnXzQAAAB5g2mD73HPPKTMzs8hrixcv1jPPPKNvv/1WMTExPmoZzII6WwAAAotpg2337t1LvLZv3z5J0oUXXujt5sCkqLMFACBwmHpUBKC6qLMFACBwBFSwHT16tAzD8HUzYDKMZwsAQGAIqGALVBZ1tgAABA7T1tgCnhLsdbYWq1UNzjvT+RwAALMi2CLoBXudrTXEptOG9PV1MwAAqDa6ZwAAABAQCLaAgvsGMsMwlH/shPKPneDmSwCAqRFsEfSC/QYye36Btjz1lrY89Zbs+QW+bg4AAFVGsAXEDWQAAAQCgi0gbiADACAQEGyB/wnmOlsAAAIBwRYQdbYAAAQCgi3wP9TZAgBgbgRb4H+oswUAwNyYeQwIcharVXHd2jufAwBgVgRbIMhZQ2xqecVFvm4GAADVRvcM4IKREQAAMC96bIH/CdaREQzDcM44Zg0NkcVi8XGLAACoGnpsARfBODKCPb9Amya9pk2TXmNKXQCAqRFsAReMjAAAgHkRbAEAABAQCLaAG9xEBgCA+RBsgWIyMjKC8iYyAADMjmALuBGMN5EBAGB2BFvADW4iAwDAfBjHFghyFotF9c5IcD4HAMCsCLZAkLOGhqj1yGRfNwMAgGqjFAEoBSMjAABgLgRbwI1gnV4XAAAzoxQBKEWwjIxQmJevTZNekyR1eewW2cJCfdwiAACqhh5boBSMjAAAgLkQbAEAABAQCLYAAAAICARboAyMjAAAgHkQbIFSMDICAADmQrAFyhAsIyMAABAIGO4LKEMwjIxgsVgU1a6F8zkAAGZFsAWCnDU0RG1GD/Z1MwAAqDZKEQAAABAQCLZAORgZAQAAc6AUAShDMIyMUJiXry1PviVJ6vzIWKbUBQCYFsEWKEcwjIxgzy/wdRMAAKg2ShGAcgTDyAgAAAQCgi0AAAACAsEWAAAAAYFgCwAAgIBAsAUqgCG/AADwf4yKAJQj0If8slgsqtuqmfM5AABmRbAFKiCQh/yyhoao3c3/8HUzAACoNkoRgApgyC8AAPwfwRYAAAABgVIEIMgV5uUrddosSdIZ40czpS4AwLQCosf25MmT2r59uzIzM33dFMCUCo6fVMHxk75uBgAA1WL6YDtlyhTVr19f7dq1U8OGDTVmzBjl5ub6ulkIQAz5BQCAfzN1sJ0zZ44mTpyoKVOmaPPmzZo8ebJmzZqlF154wddNQ4AJ9CG/AAAIBKatsTUMQ5MmTdLdd9+tO+64Q5LUuXNnLV++XF988YUefPBBH7cQgSaQh/wCACAQmDbYFhYW6qWXXlK3bt2KvB4WFqa8vDwftQqBjCG/AADwb6YNtiEhIRo4cGCR11JTU7Vs2TI98sgjbrfJzc0tUn+bnZ1do20EAACA95i6xtbhl19+0dixY3Xuueeqf//+uv/++92uN2XKFEVHRzsf8fHxXm4p4H8sFotqN2uo2s0aMqUuAMDUAiLYZmZmasuWLcrJyVFISIjy8/PdrjdhwgRlZWU5H3v37vVySwH/Yw0NUYfbr1KH26+SNdS0X+IAABAYwbZ379766aeftHLlSi1ZskT33HOP2/XCw8MVFRVV5AEAAIDAEBDB1iExMVGjRo3SJ5984uumIEAxli0AAP7LtMH24MGDmjBhQomxRWNjY1VQUOCjViGQBepYtva8fP08dZZ+njpL9jz3ZTwAAJiBaQvqoqKiNGPGDOXn5+vZZ5+VJNntdi1atEjnnnuuj1uHQBWIY9kakvKOHHU+BwDArEwbbCMiIvTQQw/poYce0sGDB9WzZ0999tln+u233/TKK6/4unkIUIxlCwCA/zJtsJWkBx98UHXq1NH06dO1YMECde3aVcuWLdP555/v66YBAADAy0wdbC0Wi+68807deeedvm4KAAAAfMy0N48BAAAArgi2AAAACAgEW6CSAm0sW4ukiIaximgYKybUBQCYmalrbAFvC8SxbK1hoep0z0hfNwMAgGqjxxaopEAcyxYAgEBAsAUqibFsAQDwT5QiAEHOnpev316eJ0nqMO5KWcNCfdwiAACqhmALBDlD0smDGc7nAACYFaUIAAAACAgEWwAAAAQEgi1QRYE2ni0AAGZHsAWqICMjI+DGswUAwOwItkAVMZ4tAAD+hVERgCoKlPFsLZLC6kU6nwMAYFYEWyDIWcNCdeYDo33dDAAAqo1SBAAAAAQEgi0AAAACAqUIQJCz5xdo2+sfS5La/d8wWUP5tQAAMCd6bIFqCISxbA3DUM6fB5Xz50EZBpPqAgDMi2ALVFFmZiZj2QIA4EcItkA1MJYtAAD+g2ALVEOgjGULAEAgINgCAAAgIBBsAQAAEBAY1weAQupE+LoJAABUG8EWCHK2sFCd9chNvm4GAADVRikCUE2BMJYtAACBgGALVANj2QIA4D8oRQCqyexj2drzC7Rj5kJJUpsbBjOlLgDAtPgEA6rJ7GPZGoahY2l/Op8DAGBWlCIAAAAgIBBsAQAAEBAItgAAAAgIBFsAAAAEBIItAAAAAgLBFvAAs0/SYA0NYZgvAIDp8UkGVJPZJ2mwhYWq6+O3+roZAABUGz22gAeYfZIGAAACAcEW8ACzT9IAAEAgoBQBCHL2/AL9/sEiSVLrkYOotQUAmBafYECQMwxD2dt2O58DAGBWlCIAAAAgIBBsAQAAEBAItoCHmH0sWwAAzI5gC3iA2ceyBQAgEBBsAQ9hLFsAAHyLYAt4CGPZAgDgWwz3BQQ5W1iouk+5w9fNAACg2uixBQAAQEAg2AIAACAgUIoABDl7foHS5n0rSWp15UVMqQsAMC1T99ja7XY98cQTio+PV2hoqJo3b65XX33V181CEDPjWLaGYehI6k4dSd3JlLoAAFMzdbCdPHmyJk2apH79+umVV15RmzZtdNttt2nevHm+bhqCEGPZAgDgW6YNtocOHdK0adP0/PPP691339VNN92kr776Sk2bNtU777zj6+YhSDGWLQAAvmPaYHv8+HE9/vjjuuOOv4cpioiIUJs2bXTo0CEftgzBjLFsAQDwHdPeJdKyZUuNHz++yGsFBQXasmWLLrnkErfb5ObmKjc31/nv7OzsGm0jAAAAvMe0PbbuzJw5U5mZmbr22mvdLp8yZYqio6Odj/j4eC+3EAAAADUlYILt4cOHNXHiRCUmJuriiy92u86ECROUlZXlfOzdu9fLrQQAAEBNMW0pQnG33nqrsrOz9eabb5a6Tnh4uMLDw73YKsD/WUND1OWxW5zPAQAwq4DosX3jjTf00Ucf6cUXX1S7du183RwEObONZWuxWGQLC5UtLFQWi8XXzQEAoMpMH2zXrFmjO++8U9ddd53Gjh3r6+YgyDGWLQAAvmPq7x137typyy67TJ06ddJrr73m6+YAksw3lq29oFB7FiyTJJ02tJ+sITYftwgAgKoxdbC99tprdejQId199936+OOPiywbNWqUj1qFYGe2sWwNu12HN2yVJMUP6SuJYAsAMCfTBtvDhw9r9erVkqSHH364xHKCLQAAQHAxbbCNi4uTYRi+bgYAAAD8hOlvHgMAAAAkLwXbOXPm6KWXXvLGoQC/YLYhvwAACAQVDrY2m03du3d3u+y9997TF198Ueq206ZN0z333FP51gEmxJBfAAD4RoVrbA3DKLWmdfTo0eratasuvfRSjzUMMDOzDfkFAEAg8NjNY9zIBfzNTEN+WUND1Pnhsc7nAACYFZ9iQJCzWCwKrVvL180AAKDaGBUBAAAAAYEeWyDI2QsKte/LFElS80sSmVIXAGBa9NgCNcgMw34ZdrsOrf5Zh1b/LMNu93VzAACoskr12O7cuVP9+vWr0jIg2GRkZDDsFwAAXlSpYHvs2DGtWLGi0sukUzeoAMGGYb8AAPCeCgfbSZMm1WQ7gIBkpmG/AAAwO4ItAAAAAgI3jwEAACAgVDrYHj9+XB9++KEWLFhQYtnhw4c1duxYNWjQQHXr1lW3bt00Y8YMZiVzYbcbzv+W9ShvHU/so6x14DlmGBkBAIBAUKmbx+bOnatx48YpMzNTw4YN09ChQ53LMjMz1atXL+3cudMZZDdt2qS77rpLK1as0Pz58z3bchP6au3vmjpvlS7ufro+/WGb8goK3a4XHhqiIb3alrpOecuru05YiFX3De2u4X3ryWq1lBl07XbDuY7Vyg2CxWVmZvr9yAjWkBCdMf5653MAAMyqwp9iixcv1ogRI2QYhho0aKC2bdsWWT5ixAjt2LFDYWFhevDBB3X22WdrzZo1euGFF7RgwQLNmjVLo0eP9nT7TWX6Z+u0//AxzV6WqryC0scLzcktKHOd8pZXd52cXGniB6v13KcbKxyOv167U7dfcpb6nxVfYp1gD7/+PjKCxWpReEyUr5sBAEC1WYwK1glcfPHFWrJkicaOHavp06crPDzcuezrr7/WoEGDZLFY9Nlnn+nSSy91Lvv88881ZMgQJSYm6rvvvvP8GVRDdna2oqOjlZWVpaiomv9gn/Fxij77cbua1o/Uxh3/1fGT+W7Xi6odrs6tG5W6TnnLq7vO8ZP5KvhfL21YiLXMcOy6jtViUURYyVmrygu/VqtFMTExkk71cLpT3nJPrVMTx4mJiVFsbGyp6wIAgNJVJq9VONjGxcUpLy9PR44ckc1WNLz07NlTP/30k0aOHKn33nvP7bZ2u73MIOAL3g62GRkZzuf+Fr5cffzjTr2/fKtsVmuFw/GSDWnKzXffqyuVHn7/LnvoLKvVosOHT71HxXt2zR5sN2/erKSkpFLX9yV7QaH2L14lSWp6cU+m1AUA+JXK5LUKlyIcP35cLVq0KBFqf/rpJ61Zs0ZWq1UPP/yw221jY2O1Z8+eih4KPjasV4KG9UqoVIB768u1en/51jJ7mB3hNye3wLnMXdmDZOi+od2dPbtmL1/w9zpbw27XXykbJUlNLjxXEsEWAGBOFQ62rVq1Ulpamg4ePKiGDRs6X3/sscckSf3791e7du1KbPfnn3/qjz/+UNOmTT3QXPgrRxh2p6zw6yh7yDx2ski978QPVuvpj9a6vZHNjEHX3+tsAQAIBBUOtldddZUef/xxDR06VP/6179Ur149vfXWW/rqq69ksVj0yCOPlNgmPT1d1157rex2u/r37+/RhsN83IVfd2UPB4/kqMBuKCe3oESPrmudrpkCbmJiopYvX+635QgAAASCCgfb8ePH6+OPP9aqVavUt2/fIsvGjh1bZOrQefPmaerUqdq6datOnDihOnXq6MEHH/RYoxE43JU9OMLu8ZP5bnt0J36wWs8uWO8sVzBDwPX3cgQAAAJBhSdoqF27tr777juNGjVK4eHhMgxDERERuu+++/TKK68UWXfv3r3auHGjTpw4ocaNG2vhwoUlhgcDSjOsV4IWPHypFj8xVPcO7aamsXUUXz9SF3U/XeGhNtkNQ0eO52niB6uVPPlTfbtxjykmlaAcAQCAmlWp0dhjY2P13nvv6e2331Z6erri4uLcflj36tVLTz/9tNq0aaNBgwapVq1aHmswgkvxHl1Hna6jXMERcJ9dsF6Tr71Ag85xX+frD1y/1QAAAJ5X4WD7+OOPV3rnv/zyi3755RdJksVi0aOPPlrpfQCuHEHXUa7gGnDve3OpLBaLzjn9VAj2xxIF6mwBAKg5FQ62kydPlsVSNCiUNgSuu/UItvAktwG30NCUD3/UybwCOYYMu6jrab5uqpO/1tlaQ0LU8e4RzucAAJhVlT7FHEF10KBB6ty5c5FZyABvcg24327ap33pR3XkeK4kadLs1ZLkV+HWH+tsLVaLajWK83UzAACotgoH26FDh2rjxo36448/JJ0Kt1999ZW+/fZbdejQQd26dXM+unTpotq1a9dUm4EShvVK0NhLztbs5b/ojS83OEsUHOHWX0ZPYNgvAABqToWD7ccffyxJOnLkiDZs2KANGzZo48aN2rBhg1JTU7VlyxbNmjVLFotFVqtVbdq0KRJ2u3btqujo6Bo7EUCSRiR1UnKXpvr4x516ceEm5eYXFhke7Mp+MT5tnz+WI9gLCnVgxTpJUuO+PZhSFwBgWpUuRahXr5769eunfv36OV87fvy4Nm3apA0bNmjNmjWaPXu2tm3bpm3btmnOnDmSTtXdFhQUlLZbwKMcE0E4wu2R43maNHu1IiPrKvns1j5tm7+VIxh2u/679CdJUqMLuokpdQEAZlWtO0Vyc3O1ZcsWZw+uo/dWKnpjWfPmzdWtW7fqtRSoJEe4dR09Yfpn63webBn2CwCAmlHhYHvs2DFnr6zjsXXrVhUWFjpDrMViUevWrdW1a9ciZQhxcdyYAt9wvbnsw5XbdSgrR1+t/V3nnB7j05pb6mwBAPC8Cgdbd/WxHTt2LHHTWGRkpEcbCHjCsF4J+uj7Hco6nqspH/4oydDtl5zlkxET/LHOFgCAQFDhYOsY4svV8ePHlZKSopSUlHK3t1gs+v333yvfQsBDrujdxjkk2KGsHJ8OB+ZvdbYAAASCStXYFp+QwTH0V0UUD8WAt7kOCTblwx+Vm1+oSbNXy2q1qP9Z8V5tC8N+AQDgeRUOtmlpaTXZDsBrRiR1Uk5OjnPEhDe/SfV6sKUcAQAAz6twsG3RokVNtgPwKseICR+u3K7DR0/o2417vF6S4C/lCNYQm9rfdqXzOQAAZmX1dQMAXxnWK0E2q0XZOfmaNHu1lm7e69Xj+8uwXxarVXXiG6lOfCNZrPxKAACYF59iCGpX9G6j8FCbCu2G3vwm1SdtWL58uU+OCwBAoCHYIqgN65WguwZ30WkNInX46Akt+mmnV4+fkZHh81pbe0GhDqzcoAMrN8heUOjTtgAAUB0EWwQ915KE+95cqq/WendYOl/X2hp2u/786gf9+dUPMux2n7YFAIDqINgC+rskoaDQ0IzP1sluN8rfyEP8pdYWAACzI9gC+rskoWvrRuqS0EhDn/7CqzeTUWcLAED1EWyB/xnWK0FzHx6qjTv/0oHMHK/dTMaYtgAAeAbBFihmVP8znDeTeavX1td1tgAABAKCLVDMiKROzpvJvNVrS50tAADVR7AF3Liidxuv99pSZwsAQPUQbAE3XIcA80avrS/rbK0hNrW9aaja3jSUKXUBAKZGsAVK4e1eW1/V2VqsVkWe3lyRpzdnSl0AgKnxKQaUwtu9ttTZAgBQPQERbA8dOqT4+HitWLHC101BgPF2r60v6myNwkIdXLVFB1dtkVHIlLoAAPMyfbA9ceKErrzySu3bt8/XTUEAcu21fWtxzfba+qrO1l5o196F32nvwu9kL2RKXQCAeZk62B4+fFj9+/fX9u3bfd0UBLArerfRmS3jNKxXQo0fi/FsAQCoOlMH2/fff1+hoaFauHChr5uCADasV4KuuaCd/rN8a42XI1BnCwBA1Zk62A4ePFjLli1TXFycr5uCAPfmN6lem2aX8WwBAKgaUwfbVq1ayWar+Libubm5ys7OLvIAKsL1JrKv1v5eY8fx5Xi2AACYnamDbWVNmTJF0dHRzkd8fLyvmwSTcL2JbMZn62r0WNTZAgBQNUEVbCdMmKCsrCznY+9e70yVisDguIlsZL8zavQ41NkCAFA1Ib5ugDeFh4crPDzc182ASQ3rlaB6dcI148uNiomM0HkJsTV2rOXLlyspKanG9u/KarMp4fpLnc8BADCroOqxBarrzW9Stf/wsRotR/B2na3FZlV0+1aKbt9KFhu/EgAA5sWnGFAJV/Ruo66tG2lU/zNktxs1dhzqbAEAqDyCLVAJw3olaO7DQ1WvboSGPv1FjY1r6806W6OwUOnrf1P6+t+YUhcAYGoEW6AKpn+2rsbHtfXWeLb2Qrt2z1+i3fOXMKUuAMDUAiLYtmzZUoZhqG/fvr5uCoLEqP5nOMe1rYleW8azBQCg8gIi2ALeNiKpk3Nc27cW10yvLXW2AABUDsEWqCLHuLbDeiXUyP4ddbZMsQsAQMUE1Ti2gCc5xrV96fNNiqkbof5neX4mu4yMDEoSAACoIHpsgWp485tUHcjMqbFyBImSBAAAKopgC1RDTZcjSEyxCwBARVGKAFSDN8oRpJqdYtdqs+n0EQOdzwEAMCt6bIFqqulyhJoe+stisyrmzDaKObMNU+oCAEyNTzGgmrxRjkCdLQAA5aMUAagmb5Qj1GSdrVFo15Fff5ck1evYml5bAIBp8QkGeICjHMGMU+zaCwu1a/bX2jX7a9kLC2vkGAAAeAPBFvCAK3q3YYpdAAB8jGALeMCwXglMsQsAgI8RbAEP8dYUuwAAwD1uHgM8xPUmsuaN4pR8dmtfNwkAgKBCjy3gQY6byGZ8tq5G9l9TN5ABABAICLaABznKEUb2O8Pj++YGMgAAykYpAuBBjnKEGV9uVExkhM5LiPXo/mviBjKrzaoWwy90PgcAwKz4FAM87M1vUrX/8DHN+Gyd7HbDo/uuiRvILDab6nfvoPrdO8his3l8/wAAeAs9toCHXdG7jb7dtE8JzWI09OkvdOdlXWpkNjIEN3f11klJSeXWYZe3jif24U/H8ae2BOM5+1NbOGfPr5OUlFTmdr5AsAU8bFivBI295GwNemSuczYyTwbb5cuXe/SXiVFoV/aO3ZKkqDYtmFLXj7h+kLh+sGRnZ+vw4cNF1o2MjFRGRobS0tJK3V9563hiH/50HH9qSzCesz+1hXP2/HEiIyNL3caXCLZADRnV/wy98/UmHT56Qt9u3KOLup5W7X3WxA1k9sJC7Xz3C0lSl8dukY1g61WOP1SK94a4hld3HyzF660d/y6rDru8dTyxD386jj+1JRjP2Z/awjl7/jj+OmmQxTAMzxYBmkh2draio6OVlZWlqKioGj9eRkaG83lmZmap68XExJS5TnnLPbWOt47jT23x9HEGTJittL+yZbNa9NiI84qE26q2JTU1VUOGDCn12JVVmJevTZNek/S/YBsW6rF9o2wZGRlaunSp+vfvr08//bTEcscHR2RkpBITE7Vo0SLnsp49exZZ1x+vf18fx5/aEozn7E9t4Zw9f5yYmBjFxnr2BunSVCav0WML1KArerfRiws3KTe/UJNmr5akavfcMgNZYCmrx8QRXh0fLMXDLACgKIItUIMc0+t6OtwiMKSkpDj/UCG0AkD1UUwH1LBhvRJ01+AuCg+1qdBuaNLs1Vq6eW+19skMZOaXkZFR4gYwAED1EGwBLygebt/8JrXK+2IGssCQkpLitzdfAIBZEWwBL3GE29MaROrw0RNa9NPOKu+LQGRujt5ayg8AwLOosQW8aFivBH30/Q5l5+TrvjeXymKx6JzTY2S1Wiq1H0/eQGa1WRU/uI/zOWoevbUAUDP4FAO87IrebRQealNBoaEpH/6ooU9/oW837vFZeyw2mxr27KyGPTszpa4X0FsLADWHYAt4maMkoWvrRiq0GzqQmVOlG8q4gcyc6K0FgJpDsAV8YFivBM19eKjGDe5epRvKPHkDmWG36+iufTq6a58Mu90j+4R79NYCQM0i2AI+NCKpU5EbyipTkuCpXj97QaG2v7lA299cIHtBoUf2CfforQWAmkWwBXxsWK8E2awWZefkV6okgRnIzOWzzz6jtxYAahjBFvADjhvKqjvGLfyTowSB3loAqFkEW8APFB/jtqIlCdxA5v8cP6OwsDB6awGghhFsAT9RvCShvAkcmIHM/3322WfOnxGhFgBqHsEW8COuJQn3vblUX639XXa7Uer6fLXtvxw1tfyMAMB7mHkM8CPDeiVIkl5cuEm5+YWa8uGPkgzdfslZuqjraSXW5wYy/+Qaamv6Z+T4w8duN2S1Wsr8Q6i8dTyxD386jj+1JRjP2Z/awjl7/jhlbeNLBFvAzzjC7beb9mlf+lEdyjo1gYPValH/s+I9fjyL1apmyec7n6N6XENtTZYf2O2Glm7Zq2c/2aC8gkKFh4ZoSK+2+vSHbcorZdi28tbxxD786Tj+1JZgPGd/agvn7PnjhIeG6NlbkzXk/A5ut/MVi2EY/hm5vSA7O1vR0dHKyspSVFRUjR8vIyPD+TwzM7PU9WJiYspcp7zlnlrHW8fxp7b42znPXv6Lpnz4o3LzC9WqUZQ+HJ9cYp3Y2FgtX75cSUlJpR4L3uEu1Hrymjt8+NTvEEegzc7JlWunSViIVXkFZU+yUd46ntiHPx3Hn9oSjOfsT23hnD2/Tvv4+vp++s1lbusJlclr9NgCfmxEUifl5OTow5XbnaMlFC9JyMjI4CYyP+DpnlrXr/nsdkPfrN+lSe+tVF5BoU7mFTgDbYjNojrhoYqqHa7OrRtp447/6vjJfLf7LG8dT+zDn47jT20JxnP2p7Zwzp4/TlTtcN14SQ+32/gSwRbwc8N6Jeij73c4R0twV5JQnRuUDLtdOX8ekiTVbtaAcoRKcPSUVyfUuqtfcy0xkE595RceatOR47nOdUJsFjWMrq1rk9rrH70STPWNhb99MxJIx6EtnLO3juP4xtDfEGwBE7iidxvnDWVvfpNaIthW5wYle0Ghtr4yT5LU5bFbZAsj2JZn+fLlys7OVl5eXpHJFyobar/dtEevLPpSF3c/vUj9mmuPrCTl5BaoQXRtNY2to+Mn81UnItQZaAEAfyPYAibguKHMUZKwdPPeEuGWOtua5wi0hw8fliTFxcVJqtrkC99u2qNJH6xWod3Q7GWpJerXHCUG0qmv/MYmd9HALk09cBYAELgItoBJuJYkFO+1ZbKGmlU80DpKPxw95ZUJtY7RDByhNiLUpv7dWhWpXyveI1uRrwsBAARbwFSu6N2myI1kV/aLcS5jIoCa4aiflf5+j6tyc5jr8FyO0QzCQ22acHUvXZPUidAKAB5AMR1gIsWn3f1q7e/OZYmJiVq+fLkPWxdYli9fXqR+1lFuUNneWbvd0Fdrf1fy5M808f1VOnL871B79+AuuiapUw2eBQAEF3psAZNxvZFs+mfrlHx2a0mUI3iK641hUtXqZ117Zy0WS5ERDYqPZgAA8ByCLWAyrjeSHcrK0Vdrf9d5CaeGXAkLC+Mmsioq7cawioRad0N1uU6e4BjRQBKBFgBqEMEWMCHHjWRZx3M147N1Oueei2S1WnTGGWdow4YNzpKEigRci9WqJv3PcT4PNuXdGObK3VzprhMnSCoxeUKTmLqMaAAAXkKwBUzqit5t9O2mfUpoFqOhT3+h2y85Sxd1PU25ublKS0tTZGRkkYBbWk+uNcSmphee6+3m+4xrHfJZZ52ltLQ0SaXfGOYIs8UnTZDcT5wgFS03uPGSsyUxogEAeAPBFjCpYb0SNPaSszXokbk6kJmjSbNXS5Iu6tlTq1atKhJwHdPuVqYnN1C4BlnXntnIyEhJfwfac889T9LfQbasaWwdik+cIJUcqgsA4D2mD7YrVqzQfffdp19//VXdu3fXzJkzlZDABwqCx6j+Z2jKhz8qN7/QGW77/y+krVq9SiGhpwb5L60n17AbOnkoQ5IU0SBWFqvFB2dRdaWNBOE61a0k2Y1TqdQRZB3vy7nnnqelW/YqefJn5fbGuk6aIDFxAgD4G1MH23Xr1ik5OVlt27bVlClT9Mknnyg5OVk///yzIiIifN08wCtGJHVSTk6Oc6SEiR+s1tMfrXUuDw89pMm1muncc8/TqtWrdOLkSf2+a5ciIyO1dNky9e2dqF//PVuSY0rd0NIOVSGOoOkIksX179dPS5ctK3X78pa7ruPaA+vK0Ut9KD1dYWFh2rwvR/M3ZqrAbshms0k69b4M2R+iT3/YVuRGL4eKTGPLxAkA4F9MHWzvv/9+xcbG6rvvvlO9evU0duxYJSQk6I033tCdd97p6+YBXuMYKcERbnNyC5zLcnILNOXDH3Uyr8DZI1lYWCjpoEJtuzR2a7qSdSqIrljxnRTi/gayigTOrl266Pddu/Tz/pP69OdsFRRLi6E2q4ZvzdFHK34usawiy92vU7KHOdSWrrXpP+qjFYdkWKxFywgKCpzvi+tUtvTGAoD5mTbYZmZmauXKlZowYYLq1asnSapTp46uvvpqLVy4kGCLoOMIt+8v3+qs95ROBbST+YUlbnCSpNwCQ9O/3ak9MaFakpWv3J0/ud13hQNnnxx9tOKQcvLscrdWboH9f2HS/T7KW+5+nZLr5hYUuoTW0oNr59aNtHHHfyWVHIaL3lgAMB/TBtutW7fKbreXGJKnc+fOev/9991uk5ubq9zcvz/cs7Oza7SNgLcN65XgDLgOMTExmr38F73x5QYdP5mvgoK/Q++JfEOFhvR5Rr7yi+/MReUC56kgabNIEaFFe1NrhYfo3I4ttDo1TSfz7SX2Ud7yqq4THmpV/7ZROr91pHOdyMhIJSYmatGiRZKknj2pzQcAszNtsD1y5IgkqUWLFkVeb9CggdLT03Xy5MkSdbZTpkzRY4895q0mAj6zatUq5/PIyEiNSEpUvRO7FRkZqfDwcOeyJb8c0g/bDqvZyTxtPVGoY6WMY1srzKbeXRL0/aadOpFXWO46knRpl0bq36lBifUGDhyor7/+utS2l7fck+vExsaqfv36Onr0aJH3TPo7+Dper+zsYwAA7zNtsD1VI3iq/MCVI8wePXq0RLCdMGGC7r33Xue/s7OzFR8fX8MtBbxn1apVioyMVP369Yu87ghwkooE20u6NdclnZsqatmvkqTsfh1LrbF1jIVbloqsU7wNVVnu6XXCw8OLfJvjUFbwlUqGX8n7AdhxbHdtKa68dTyxD386jj+1JRjP2Z/awjl7/jiRkZEaMmRIqdv5immDba1atSRJRrE7rx3/dvch5fgAAwJRamqq2/Dq4Hit+Bi2hXn52vS/YHtBnwvKHBWhIuPfemIdbx3HdZ3SAnlZwVcq+kdD8QBc0yHX9WdevC2lKW8dT+zDn47jT20JxnP2p7ZwzjW7jr8wbbBt1KiRJGnv3r067bTTnK+np6dL+nvwdSAYxMTEFPnDzV2gKy3kWaxWNUrs6nwerEp7f8oLvtLffzS4BmBHyHX0dnhaamqq2z/WPdGT7a0ec2/3zAfTcWiLb4/jT23x5jn7A4tRvMvTJHJzc1WvXj09+eST+uc//+l8ffz48Zo+fbpOnDhR7j6ys7MVHR2trKwsRUVF1WRzJUkZGRnO52XdaV3e3dgVuVvbE+t46zj+1BaznnNKSoqioqKCakYxf+UagB0h95xzznG5Sa1oL25lrxVHWHaEWn7mAAJdZfKaabtnwsPDddFFF2nmzJnKy8uTJOXn52vu3Lnq06ePj1sHeEdKSopiY2MJOH4kKSnJ+XCET8fXeOHh4Vq1alWZ9WzupKSkSPq79IBQCwDumbYUQZIeeOABXXDBBRo5cqRuueUWvfTSS9qzZ49efPFFXzcNqHGO8gOpel8RGXZDeVlHJUlh0ZGmm1LXn7kGT9daXdda3EGDBpXYrvioFo5wXF65CQAEO1MH2/PPP19vv/22xo0bp/nz58tiseihhx7S5Zdf7uumATXOUX4gVS/k2AsKlDrtXUmemVIX7rnW6roG3JSUlCJ3HLsb1cL1DxgCLQCUztTBVpJGjx6tQYMGac2aNWrbtq3atWvn6yYBNc7RW0vIMR93Abf4HcfFe+Ad2/DzBoCymT7YSlLDhg112WWX+boZgNe49tbCnIqPtuAaZgmwAFA1ARFsgWDiGOaJ8BMY6I0FAM8x7agIQDAqPl4tAAD4G8EWMJGUlBR6awEAKAXBFjAJbhgDAKBs1NgCJlFTN4xZrFY1OO9M53MAAMyKYAuYQE3eMGYNsem0IX09vl8AALyN7hnAzzlCLTeMAQBQNoIt4KdSUlKKTKNaU7W1hmEo/9gJ5R87IcMwauQYAAB4A8EW8EOOG8Wkmp9G1Z5foC1PvaUtT70le35BjR0HAICaRrAF/JBjWC+JgfsBAKgogi3gZ5hZDACAqiHYAn6EG8UAAKg6gi3gJ1yny6W3FgCAyiPYAn6C6XIBAKgegi3gB6irBQCg+ph5DPAxX9fVWqxWxXVr73wOAIBZEWwBH/LGBAzlsYbY1PKKi3xybAAAPInuGcBHUlJSJNX8BAwAAAQLemwBH3CUH0i+n4DBMAznjGPW0BBZLBaftgcAgKqixxbwMl/X1BZnzy/QpkmvadOk15hSFwBgagRbwItcQ62ve2oBAAg0BFvASwi1AADULIIt4AWEWgAAah7BFqhhTJULAIB3EGyBGsZUuQAAeAfBFqhBjt5aQi0AADWPcWyBGpSSkqKoqChfN6NMFotF9c5IcD4HAMCsCLZADXFMl+vvvbXW0BC1Hpns62YAAFBtlCIANcB1ulwAAOAdBFvAwxx1tZLvp8sFACCYEGwBD3OMgmAWhXn5Wj9hutZPmK7CvHxfNwcAgCoj2AIexCgIAAD4DsEW8DAz9dYCABBICLaAB6WkpNBbCwCAjxBsAQ9xvWkMAAB4H8EW8CCCLQAAvkOwBTyEMgQAAHyLmccADzBzGYLFYlFUuxbO5wAAmBXBFvCAlJQURUVF+boZVWINDVGb0YN93QwAAKqNUgSgmhi7FgAA/0CwBTzArGUIAAAEEoItUE1mv2msMC9fGye+qo0TX2VKXQCAqVFjC1SDmW8ac2XPL/B1EwAAqDZ6bIFqCoRgCwBAICDYAtVg9jIEAAACCcEWqKJAKUMAACBQEGyBaiDYAgDgPwi2QBVRhgAAgH9hVASgCmJjYwOmt9Zisahuq2bO5wAAmJXpe2wPHTqk+Ph4rVixwtdNQZBISUmRFDhlCNbQELW7+R9qd/M/ZA3lb10AgHmZOtieOHFCV155pfbt2+frpiBIuN4wRhkCAAD+xbTB9vDhw+rfv7+2b9/u66YgyARKTy0AAIHGtMH2/fffV2hoqBYuXOjrpiCIBOINY4V5+dr85Jva/OSbTKkLADA1vyyoy8rK0okTJ0pdHh0drcGDB+v222/X3r17vdgyBLNAHre24PhJXzcBAIBq88tge9ddd+ndd98tdfnMmTM1evToSu83NzdXubm5zn9nZ2dXpXkIUikpKYqKivJ1MwAAQCn8MtiOHz9eo0aNKnV5p06dqrTfKVOm6LHHHqtqsxDEHL21gVaGAABAIPHLYNuxY0d17NjR4/udMGGC7r33Xue/s7OzFR8f7/HjIDAFahkCAACBwi+DbU0JDw8nnKBKUlJSNGTIEF83AwAAlMG0oyIA3hLIN40BABBIgqrHFqiKQL9pzGKxqHazhs7nAACYFcEWKEMw3DRmDQ1Rh9uv8nUzAACoNkoRgDKkpKRQhgAAgEmYvse2ZcuWMgzD181AAAqG3loAAAKJ6YMtUFMCvbbWwZ6Xr19e+ECS1OmekbKGhfq4RQAAVA3BFnAjmHprDUl5R446nwMAYFbU2AJuUFsLAID5EGyBYmJjY4OmtxYAgEBCsAVcpKSkSGL6XAAAzIhgC/yP6wxj9NYCAGA+BFvgf6irBQDA3BgVAZCUmpoatHW1FkkRDWOdzwEAMCuCLYKeowQhWHtrrWGh6nTPSF83AwCAaqMUAUHPUYIQjL21AAAEEoItglowlyAAABBoKEVA0Ar2EgQHe16+fnt5niSpw7grmVIXAGBaBFsErZSUFEVFRQV9b60h6eTBDOdzAADMilIEBCVKEAAACDwEWwSVlJQU55S5wV6CAABAoCHYImg4emkl0VsLAEAAItgiKDhCLVPmAgAQuAi2CHiuoZZACwBA4GJUBASsVatWKTIyklBbDouksHqRzucAAJgVwRYBKTU1VfXr15dEPW15rGGhOvOB0b5uBgAA1UYpAgJGSkqKpKKlB4RaAACCBz22CAiOMOs6lBeBFgCA4EKwhakVr6OVKD2oLHt+gba9/rEkqd3/DZM1lF8LAABz4hMMprNq1SpJUmRkpNs6WkJt5RiGoZw/DzqfAwBgVtTYwhRc62fr16+v+vXrU0cLAACKoMcWfsu1Z7Z4/awDgRYAADgQbOFX3JUZSKJ+FgAAlItgC59wBFiHyMhIDRkyxG2YlUT9LAAAKBfBFjXCNbhGRkYqMTGx1N5YV5QZAACAqiLYotLKCq2O14oH19jY2FJ7Y10RZn0jpE6Er5sAAEC1EWy9LCUlpUQQLM5dWKzMck+tU9XQKrkPrvTG+idbWKjOeuQmXzcDAIBqI9h6keOufndB0N26Za3jiX1U9TjlhVbJfXAlzAIAgJpEsPUy17v7K7puVZfX1HEIrQAAwB9ZjCCeaig7O1vR0dHKyspSVFSUr5sD+IQ9v0A7Zi6UJLW5YTBT6gIA/Epl8hqfYECQMwxDx9L+dD4HAMCsmFIXAAAAAYFgCwAAgIBAsAUAAEBAINgCAAAgIBBsAQAAEBAYFQEAQ3wBAAICn2ZAkLOFharr47f6uhkAAFQbpQgAAAAICARbAAAABARKEYAgZ88v0O8fLJIktR45iHpbAIBp8QkGBDnDMJS9bbfzOQAAZkUpAgAAAAICwRYAAAABgWALAACAgECwBQAAQEAg2AIAACAgBPWoCI47wLOzs33cEsB3CvPydSz3hKRT/y/YwkJ93CIAAP7myGkVGbnHYgTx+D779u1TfHy8r5sBAACAcuzdu1fNmzcvc52gDrZ2u1379+9XZGSkLBaLr5tjGtnZ2YqPj9fevXsVFRXl6+aYEu+hZ/A+egbvY/XxHnoG72P1BeJ7aBiGjh49qqZNm8pqLbuKNqhLEaxWa7nJH6WLiooKmP9pfIX30DN4Hz2D97H6eA89g/ex+gLtPYyOjq7Qetw8BgAAgIBAsAUAAEBAINii0sLDwzVp0iSFh4f7uimmxXvoGbyPnsH7WH28h57B+1h9wf4eBvXNYwAAAAgc9NgCAAAgIBBsAQAAEBAItgAAAAgIBFsAAAAEBIItKsRut+uJJ55QfHy8QkND1bx5c7366qsl1luxYoV69Oih2rVrKzExUTt37vRBa/3boUOHFB8frxUrVpRYtnv3blkslhKPsWPHer+hfq6s91HiWqwsrr2q41qrPq6/qivrd2EwXptBPfMYKm7y5Ml68sknde2116p3796aPXu2brvtNsXFxenKK6+UJK1bt07Jyclq27atpkyZok8++UTJycn6+eefFRER4eMz8A8nTpzQlVdeqX379rldvn79eoWEhOidd94pMs1zQkKCt5poCuW9j1yLlce1VzVca57B9Vc1Zf0uDNpr0wDKcfDgQSM8PNx44YUXnK+dOHHCaNq0qTFgwADna3379jWaNm1qZGZmGoZhGMeOHTMaN25svPjii15usX9KT083evbsaTRt2tSQZCxfvrzEOhMmTDDOOussr7fNTCryPnItVh7XXtVwrXkG11/llfe7MFivTUoRUK7jx4/r8ccf1x133OF8LSIiQm3atNGhQ4ckSZmZmVq5cqVuuOEG1atXT5JUp04dXX311Vq4cKEvmu133n//fYWGhpb5fqxdu1a9evXyYqvMp7z3kWuxarj2Ko9rzXO4/iqvrN+FwXxtEmyhrKwsHThwoNRHo0aNNH78eNlsNuc2BQUF2rJlizp27ChJ2rp1q+x2uxITE4vsu3Pnztq8ebNXz8cXynsPT5w4ocGDB2vZsmWKi4tzuw/DMLRu3TqtXbtWp512mmrVqqXzzjtPX375pZfPxnc88T4G+7XoTkXe12C/9qqCa80z+N1XNWX9Lgzma5NgC911111q0qRJqY+5c+eW2GbmzJnKzMzUtddeK0k6cuSIJKlFixZF1mvQoIHS09N18uTJGj8PX6rIe9iqVasifxwUt337dh05ckRHjhzRuHHjNHXqVOXk5Gjw4MH6/vvvvXg2vuOJ9zHYr0V3yntf33vvvaC/9qqCa80z+N1XNWX9Lgzma5Obx6Dx48dr1KhRpS7v1KlTkX8fPnxYEydOVGJioi6++GJJUmFhoaRTX3W4chSoHz16NKCL1Sv7HroTFhamJ554QuPGjVNMTIwkafTo0erQoYP+/e9/q3fv3h5rr7/yxPsY7NeiO+W9r3Xq1An6a68quNY8g999nhfM1ybBFurYsaOzpKAibr31VmVnZ+vNN990vlarVi1Jp75ScuX4d25urgda6r8q+x6606pVKz3yyCNFXouKitLQoUM1Z86cau3bLDzxPgb7tehORd7Xnj17Fvl3sF17VcG15hn87vO8YL42KUVApbzxxhv66KOP9OKLL6pdu3bO1xs1aiRJ2rt3b5H109PTJUmRkZHea2SAqVOnjjIyMpSfn+/rppgC16LncO2VjWutZnH9VV0wX5sEW1TYmjVrdOedd+q6664rMWh2mzZtFBERodWrVxd5fePGjYqIiFB0dLQ3m2pKM2fO1AMPPFDi9S1btigqKkqhoaE+aJX5cC1WHtde1XCteQbXn+cF87VJsEWF7Ny5U5dddpk6deqk1157rcTy8PBwXXTRRZo5c6by8vIkSfn5+Zo7d6769Onj7eaaUlZWll566SWlpaU5X0tJSdHixYs1YMAAH7bMXLgWK49rr2q41jyD68/zgvra9NUAujCX8847z5BkPPXUU8Z//vOfIg+H77//3rBarcbw4cONJUuWGIMHDzYkGQsWLPBdw/1QWlqa28G0MzMzjUaNGhnNmjUzbr/9duP66683atWqZcTExBjbtm3zTWP9WGnvo2FwLVYW117Vca1VH9df9ZT2uzBYr02CLcqVnp5uSCr14WrmzJlG7dq1DUmGxWIxHnroIR+12n+VFci2bt1qJCcnG7Vr1zYaNGhgXHPNNcbOnTu930gTKOt9NAyuxcri2qs6rrXq4/qrurJ+FwbjtWkxjGK3zAHVdPDgQa1Zs0Zt27YtcoMZ4G1ci/AWrjX4q2C7Ngm2AAAACAjcPAYAAICAQLAFAABAQCDYAgAAICAQbAEAABAQCLYA4COGYaigoMDXzTAlplkF4A7BFgB85L777tOIESNUWFjo66aYyqFDh9StWzd9+umnvm4KAD9DsAUAH3j55Zf1/PPPKzo6WjabzdfNMZWIiAhFRETommuu0YYNG3zdHAB+hGALICC0bNlSFotFs2bN8nVTyrV+/Xrdc889GjhwoF5//XXn6/54DrNmzZLFYtHo0aM9sr9169bJYrGob9++Vd5HZGSkvvzySzVo0EDDhg3T0aNHPdI2AOZHsAUALyooKNANN9ygBg0aaPbs2bJa+TVcFQ0bNtTcuXO1Z88eTZgwwdfNAeAn+I0KAF70zjvv6Oeff9b06dMVExPj6+aYWs+ePTVu3Di9/vrr2r59u6+bA8APEGwBwEsMw9Azzzyjc845R//4xz983ZyAMHHiRIWFhen555/3dVMA+AGCLQB4yapVq7Rz506NGzfO100JGPXr19eVV16pOXPmMAQYAIItAHjLkiVLZLVademll/q6KQFlyJAhys7O1urVq33dFAA+RrAFELSysrL08MMPq23btgoPD1fjxo01atQobd26tdRtFi1apHPPPVcRERFq0KCB7r33Xu3Zs0djx45VTExMmaMHbNq0Se3atVNsbKxHz+PAgQP65z//qQ4dOqh27dqKjo5Wnz59SozzOnr0aFksFr399tt69NFHVb9+fTVu3Fhz587V7t271b9/f9WuXVtdu3bVzz//XOI469evV1JSkurUqaO4uDhdccUV+u2339y26eTJk5o8ebLatGmj8PBwtW7dWs8995wMw6j2eRTXs2dPSdLmzZvLfqMABD4DAAJAixYtDEnGzJkzK7T+H3/8YSQkJBiSDIvFYrRq1coIDw83JBm1a9c2Pv/88xLbzJs3z7BYLIYko2XLlka9evWc23fu3Nno0qWLccstt5R6zK5duxqXXnqpx87BMAxj165dRqNGjZztbt++vdGsWTNDkiHJmD17tnPd66+/3pBktG7d2oiIiDCaNGliSDKioqKMdu3aGXFxcUZUVJQhyUhOTjYMwzBmzpxpSDLOPfdco06dOobNZjNatWpl2Gw2Q5JRt25d48cffyzSppMnTxq9evVytqFJkyZGTEyMIcm48MILDUlGnz59qnwe7tSpU8f45z//WeH3DUBgoscWQNAxDEPDhg3Tzp07lZiYqD/++EO7du3SkSNHdNdddyknJ0dXXXWV0tLSimx33333yTAM/ec//1FaWpoOHDig5ORkGYah22+/XRs3btSrr75a6nGPHj2qqKgoj57LI488or/++kv9+vXTf//7X/3222/at2+fXn75ZUnSU089VWKb7Oxs/frrr9q5c6dCQ0OVnZ2thg0bau/evZo/f74klRhlYM2aNeratav++9//ateuXUpPT9fAgQN17NgxXXfddbLb7c51//Wvf+nHH39U/fr1lZKSov379+vw4cN6/vnntWTJEo+dh6vo6GhlZ2dX/I0DEJAItgCCztdff63169crJiZG8+fP12mnnSbp1IxW//73v9W/f3/l5OToxRdfdG7z119/ac+ePapXr55GjRolSQoPD9f//d//SVKFpne1WCxlfhVfFcOHD9fMmTP11ltvFQnNw4cPlyRt27atxDZjx45Vq1atVLt2bTVt2lTSqWBZq1YttWnTRtKp8XZdhYeH64MPPlCDBg0kSfXq1dO7776riIgI7dy5Uz/88INz3ffee8+5z969e0s6de733HNPqaNBVOU8XNntdsYEBqAQXzcAALztu+++kyT17dtXDRs2LLH8iiuu0NKlS53rSVKtWrVksViUn5+vwsJC5zS4OTk5klShaXGjo6OVmZnpiVNwGjp0qCTpyJEjWrZsmX755RetXbtWX3/9taSSAVWSEhISKvSaq7PPPtv5B4BDw4YN1b17d/3www/avHmzEhMTlZ2drV27dkmSkpOTS+zniiuu0CeffOKR83AwDENHjhxRdHR0mecAIPDx5y2AoHP48GFJUnx8vNvljtcd60lSVFSUBg0apOPHj+vhhx9Wdna2duzYoalTp0qS+vXrV+5xW7du7fGJBNLS0nTZZZepfv366t+/v+6//379/PPPzl5ld9z1bJbX29msWTO3rzdu3FjSqRvxJBUpB2jUqFGF91OV83DYt2+fTp48qdatW5e7LoDARrAFEHTi4uIkSXv37nW73PG6Yz0Hx/izU6dOVXR0tNq2bavNmzerZ8+euuWWW8o9bvfu3bVr1y7997//rU7znU6cOKHk5GR98cUX+uc//6nffvtNx48f18aNGz0+YUFpbT548KAkKTIyssh/JenQoUMV2k91z+P777+XdOr9BRDcCLYAgk6fPn0kScuXL3cGM1fz5s2TJF1wwQXO19avX6/hw4drzJgxuu+++5SYmKikpCRNmzZNy5YtU0RERLnHHTBggCRpwYIFnjgN/fjjj9q2bZvatm2rqVOnqn379s6SCE+P6bpu3boSoTQjI0MbNmyQJJ155pmSTpVbOHq83d0o5q4Wubrn8cknn6hhw4bq2rVrpc4JQOAh2AIIOgMGDFDXrl115MgRDR8+XHv27JF0auzVu+++W8uWLVOtWrV09913O7dZunSpcnJy1LZtWz3zzDNauXKlli1bpvvvv79CoVaSOnfurG7dumnGjBkqLCys9nkcOXJE0qnxX3fv3i1Jys/P1+zZs3XZZZdVe/+ucnJyNGrUKGd5xtGjR3XjjTfq+PHjio+PV2JionPdkSNHSpIee+wx/fTTT87XX331VX344YcePY/du3frs88+0+jRo7l5DADj2AIIDI4xYOvWrWvExcWV+li0aJFhGKfGTW3VqpUhybBarcbpp59uREREGJKMiIgI49NPPy2y/9TUVOe4tTExMUbHjh2Nrl27Gr179zZGjhxpvPvuu0Z+fn657Zw/f74hyXjttdeqfQ6ZmZnOsV/Dw8ONdu3aGZGRkYYko0uXLkZISIghyThw4IBhGH+PY+s6Tq7jmGlpaYZhGEZaWpohyWjRooVhGH+PY3vmmWcaYWFhRkhIiNG6dWvnvmvVqmUsW7asyHkcO3bM6Natm3MM2qZNmxqxsbGGJKNHjx4lxrGt7Hm4uvrqq43atWsb+/fvL/e9BxD4+PMWQEA5duyYDh8+XOojNzdXktSqVSutX79eDz74oFq1aqW9e/cqKipKI0aM0Pr16zVkyJAi+23Tpo0GDhyosLAwNW7cWPv379fmzZv1/fff64MPPtD111/vvLO/LMOGDVNSUpLGjx9fYpzcyp5DvXr19OOPP+q6665TXFyc/vjjDzVt2lQTJ05USkqKOnXqJEl65513qvOWSpK6deumb775Rmeffbb+/PNP1a1bV5dffrlWr16tpKSkIuvWqVNHK1eu1MMPP6xWrVrp0KFDqlu3riZOnKiXXnqpxL6reh4ff/yxPvzwQ02YMEFNmjSp9jkCMD+LYXh4UEUACEDXX3+9Fi1apLVr16ply5bO13Nzc7Vjxw794x//0I4dO7RlyxZnvWlp/vjjD3Xt2lXx8fH64YcfitxwhYpJTU1Vr1691KFDB/3www8KCWH0SgDU2AJAhcyfP18Wi6XE7Fbh4eGy2WzOcVZDQ0PL3VfLli01Z84c/frrr0pOTi5zjFaUtHfvXl100UWqU6eOPv74Y0ItACeCLeAHVq9eraNHj3psf99++63H9oVThg0bpkOHDumss85So0aNdMYZZ+iMM85Q06ZN1bFjR6WlpWnYsGFq3759hfY3cOBAzZo1S5dffjnBrJKaNGmi5ORkffvtt2revLmvmwPAj1CKAPjYmjVrdOGFF6pz585auXJlhWawKssDDzygadOmacaMGc5xV1F9hYWFevvttzVv3jz98ssvOnz4sGw2mxo1aqTu3bvr6quv1vDhw2WxWHzdVAAIWgRbwIf27NmjHj16KD09XW+++aZuvPHGau9z/fr1uvDCC5Wdna3PP/9cgwYN8kBLAQDwf5QiAD6Sn5+vf/zjHzp06JCmTp1aJNTOmjVLFotFFotF6enpldpv9+7dtXDhQoWEhGjEiBH6448/KrW947h9+/at1HaovNGjRzvfb4vFot9++63EOuPHjy+yzooVK0qsk5OTo+eee07nnHOOIiMjVatWLZ1++ukaM2aMfv31V7fHnjx5cpH9lvdwjDVbHStXrnTu74Ybbihz3YyMDNlsNlksFp177rnVPra/WbFihfO9qOz/o5Xh+Dm73vAIBDKCLeAjTzzxhNavX68RI0bo/vvv9+i+ExMTNX36dGVlZWnMmDHiixlzWLduXYVec7Vr1y51795d9913n9auXatjx47p5MmTSktL08yZM9W1a1d98cUXNdXkSjn//PMVExMjSVq8eHGZ63777bey2+2SpEsvvbTG2wYgMBBsAR/YtWuXpk6dqtNOO02vvvpqjRzj5ptv1uDBg7V8+XLNnj27Ro4BzyoeYg3DcE5Z687Jkyc1cOBAbd26VbVq1dLkyZO1Zs0apaam6tlnn1WtWrWUl5enUaNG6cCBA6XuZ86cOUpLSyvzERUVVe3zs9lsSk5OliTt379fP//8c6nrfv31187nnp5FDUDg4lZcwAcmTZqkvLw8TZs2zSOBoTQvvviivvnmGz3yyCO6+uqrq31jGmrW+vXri/x7586dysrKKnX9d999Vzt27JAk/fvf/9bNN9/sXNapUyfFxsZqzJgxysrK0ksvvaSnn37a7X4aN27sta+qL730UucfWt98802pY/46enSbN2+uLl26eKVtAMyPHlvAyw4ePKh58+apY8eOuuqqq2r0WC1bttSYMWP0xx9/+M3X0Sjdxo0bVVhY6Px3eWUIy5Ytcz4fPnx4ieXXX3+98w+n8r7695bk5GTn8GbffPON23W2bNmi/fv3S6IMAUDlEGwBL5s/f77y8vJ0yy23eOV4juNQjuD/cnJyitxAVl6wdb2h6+TJkyWWW61WTZ06VVOmTNGYMWM81s7qqFevns4//3xJUkpKinJyckqs4xp4CbYAKoNgC3iZY/IEb9UNdu7cWS1atNCSJUu4icxP9enTRw0bNpRUtBzB8fzKK690u11CQoLz+b/+9S+369xyyy168MEHddttt3mqudXmCKu5ubluR3lw1NfWqlVL/fr182bTAJgcwRbwso0bN6pJkyaVqmlctGiRLrjgAkVFRSkyMlK9evXSO++847xrvDy9evVSRkaG9uzZU8VWV87x48c1bdo0nXvuuYqOjlZoaKgaNGigiy66SHPmzClzW7vdrnfffVcXXXSRGjZsqNDQUNWtW1ddu3bV448/Xu4Mbd99952uuuoqtWjRQhEREYqIiFCbNm10yy236Pfffy9z27y8PL3yyitKTExUTEyMatWqpbZt2+rOO+/U7t27K/0+VEaPHj0k/d1LaxiGNm7cKEk6++yz3W5z6623OqfwnT59ugYPHqwtW7bUaDs9wfWPuuLlCMePH9f3338vSbrwwgtVq1Ytt/vYtWuXxo0bp44dO6pOnToKCwtTs2bNNGzYMLdh2VMMw9CcOXM0cOBA5/UZHR2tc845R//617904sQJjx+zZcuWslgsuv3227V//37deOONatasmcLDwxUfH68bbrhBW7durdC+Tpw4oWeeeUZnnXWWIiMjVbduXZ177rllfqPji3MGqswA4DW5ubmGxWIx+vbtW+Z6M2fONCQZkownn3zS+bz4Y9CgQcaJEyfKPe7kyZMNScaSJUvKXdex7z59+lT0tIrYtWuX0bp161LbLMkYM2aM223z8/ONSy65pMxtExISjP/+979ut3/66afL3LZ27drGd99953bb33//3TjzzDNL3TYiIsJ47733qvSelOb66693vtcTJ040JBnnnXeeYRiG8dtvvzmPvXz58iLPXX3wwQdGWFhYkbb27dvX+Pjjj43CwsJSjz1p0qQy3yvXx8yZMz163oZhGG3atDEkGe3atSvy+ueff+487uuvv+522yVLlhi1a9cus83PPPOMx9t88uRJIzk5uczjdunSxcjOzi53X64/07S0tDLXbdGihSHJGDJkiNG0adNSr8958+aV2Nbxc27RooVx8OBBo0ePHqW2/YUXXqjRcwa8gWALeNHBgwcNScbw4cPLXM812FosFqNDhw7GggULjG3bthmLFy82+vfv71x+6623lnvc6dOnG5KMjz76qNx1qxtszzvvPGe7x48fb6xZs8bYsWOH8f333xtXXnmlc//Lli0rse0rr7ziXJ6cnGwsXrzY2Lp1q7FhwwbjmWeeMSIiIgxJxk033VRi29TUVMNisRiSjPbt2xtz5841fv75ZyM1NdX44IMPnOGgTZs2JbZNT083Tj/9dEOSYbVajdtvv91YtWqV8euvvxqvvfaaM0xYLBbjk08+qdL74o5rsHUEulq1ahn5+fnG+++/b0gy6tevb6SlpZUabA3DMDZv3mz07du3RODo0KGD8fXXX7s9tq+D7d133+3c/x9//OF8/fbbb3e+1/v27SuxXUFBgdGkSRNDkhEZGWm89NJLxvr1642tW7caixYtMpKSkgxJhs1mM3bv3u3RNj/44IPONg8ePNhYunSpsW3bNmPDhg3GY4895rz+Jk6cWO6+qhJsHef14IMPGhs3bjS2bNliPP/880ZUVJQhyQgPDzdSU1OLbOv4OTdt2tQ4//zzDUnGqFGjjJSUFGPTpk3Gk08+aYSGhjrfz+J/KHvynAFvINgCXrRnzx7nB0tZXINtgwYNjMOHDxdZnp+fb3Tt2tWQZISEhBh79uwpc39vvfWWIcn4z3/+U24bqxNsf/nlF8Nmsxk2m824/fbbSyzPzc11hpLx48eXWH7zzTc7P2Dz8vJKLJ88ebLRqVMn4/LLLy+xbPbs2c62r1q1qsTyL7/80ujUqZPRqVMnIzMzs8iyO+64w7nthx9+WGLbAwcOGC1btjQkGY0bNzaOHz9e1ttQYa7B9sCBA842bN682Rn8Bg4cWG6wdfjhhx+MkSNHGiEhIUWC6X333WfY7fYi67oG2zlz5hhpaWmlPo4ePeqR83W1dOlS5/Ffe+015+sJCQmGJKNbt25ut9u/f79zuylTppRYfvDgQefPefHixR5tc5MmTQybzWZ06dLFbW/4sGHDDEnGOeecU+6+qhpsn3322RLLv/jiC+fya6+9tsiy4n/AzJgxo8T2jz76qHP52rVra+ycAW+gxhbwotq1a0uS2zvBS/PAAw8oNja2yGshISF68MEHJUkFBQWlDpvkcPz4cUlSnTp1KtPcSuvYsaMKCgpUUFCg6dOnl1i+c+dO5/NDhw6VWN62bVtJp94fd/XAkyZNUmpqqhYsWFDqtpK0bdu2EssHDRqk1NRUpaamql69es7XCwoK9N5770mS+vfv73YItkaNGumJJ56QJB04cEBfffVViXWqq1GjRmrevLmkU3W2jlrb0upr3enVq5fef/997dy5U9dff73z9WeffVaPPPJIqds5xrEt7VG3bt0qnlXpEhMTFR0dLenvOttdu3Y5r5HSRkNo0KCB8+fn7ufcoEED58/5oosu8mib9+/fr4KCAm3cuFFWa9GPz/T0dOcoFe6ubU9o2rSp7rjjjhKvX3LJJc6xfsv6XTBy5EiNGzeuxOuuNc/Hjh0rsszX5wxUFhM0AF4UHR2tkJCQMmeBKs4xNFJxroGnvBtH/vrrL0lSXFxchY9bXXv37tVPP/2kX375Rb/99pvWrl1b5OYtdze+3XjjjXr11Vf1+++/q0ePHho6dKi6du2qDh06qEuXLqpfv36px+vevbuGDRumjz/+WGPHjtWCBQvUs2dPtW/fXp07d1arVq3cbrd9+3bnJAhlBaELL7zQ+Xzt2rUaNmxYue9BZfXo0UP79u3TTz/9pE2bNkmqXLB1aNGihWbNmqUBAwbouuuuU0FBgaZNm6axY8eW+j54W2hoqAYMGKB58+Zp6dKlKigoqNBsYyEhIXr88cd15513atasWdq2bZsuvPBCdejQQWeccYY6duxY4xORZGVladWqVc5re8OGDdqyZYtzDOKK3tRZWV27dlVYWFipyzZt2qSDBw8qOzvb7cQv48ePd7ttgwYNyj22r84ZqCyCLeBFISEhOv3004uMVVoeRy9ecU2aNHE+L2t2Kkn69ddfJRXt1awpa9as0b333qsff/zR+ZrValVCQoJuuukmLV++vEjPrat69erpp59+0nPPPacFCxZo1qxZmjlzpiTJYrGoS5cuuu2223TjjTfKYrGU2H7u3Ll644039P7772vRokX67LPPnMvi4+N13XXX6YEHHlBkZKTz9czMTOfzsoK/67KMjIwKvBOVd/bZZ+vTTz/V66+/7nytR48eys3NLbHu/v37tXDhQuc6jlEVXF1zzTXatGmTpk2b5gyOt956a420vSouvfRSzZs3T9nZ2Vq9erWzt7Fx48bq3r17qdvdcccdatu2rV5++WWtWLFCq1atci6Ljo7W5ZdfrkcffVStW7f2aHv379+v++67Tx999JEKCgqcrzdp0kRDhw7VkSNHtGTJEo8e01Xjxo1LXeYYLk6Sjh49WiLYRkZGqnPnzpU+pq/PGagsShEALzv77LOVmZlZ4WGZSuvdPXjwoPN5WV8V2+12paSkqHnz5mV+MHrC6tWr1adPH/34448677zzNGvWLG3evFnHjh3Ttm3b9MYbb6hZs2Zl7iM2NlZPPfWUfv31Vx09elTr16/X22+/raFDh2rTpk266aab3H6dKkk2m0233nqrfvjhB+dkBx999JFuu+02ZWRk6KmnntIFF1xQJCjGxMQ4nx8+fLjUdrmG2eKlIZ5SvHe2WbNmRf6AcbVr1y7deuutuvXWW92WZjj07dvX+by8P4C8bdCgQc6vtz///HMtX75c0qnA6+4PF1cDBgzQwoULlZWVpd27d+urr77So48+qqioKL377rvq0aOHtm/f7rG2HjlyRL169dKcOXPUpEkTPf/881q1apUyMzO1f/9+ffTRR6V+u+Ipf/75Z6nLXH9PuOutdb3OK8ofzhmoLIIt4GUDBgyQpDLDiKuffvrJ7euOr6olqU2bNqVuv3LlSh0+fNh53Jr05JNPKjc3Vw0bNtTSpUt1/fXXq3Pnzs6xSA3DKNJDWtx3332nFStWOD+k69Spo27dumnMmDH6+OOPNXHiREnSq6++qn379hXZdvPmzVqxYoWzLCMkJETt27fX8OHD9fLLLzunFN60aZPmz5/v3K5t27bOWs+yep4cE2tIVSsPqIjiva7uemEdXP9AKOsbANfyj/L+qPC2uLg49ezZU5I0bdo05xjFZc02tnv3bq1YsUKrV6+WdKon/7TTTtPAgQP1+OOPa8OGDYqLi9ORI0f0zDPPeKytb731lnMs4wULFuiee+7ReeedV6Reu6Z68h02bNjgdoY56e/xj5s2bVrkGwmH8v5QcMcfzhmoLIIt4GVDhw5VZGSk3njjDeXl5ZW7/pQpU0pMSmC3250zTVksFl188cWlbj9jxgxJKnIzUU3ZsWOHpFM3QjlulHMoLCzU/fffX2ZP9fDhw5WUlKSXXnrJ7XLXmbaKB9snn3xSSUlJpU4d67rt3r17nc9DQkJ03XXXSToVbOfOnVti27/++kuPPvqopFNfBycnJ5d6DtURExNT5OvzsgJ0q1at1L59e0mnejvdTb978uRJvfjii5JOnadrnbC/KB5iw8PDy2znF198oaSkJCUlJSk7O7vE8ri4OGfvpOvPuboc17Ykt3XK8+fP1yuvvOKx47lz8OBBPf/88yVe//TTT/XLL79IOnUjmaf4wzkDlUWNLeBldevW1dixY/XCCy9oxowZuvfee8tcf9++fUpMTNQTTzyh9u3ba8+ePc6vBCVpxIgRRUKbq3Xr1umTTz5Rt27dlJiYWKl2ZmVlOWeAKsvpp5+upk2bSjo1Q9L27dv1888/66GHHtI111yjgoICrVq1Sm+88YY2b97s3M5x04mrIUOG6O2339Zzzz2nwsJCDRgwQM2aNdPJkyf1008/6eGHH5YkRUREOEOd67bz58/XqlWrdPXVV+uaa65R69atZbPZtG3bNueoBpKcd5A7TJo0SV988YXS0tI0YsQIff/997r22msVFRWllStX6vHHH9eff/4pi8Wil19+uURo96QePXo4e1nL6rGVTv3RM3ToUBUUFOiCCy7Q/fffr+TkZMXExCg1NVVTp0511jPfeeedpZY1HDhwQH/88UeZx6pdu3aROk5PueyyyzRhwgTnv/v161fm6B2DBg1SaGioTp48qQsvvFC33XabzjrrLNWpU0d//vmnXnvtNec5F/85V4frTIHXX3+9HnnkEUVGRio1NVX/+c9/nN8ISO6vbU+wWCx69NFHlZWVpREjRshms2nx4sWaPHmypFPfcJR2g1hV+MM5A5Xm6/HGgGCUnp5u1KtXz4iKijJ27dpVYrnrOLa33XZbqQPn9+nTp9QxRnNzc51j3S5durTCbSvtWKU9XGcr+v77741atWqVum5iYqLRs2dPQ5LRo0cPt++Lo82lPaxWq/HGG2+U2NZutxvXXXddue0tPs6nw++//26cccYZpW4XERFhvPvuuxV+HyvCdRxbh2effdZ5zPT0dMMwjDLHsf33v/9t2Gy2cs85Pz+/yHaVmaBBOjXrVU1p1aqV8zgvv/xyueu//vrr5Z5z586dS4xXXB3p6enOsYzdPZo1a2aMHDnSkE5NLlHeTFxVGcd25MiRRocOHdwev3bt2sbnn39eYlvXmcdKU9r15elzBryBUgTAB+Li4vTCCy8oOztb11xzjdu73h0ee+wxvffeezr77LNVp04d1a5dW+ecc45eeeUVLVmypNQbx+6//35t3LhRo0ePVr9+/WrqVIo4//zztWnTJt18881KSEhQRESEYmNj1b9/f82aNUvLly9X//79JZ3qTV66dGmR7ePi4rRmzRq98cYbuvjii9W4cWOFhoYqKipKnTp10m233aYtW7bopptuKnFsi8Wid999V1999ZWuuuoqnX766YqIiFBERIRat26tYcOG6ZtvvnGOWVvc6aefrnXr1mnGjBnq1auXoqKiFB4eroSEBN1+++367bffnCULNclRftCqVasKDc921113aePGjbrpppuUkJCgWrVqKTw8XKeddpquvPJKLV68WO+9955CQvz3CzrXcoSy6msdbr75Zm3evFm33XabzjjjDNWtW1ehoaFq3ry5kpKSNHPmTK1Zs6ZILWh1xcXFaf369Xr88cfVuXNnRUZGqm7duurWrZuefPJJ/frrr7rhhhsknaolnzZtmseO7VCvXj2tWrVKd955p+Lj4xUWFqZmzZppzJgx2rx5c4Xeu8rwh3MGKstiGIbh60YAwerqq6/W3LlzddVVV2n27NklBkCvqhkzZjiHRFq7dq3bu6QBmEPLli21e/dujRs3zlkzD8A9//0THggC77zzjnbs2KG5c+cqJCRE7733XrXD7auvvqo777xT9erV08KFCwm1AICgQSkC4EO1a9fW119/rY4dO6pZs2Ye6bFt0qSJYmNj9fXXX6tdu3YeaCUAAOZAjy3gYw0aNNDq1avdjj1ZFZdffrn69+/vsf0BAGAW9NgCfsDTIZRQCwAIRtw8BgAAgIBAjy0AAAACAsEWAAAAAYFgCwAAgIBAsAUAAEBAINgCAAAgIBBsAQAAEBAItgAAAAgIBFsAAAAEhP8HQl5/xGKJEBsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MSEs =lasso_model.mse_path_  # 读出在所有 lambda 交叉验证情况下的均分误差 MSE\n",
    "\"\"\"\n",
    "# 等价于下面两行\n",
    "MSEs_mean, MSEs_std = [], []\n",
    "for i in range(len(MSEs)):\n",
    "    MSEs_mean.append(MSEs[i].mean())\n",
    "    MSEs_std.append(MSEs[i].std())\n",
    "\"\"\"\n",
    "MSEs_mean = np.apply_along_axis(np.mean,1,MSEs) # 对均分误差 MSE 每行求均值 mean （0=列，1=行）\n",
    "MSEs_std = np.apply_along_axis(np.std,1,MSEs) # 对均分误差 MSE 每行求标准差 std\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(np.log(lasso_model.alphas_),MSEs_mean    #x, y 数据，一一对应\n",
    "             , yerr=MSEs_std                    #y 误差范围（误差棒）\n",
    "             , fmt=\"p\"                          #数据点标记（o=圆点）\n",
    "             , ms=1                             #数据点大小\n",
    "             , mfc=\"#134F85\"                          #数据点颜色 r=红色\n",
    "             , mec=\"#134F85\"                          #数据点边缘颜色\n",
    "             , ecolor=\"black\"               #误差棒颜色\n",
    "             , elinewidth=0.1                   #误差棒线宽\n",
    "             , capsize=0.1                     #误差棒边界线长度\n",
    "             , capthick=2\n",
    "            ,label='lambda')                      #误差棒边界线厚度\n",
    "# plt.semilogx()    # 用 x 的对数作为横坐标\n",
    "plt.axvline(np.log(lasso_model.alpha_),color = '#CC7892',ls=\"--\",label='选定lambda') # 选出的 lambda 值用虚线标出\n",
    "print(lasso_model.alpha_)  # 选出的 lambda 值 0.0006551285568595509\n",
    "plt.xlabel('Log(Lambda)',fontsize=18)\n",
    "plt.ylabel('MSE',fontsize=18)\n",
    "plt.title('(b)Lasso MSE Vs alpha',y=-0.18,fontsize=22)\n",
    "plt.savefig('lasso2.jpg', dpi=600, bbox_inches='tight',pad_inches=0)#bbox_inches='tight',pad_inches=0解决图片空白边缘\n",
    "plt.savefig('lasso2.svg', dpi=600, bbox_inches='tight',pad_inches=0)#bbox_inches='tight',pad_inches=0解决图片空白边缘\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI_lasso_sort=FI_lasso[FI_lasso[\"Feature Importance\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI_lasso_sort=FI_lasso_sort.sort_values(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ldrmodules.not_in_init_avg</th>\n",
       "      <td>-2.108956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psxview.not_in_csrss_handles_false_avg</th>\n",
       "      <td>-1.930373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svcscan.shared_process_services</th>\n",
       "      <td>-0.119589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pslist.avg_threads</th>\n",
       "      <td>-0.082743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dlllist.avg_dlls_per_proc</th>\n",
       "      <td>-0.024059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malfind.uniqueInjections</th>\n",
       "      <td>-0.014996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svcscan.nactive</th>\n",
       "      <td>-0.014142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pslist.nppid</th>\n",
       "      <td>-0.012861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.ntimer</th>\n",
       "      <td>-0.009236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.nsection</th>\n",
       "      <td>-0.001164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ldrmodules.not_in_load</th>\n",
       "      <td>-0.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>callbacks.ncallbacks</th>\n",
       "      <td>-0.000960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ldrmodules.not_in_mem</th>\n",
       "      <td>-0.000524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dlllist.ndlls</th>\n",
       "      <td>-0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ldrmodules.not_in_init</th>\n",
       "      <td>-0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.avg_handles_per_proc</th>\n",
       "      <td>-0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.nkey</th>\n",
       "      <td>-0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pslist.avg_handlers</th>\n",
       "      <td>-0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.nevent</th>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.nsemaphore</th>\n",
       "      <td>0.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.nthread</th>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.nmutant</th>\n",
       "      <td>0.000983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psxview.not_in_pslist</th>\n",
       "      <td>0.001696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malfind.ninjections</th>\n",
       "      <td>0.003726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handles.ndirectory</th>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psxview.not_in_eprocess_pool</th>\n",
       "      <td>0.018669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svcscan.kernel_drivers</th>\n",
       "      <td>0.027690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svcscan.process_services</th>\n",
       "      <td>0.029946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>callbacks.nanonymous</th>\n",
       "      <td>0.118561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>callbacks.ngeneric</th>\n",
       "      <td>0.321794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svcscan.fs_drivers</th>\n",
       "      <td>0.352316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modules.nmodules</th>\n",
       "      <td>0.431147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psxview.not_in_pslist_false_avg</th>\n",
       "      <td>1.164804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Feature Importance\n",
       "ldrmodules.not_in_init_avg                       -2.108956\n",
       "psxview.not_in_csrss_handles_false_avg           -1.930373\n",
       "svcscan.shared_process_services                  -0.119589\n",
       "pslist.avg_threads                               -0.082743\n",
       "dlllist.avg_dlls_per_proc                        -0.024059\n",
       "malfind.uniqueInjections                         -0.014996\n",
       "svcscan.nactive                                  -0.014142\n",
       "pslist.nppid                                     -0.012861\n",
       "handles.ntimer                                   -0.009236\n",
       "handles.nsection                                 -0.001164\n",
       "ldrmodules.not_in_load                           -0.001157\n",
       "callbacks.ncallbacks                             -0.000960\n",
       "ldrmodules.not_in_mem                            -0.000524\n",
       "dlllist.ndlls                                    -0.000351\n",
       "ldrmodules.not_in_init                           -0.000284\n",
       "handles.avg_handles_per_proc                     -0.000130\n",
       "handles.nkey                                     -0.000105\n",
       "pslist.avg_handlers                              -0.000104\n",
       "handles.nevent                                   -0.000100\n",
       "handles.nsemaphore                                0.000578\n",
       "handles.nthread                                   0.000847\n",
       "handles.nmutant                                   0.000983\n",
       "psxview.not_in_pslist                             0.001696\n",
       "malfind.ninjections                               0.003726\n",
       "handles.ndirectory                                0.008500\n",
       "psxview.not_in_eprocess_pool                      0.018669\n",
       "svcscan.kernel_drivers                            0.027690\n",
       "svcscan.process_services                          0.029946\n",
       "callbacks.nanonymous                              0.118561\n",
       "callbacks.ngeneric                                0.321794\n",
       "svcscan.fs_drivers                                0.352316\n",
       "modules.nmodules                                  0.431147\n",
       "psxview.not_in_pslist_false_avg                   1.164804"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FI_lasso_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['psxview.not_in_csrss_handles_false_avg'], dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FI_lasso_sort.iloc[1:2,::].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FI_lasso_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dibanben)",
   "language": "python",
   "name": "dibanben"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
